{"pages":[{"title":"在远程服务器上挂载 Ceph RBD 块设备","url":"/2020/Map-and-Mount-Ceph-RBD-on-Remote-Servers/","text":"Ceph 存储一个重要的使用场景就是其块设备，本文将会介绍一下如何挂载 Ceph RBD 到远程服务器上。 准备工作 Ceph 安装 repo: [ceph] name=Ceph packages for $basearch baseurl=https://download.ceph.com/rpm-mimic/el7/$basearch enabled=1 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-mimic/el7/noarch enabled=1 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph source packages baseurl=https://download.ceph.com/rpm-mimic/el7/SRPMS enabled=0 priority=2 gpgcheck=1 gpgkey=https://download.ceph.com/keys/release.asc 安装 Ceph 和 RBD 客户端: $ sudo yum install -y ceph-common python-rbd 远程服务器挂载 创建 RBD $ rbd create --pool volulmes --size 50G lintao_test Ceph 配置 /etc/ceph/ceph.conf: [global] fsid = 1da65d0e-4284-41e0-868e-8c25b89648fc mon_initial_members = ceph01, ceph02, ceph03 mon_host = 192.168.15.38,192.168.15.39,192.168.15.40 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx [mon] mgr initial modules = dashboard [client.cinder] keyring = /etc/ceph/ceph.client.cinder.keyring 远程客户端使用 key: /etc/ceph/ceph.client.rbd.keyring: [client.rbd] key = AQBooSldwwbkAhAAtxRH0lHGfnUautW2rF6pkQ== Ceph RBD Map map rbd $ sudo rbd map volumes/lintao_test --id cinder /dev/rbd0 if unmap rbd $ sudo rbd unmap volumes/lintao_test --id cinder 格式化与挂载 $ sudo fdisk /dev/rbd/volumes/lintao_test ... $ sudo mkfs.ext4 /dev/rbd/volumes/lintao_test-part1 ... $ sudo mkdir /data $ sudo mount /dev/rbd/volumes/lintao_test-part1 /data 自动挂载 /etc/ceph/rbdmap volumes/lintao_test id=cinder,keyring=/etc/ceph/ceph.client.keyring enable rbdmap service $ sudo systemctl enable --now rbdmap /etc/fstab $ sudo blkid /dev/rbd/volumes/lintao_test-part1 ### /etc/fstab UUID=0a9d74d8-8f37-4a80-ac70-28c292e79784 /data ext4 defaults 0 0 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph rbd"},{"title":"在 CentOS 8 下安装并配置 Docker","url":"/2020/Install-and-Setup-Docker-under-CentOS-8/","text":"红帽已经在 CentOS 8 中用 Podman 替代了 Docker， 它是与 Docker 类似的容器技术，使用命令都十分相似， 但对于熟悉 Docker 及配置的管理人员，他们可能会倾向于使用 Docker。 本文会简单介绍如何在 CentOS 8 下安装、配置 Docker。 安装 Docker 准备工作 卸载旧版本的 Docker. $ sudo dnf remove docker-common docker container-selinux docker-selinux docker-engine 添加 Docker 安装源 $ sudo curl https://download.docker.com/linux/centos/docker-ce.repo -o /etc/yum.repos.d/docker-ce.repo 安装最新版的 Containerd. 你可能需要主动查看最新版本，目前 (06/03/2020) 是 1.2.6-3.3. $ sudo dnf install -y https://download.docker.com/linux/centos/7/x86_64/stable/Packages/containerd.io-1.2.6-3.3.el7.x86_64.rpm 安装 Docker CE $ sudo dnf install -y docker-ce 启用 Docker 服务 $ sudo systemctl enable --now docker 为非 Root 用户添加运行 Docker 的权限 $ sudo groupadd docker $ sudo useradd -m -d /home/test -U -r test $ sudo usermoad -aG docker test 验证 Docker 是否正确安装 $ docker run busybox ping -c 5 google.com 不出意外， docker ping 是不通的。 我们安装的 Docker 是有问题的， 防火墙并没有正确配置。 配置系统防火墙 0. 开启 IP 转发 $ echo \"net.ipv4.ip_forward = 1\" | sudo tee /etc/sysctl.conf $ sudo sysctl -p 1. 关闭防火墙 One easily way that makes docker access the network is stop the firewall. CentOS 8 using firewalld with nftables to protect the system. $ sudo systemctl disable --now firewalld $ sudo systemctl mask --now firewalld But this will expose our server to security attacks. So, we need configure the firewalld to granting docker the network access. 2. 配置防火墙 检查活动的 zone 和 docker0 所在的 zone $ sudo firewall-cmd --get-active-zones $ sudo firewall-cmd --get-zone-of-interface=docker0 我们发现 Docker 的 docker0 没有在任何 zone 中, 我们需要将其加到某一个 zone 如 public 中 添加 docker0 到 public zone. $ firewall-cmd --permanent --zone=public --change-interface=docker0 #or $ firewall-cmd --permanent --zone=public --add-interface=docker0 #or $ nmcli connection modify docker0 connection.zone public 为 docker0 配置 iptables 规则. $ firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 4 -i docker0 -j ACCEPT $ firewall-cmd --permanent --direct --add-rule ipv6 filter INPUT 6 -i docker0 -j ACCEPT $ firewall-cmd --zone=public --add-masquerade --permanent 打开所需要的端口. ## dns $ firewall-cmd --zone=public --add-port=53/tcp ## http $ firewall-cmd --zone=public --add-port=80/tcp ## https $ firewall-cmd --zone=public --add-port=443/tcp 重新载入防火墙规则，重启 Docker 服务. $ sudo firewall-cmd --reload $ sudo systemctl restart docker 验证 Docker 是否能连接网络 $ docker run busybox ping -c 5 google.com PING google.com (172.217.11.46): 56 data bytes 64 bytes from 172.217.11.46: seq=0 ttl=55 time=10.850 ms 64 bytes from 172.217.11.46: seq=1 ttl=55 time=1.786 ms 64 bytes from 172.217.11.46: seq=2 ttl=55 time=1.759 ms 64 bytes from 172.217.11.46: seq=3 ttl=55 time=1.978 ms 64 bytes from 172.217.11.46: seq=4 ttl=55 time=1.797 ms --- google.com ping statistics --- 5 packets transmitted, 5 packets received, 0% packet loss round-trip min/avg/max = 1.759/3.634/10.850 ms 可以看到，现在 Docker 已经能够访问网络了。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"centos docker firewalld"},{"title":"connecting redis cluster using python","url":"/2020/connecting-redis-cluster-using-python/","text":"Connecting redis cluster using python. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"redis"},{"title":"给你的网站加上 Google Adsense","url":"/2020/adding-google-adsense-to-your-site/","text":"本文简单记录了给 Hexo 添加 Google Adsense 的过程，模板是简单修改过的 tranquilpeak. Intro document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"hexo google"},{"title":"Dashboard in Ceph","url":"/2019/Dashboard-in-Ceph/","text":"This post will discuss the dashboard feature in ceph mgr. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph grafana"},{"title":"使用 Ansible 来部署 Ceph 集群","url":"/2019/Using-Ansible-to-Deploy-Ceph/","text":"Ansible is the AutoDev tool developed by RedHat and we can easily deploy Ceph using ansible. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph ansible"},{"title":"在 Beamer 中添加计时器和 Logo","url":"/2019/Adding-Timer-and-Logo-in-Beamer/","text":"在 PPT 中有一个演讲计时功能，能够帮助报告人掌握演讲时间。 其实 pdf 中也有这一功能，在 beamer 中添加计时器比较方便。 此外，在 beamer 标题中加上特定的 Logo 也是比较简单的。 添加 timer 计时器 LaTeX 中 tdclock 包可以提供定时功能。添加一个计时器也很简单。 在导言区加入 tdclock 包。可以设置一次提醒，二次提醒时间，更新时间间隔等等。 \\usepackage[timeinterval=2.0, timeduration=2.0, timedeath=0, fillcolorwarningsecond=white!60!yellow,timewarningfirst=900,timewarningsecond=1080]{tdclock} \\initclock 初始化。在适当的位置，如 titlepage 中加入这一命令。 \\begin{frame} \\titlepage \\initclock \\end{frame} 在要显示的地方加入 \\crono 命令，比如在 footer 中。如果使用的三段式 footer 结构，可以在 footer 中显示时间，则可以这样做： \\begin{frame} \\titlepage \\initclock \\end{frame} \\date{\\today \\crono} 这样会显示日期和从 00:00:00 开始的计时器，如下图黑框所示。 tdclock 与 xelatex 一起使用会有些小问题，会显示不完全， 这里 提供了一种解决方法。测试是可以的。 在 frame 中加入 logo 在标题中加入 logo 通过简单地修改 frametitle 就能加入 logo， 而不用 textopt 等包。 在 \\begin{document} 之前重新定义 frametitle，这会在 frametitle 最右端显示一个 logo: \\setbeamertemplate{frametitle} {\\begin{beamercolorbox}[wd=\\paperwidth]{frametitle} \\strut\\hspace{0.5em}\\insertframetitle\\strut \\hfill \\raisebox{-2mm}{\\includegraphics[width=1cm]{$logo$}} \\end{beamercolorbox} } 只为 titlepage 页面，定义新的 logo，而不是在 title 中添加 logo。 {\\setbeamertemplate{logo}{} \\titlegraphic{\\includegraphics[height=1.8cm]{images/logo.jpg}\\hspace{1em}\\includegraphics[height=1.8cm]{images/lhaaso.png}} \\begin{frame} \\titlepage \\initclock \\end{frame} } 最终结果如下图所示 在页面右下角加入 logo。 有时我们会想在其它地方加上 logo， 比如 页面右下角， 这时一般直接使用 \\logo 命令即可。 \\logo{\\includegraphics[height=1cm]{my_logo.png} 如果想要调整位置，可以使用 pgf 命令， 但坐标位置需要仔细调整。 \\logo{\\pgfputat{\\pgfxy(-9,9)}{\\pgfbox[center,base]{\\includegraphics[width=1.5cm]{$logo$}}}} document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"latex beamer"},{"title":"使用 Autotool 工具链编译动态链接库","url":"/2019/Build-Shared-Libs-Using-Autotools/","text":"最近需要将几个静态编译的包改成动态编译的库，幸运的是，它们都是用 autoconf 那一套工具生成 configure 文件，转换起来很方便。 从静态编译到动态编译 基本上，对于一个目录结构如下的简单的包， - include: - a.h - b.h - Makefile.am - ... - lib: - a.cpp - b.cpp - Makefile.am - ... - configure.ac - ... 需要修改的只有两个文件 - configure.ac 和 lib/Makefile.am. Configure.ac 一般地， configure.ac 文件的开头会类似如下面所示： dnl Process this file with autoconf to produce a configure script. AC_INIT(xxx, xxx, [xxxx]) AC_CONFIG_AUX_DIR(config) AC_CONFIG_SRCDIR([lib/xxx.c]) AC_CONFIG_SRCDIR([examples/xxx.c]) AC_CONFIG_HEADER([include/xxx.h]) AM_INIT_AUTOMAKE([subdir-objects]) 需要做的是，在这后面加上 AC_ENABLE_SHARED AC_DISABLE_STATIC LT_INIT 这里只编译动态链接的版本，所以加上了 AC_DISABLE_STATIC，这也是为方便之后修改 lib/Makefile.am 文件。 lib/Makefile.am lib/Makefile.am 文件中一般会有如下的内容 lib_LIBRARIES = libxxx.a libxxx_a_CFLAGS= xxx libxxx_a_CPPFLAGS= xxx libxxx_a_CXXFLAGS= xxx libxxx_a_LDFLAGS= xxx libxxx_a_SOURCES= xxx 如果只编译动态链接的版本，只需要替换成如下的内容： lib_LTLIBRARIES = libxxx.la libxxx_la_CFLAGS= xxx libxxx_la_CPPFLAGS= xxx libxxx_la_CXXFLAGS= xxx libxxx_la_LDFLAGS= xxx libxxx_la_SOURCES= xxx 如果要编译静态版本，最好用没修改过的版本进行编译，或者将上面的内容加上 Makefile.am 中。 注 ： 若有多个 libxxx.a, 则都需要进行修改。 tests 或 examples 有的库会带有测试程序，放在 tests 或 examples 里面。这样修改了前面的文件后，这里面的 Makefile.am 也需要进行修改。 相应的库文件 libxxx.a 改成 libxxx.so 链接的库路径从 -L../lib/ 改成 -L../lib/.libs，因为 生成的库文件存放在 lib/.libs 目录下。 重新生成 configure 在做完上面的工作后，需要做的是重新生成 configure 文件: autoreconf -i 之后编译时加上 --enable-shared 选项即可。 转换过程中的问题 若 configure.ac 中有 AC_CHECK_TOOL(AR, ar, [ar]) 语句，要将添加的内容放在这句后面，或去掉这一句。 若有其他依赖库，比如放在 other_libs 下面，这些也是需要做相应修改的。 qdpxx 需要从 master 分支转到 devel 分支。 若有可执行程序， 比如在 mainprog 目录下， mainprog 下的 Makefile.am 也需要进行相应修改。 有的库会在 configure 时，没有正常获取 include 目录，需要手动加上, 在 lib/Makefile 中的 DEFAULT_INCLUDES 选项。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux automake chroma quda"},{"title":"Find What Ports Are Listening in Linux","url":"/2019/Find-What-Ports-Are-Listening-in-Linux/","text":"Sometimes we need to know which program is using the port like 80 port, and this post will show you how to find the program occuping the specific port. 参考 What Are Ports? How To Check Linux Open Ports? 4 Ways to Find Out What Ports Are Listening in Linux Linux Find Out Which Process Is Listening Upon a Port document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"vgpu nvidia"},{"title":"在 Linux KVM 中使用 NVIDIA vGPU","url":"/2019/Using-NVIDIA-vGPU-in-Linux-KVM/","text":"当前 CPU 虚拟化是非常普遍了。 xxx。这篇博文将记录如何在 RHEL/CentOS 下使用 NVIDIA 的 Virtual GPU。 获取 NVIDIA vGPU 软件 安装配置 License Server 安装准备工作 作为 License server， 必须有固定的 IP 地址 至少一个不变的以太网 MAC 地址。 准确的时间，推荐 NTP。 License server 依赖于： Java Runtime Environment tomcat 和 tomcat-webapps $ sudo yum install java tomcat tomcat-webapps $ sudo systemctl enable --now tomcat 如果安装成功， 访问 localhost:8080, 会看到如下类似的页面， Linux 下安装 License Server 解压下载的 NVIDIA License Server 包如 NVIDIA-ls-linux-2018.10.0.25098346.zip $ unzip NVIDIA-ls-linux-2018.10.0.25098346.zip && cd NVIDIA-ls-linux-2018.10.0.25098346 $ sh setup.bin 安装结束后，访问 http://localhost:8080/licserver, 应该就能看到如下的页面： 安装配置 Host 宿主机 我们需要在宿主机上安装 NVIDIA vGPU Manager 软件，登陆 NVIDIA Enterprise 后，可以下载 NVIDIA vGPU for RHEL KVM 或 NVIDIA vGPU for Linux KVM，安装里面的 NVIDIA-vGPU-rhel-7.5-410.91.x86_64 或 NVIDIA-Linux-x86_64-410.91-vgpu-kvm 到 Host 宿主机。 安装配置 Guest 虚拟机 需要注意的事情 虚拟的 GPU 在宿主机重启后会消失 每次重启后， vGPU 都需要重新生成，可能有设置使其得以保存。目前需要将 vGPU id 保存下来，宿主机重启后，依照原来的 vGPU ids 来创建 vGPU， 这样分配到各虚拟机的 vGPU 就不需要再进行更改。 License Server 重启后可能需要重新进行配置 LS 重启后，有可能与绑定的网卡解绑了，需要手动确认，然后重新进行配置。(-_-) 参考文献 Virtual GPU Software R390 for Linux with KVM Release Notes Virtual GPU Software User Guide Virtual GPU Software R410 for Red Hat Enterprise Linux with KVM Release Notes RedHat: 16.7. ASSIGNING GPU DEVICES OpenStack Docs: Attaching virtual GPU devices to guests OpenStack Docs: Support virtual GPU resources document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"kvm qemu vgpu"},{"title":"利用 Grafana 监控 Ceph 集群的状态和性能","url":"/2019/Monitor-Ceph-Cluster-Status-and-Performance-with-Grafana/","text":"Grafana 是一套开源的跨平台的可视化工具，可以将采集的数据进行可视化展示，并进行通知，展示方式多样，能够让我们随时掌控系统的运行情况。这里我们介绍如何使用 Grafana 来对 Ceph 集群进行性能和运行状态的监控。 在 CentoS 7 下安装 Grafana 最简单的方法是通过 Grafana 的官方 Repo 来安装 Grafana。 添加 grafana repo $ cat < EOF | sudo tee /etc/yum.repos.d/grafana.repo [grafana] name=grafana baseurl=https://packagecloud.io/grafana/stable/el/7/ repo_gpgcheck=1 enabled=1 gpgcheck=1 gpgkey=https://packagecloud.io/gpg.key https://grafanarel.s3.amazonaws.com/RPM-GPG-KEY-grafana sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt EOF","tags":"ceph grafana"},{"title":"GPU 虚拟化与云计算","url":"/2019/GPU-Virtualizations-and-Cloud-Computing/","text":"目前 GPU 在 HPC 和云计算中占据着越来越重要的位置。 GPU 虚拟化历史 GPU 虚拟化现状 vGPU 之 NVIDIA vGPU 之 AMD vGPU 之 Intel OpenStack 中的 GPU 虚拟化 (to-update) 目前 OpenStack 已经可以支持 GPU 虚拟化 vGPU，但是尚处于实验阶段。OpenStack 从 Queens 版本开始支持 vGPU，目前开发版本是 Stein, 与 vGPU 相关的计划和进展可在 这里 找到。 文献参考 【技术系列】浅谈 GPU 虚拟化技术 - 孟蓁蓁 Attaching virtual GPU devices to guests - OpenStack Consumer-grade GPU passthrough in an OpenStack system (NVIDIA GPUs) Setting up GPU Hypervisors on OpenStack Virtual GPUs In OpenStack: Filling A Hole In The Cloud Machine Learning World document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"openstack vgpu virtualization cuda"},{"title":"XCache 的配置、测试与优化","url":"/2019/XCache-Config-Testing-And-Optimizing/","text":"XRoodD is the common protocol generally used in high energy physics computing. XRootD proxy for file cache(XCache) is a typical way for XRootD. XRootD 代理服务 XRootD 直接代理模式 XRootD 转发代理模式 XRootD 联合代理模式 XRootD 自动代理 XRootD 代理缓存服务 XRootD 磁盘缓存模式 XRootD 自带了几个简单的文件缓存配置 - xrootd-filecache-cluster.cf 和 xrootd-filecache-standalone.cfg, 下面是一个磁盘缓存模式 Disk Cache 的配置文件: all.export / ofs.osslib libXrdPss.so xrootd.trace emsg login stall redirect xrd.trace info ofs.trace info pss.origin = some_server:1094 ### 包含 Disk Cache Proxy 插件的共享库。 pss.cachelib libXrdFileCache.so ### 用来缓存需要写到磁盘上的代理缓冲的最大允许 RAM。超过这个值，代理会将请求发送到远程服务器。 pfc.ram 16g pss.setopt DebugLevel 0 oss.memfile max 80% check map preload ### 用来保存缓存文件的位置 oss.localroot /data pfc.trace DebugLevel pfc.prefetch 8 pfc.blocksize 512k ofs.ckslib * libXrdPss.so pfc.user xrootd XRootD 内存缓存模式 与 Disk Cache 模式不同的是，XRootD 内存缓存模式 Memory Cache 是将最近访问过的文件缓存在内存中，所以要求作为服务器的内存要足够大。 XRootD Disk Cache 与 Memory Cache 是不能共存的。 XRootD Memory Cache 与标准的 XRoot Proxy 服务 相比，只多了一个 pss.cahce 语句。 下面是一个 Memory Cahce 的配置文件。 all.export / ofs.osslib libXrdPss.so xrootd.trace emsg login stall redirect xrd.trace info ofs.trace info pss.origin = some_server:1094 pss.cache logstats max2cache 16g sfiles on r preread pss.setopt DebugLevel 0 XCache 测试 参考 Proxy Storage Services (Caching, Non-Caching, &amp; Server-less Caching) Configuration Reference XRootd, disk-based, caching proxy for optimization of data access, data placement and data replication Installing and configuring xrootd A fedorated Xrootd Cache Stash Cache Installation Distributed and on-demand cache for CMS experiment at LHC XRootD-Caching-Setup The adoption of the HTTP/XRootD protocols for a new data caching architecture in WLCG experiments Distributed Caching Using the HTCondor CacheD document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"eos xrootd"},{"title":"将 Ceph 文件系统 CephFS 挂载为 NFS","url":"/2019/Mount-CephFS-over-NFS/","text":"利用 NFS-Ganesha 或 ceph-fuse 与 nfsd， 我们可以将 CephFS 通过 NFS 协议对外发布出去，用户可以通过 NFS 客户端挂载 CephFS 文件系统进行访问。 Ceph 与 NFS Ceph 是一种开源的分布式存储系统，具有优异的性能、可靠性和可扩展性。 Ceph 底层使用 RADOS，提供 RBD、RGW 和 CephFS 三种标准的访问接口。从 Luminous 版本开始， CephFS 已经具有相当好的稳定性，可用在生产环境。 Ceph 架构 NFS (Network File System) 是常用的网络文件系统，用户通过 NFS 可以像访问本地文件一样访问远程的文件，NFS 服务端和客户端之间通过 NFS 协议进行通信。 用户可以用 Ceph 提供的 ceph-fuse 挂载 CephFS 进行访问，也可以通过 ceph 的内核模块进行挂载访问，具体可参考 Ceph 文件系统 CephFS 的介绍与配置 。 但有时候，由于各种原因，一些系统无法通过这两种方法访问 CephFS，我们可以将 CephFS 通过 NFS 协议发布出去，用户可以通过 NFS 客户端挂载发布出去的 CephFS。 NFS-Ganesha 挂载 CephFS CephFS 可以通过 NFS-Ganesha 使用 NFS 协议发布出去。 准备条件 一个设置好的 CephFS 文件系统 在 NFS 服务器上，安装 libcephfs2、nfs-ganesha 和 nfs-ganesha-ceph (ganesha &gt;= 2.5) 包。 NFS-Ganesha 服务器与 CephFS 网路相连。 安装 NFS-Ganesha 通过包管理器安装： $ sudo yum install nfs-ganesha-fsal-ceph 通过源码安装： $ mkdir -p ~/tmp/ && cd ~/tmp $ git clone https://github.com/nfs-ganesha/nfs-ganesha.git $ mkdir build && cd build $ cmake -DUSE_FSAL_CEPH=ON ../nfs-ganesha $ make && sudo make install 配置 NFS-Ganesha 关闭防火墙或打开 2049 端口： ### turn off firewall $ sudo systemctl disable --now firewalld $ sudo systemctl disable --now iptables ### or open port 2049 $ sudo firewall-cmd --permanent --add-port=2049/tcp $ sudo firewall-cmd --reload 启动 rpcbind 和 rpcstatd 服务： $ sudo systemctl enable --now rpcbind $ sudo systemctl enable --now rpcstatd 复制 Ceph 配置文件到 /etc/ceph/ceph.conf 根据 NFS-Ganesha 的 Ceph 配置例子 ， 创建 /etc/ganashe/ceph.conf，类似如下: NFS_CORE_PARAM { Enable_NLM = false; Enable_RQUOTA = false; Protocols = 4; } NFSv4 { Allow_Numeric_Owners = true; Only_Numeric_Owners = true; Delegations = active; } CACHEINODE { Dir_Chunk = 0; NParts = 1; Cache_Size = 1; } EXPORT { Export_ID = 1; Protocols = 4; Transports = TCP; Path = /; Pseudo = /cephfs/; Access_type = RW; Delegations = RW; # Squash = root; FSAL{NAME = CEPH;} } 启动 NFS-Ganesha 服务： $ sudo ganesha.nfsd -f /etc/ganesha/ceph.conf -L /var/log/ganesha.log -N NIV_CRIT $ sudo showmount -e 这样就把 CephFS 通过 NFS 协议导出了。值得注意的是， 目前一个运行的 Ganesha 进程只能导出一个 CephFS 中的一个目录 ；如果多个 CephFS 或 一个 CephFS 中的多个目录需要导出，需要有相应多个 Ganesha 进程。 NFS 客户端挂载 在 NFS 客户端服务器上挂载 CephFS 到 /cephfs： ### Usage: $ sudo mount -t nfs4 -o nfsvers=4.1,protoc=tcp,rw,notime : ### e.g.: $ sudo mount -t nfs4 -o nfsvers=4.1,proto=tcp,rw,notime :/ /cephfs 这样用户就可以通过 NFS 协议访问 CephFS 了。 ceph-fuse 和 nfsd 挂载 CephFS Ceph 提供了 ceph-fuse 来挂载和访问 CephFS。在 NFS 服务器上通过 ceph-fuse 挂载 CephFS 后，我们可以通过 nfsd 将其发布出去。 NFS 服务端设置 在 NFS 服务器上挂载 CephFS。 在 /etc/exports 中添加如下内容: ### Usage: /path/to/mount * (rw,sync,no_root_squash,fsid=ceph-id) ### example: /cephfs * (rw,sync,no_root_squash,fsid=87761f6e-f839-4597-bfe2-fd850d9a03cf) 运行 exportfs，并启动 rpcbind 和 nfs-server 服务，将 该节点变成 NFS 服务器。 $ sudo exportfs $ sudo systemctl enable --now rpcbind $ sudo systemctl enable --now nfs-server NFS 客户端挂载 跟前面一样，在 NFS 客户端上通过 NFS 协议挂载 CephFS： $ sudo mount -t nfs4 -o nfsvers=4.1,proto=tcp,rw,notime :/cephfs /cephfs 这样用户就可以通过 NFS 协议访问 CephFS 了。 两种方案对比 参考文献 NFS+CephFS 构建基于 Ceph 的 NAS 服务 Ceph 提供 NFS 服务 —— RGW＋nfs-ganesha MPI-IO with CephFS? Exporting Ceph FS Over NFS NFS CephFS Using Ganesha NFS RadosGW Using Ganesha 以 CephFS 为例解析如何在云中提供 NAS 服务 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph cephfs nfs"},{"title":"在 OpenStack 中将虚拟机实例转成启动镜像","url":"/2018/Convert-An-Instance-to-An-Image-in-OpenStack/","text":"在 OpenStack 中，我们对虚拟机实例做了修改后，可以以此实例为模版创建新的虚拟机镜像，其过程也很简单。 创建实例的快照 找到将要作为模版的实例，并将其关掉。 $ source ~/keystonerc_admin $ nova list ... | 8e640acc-6cca-4a4a-8ca5-763150d107bd | my_instance | ACTIVE | - | Running | V2018=192.168.18.20 | ... $ nova stop my_instance 确保实例已经关机。 $ nova list ... | 8e640acc-6cca-4a4a-8ca5-763150d107bd | my_instance | ACTIVE | - | SHUTOFF | V2018=192.168.18.20 | ... 使用 nova image-create 创建实例快照。 $ nova image-create --poll my_instance my_instance_snp Server snapshotting... 100% complete 确保 Nova 镜像处于 ACTIVE 状态。 $ nova image-list ... | 5c8e13b7-2eea-4bf7-9440-c1e7c603380f | my_instance_snp | ACTIVE | 8e640acc-6cca-4a4a-8ca5-763150d107bd | ... 下载快照到本地 获取快照 ID。 $ nova image-list ... | 5c8e13b7-2eea-4bf7-9440-c1e7c603380f | my_instance_snp | ACTIVE | 8e640acc-6cca-4a4a-8ca5-763150d107bd | ... 将快照下载到本地。使用快照 ID 而不是名字。 $ glance image-download --file my_instance_image.qcow2 5c8e13b7-2eea-4bf7-9440-c1e7c603380f 如果需要，将镜像复制到新环境中。 从快照镜像生成镜像 $ glance --os-image-api-version 1 image-create --container-format bare --disk-format qcow2 -name \"my_new_image\" --file my_instance_image.qcow2 ### or $ glance --os-image-api-version 1 image-create --container-format bare --disk-format qcow2 -name \"my_new_image\" --copy_from my_instance_image_url 从新镜像启动新实例 $ nova boot --flavor m1.tiny --image my_new_image my_new_instance 参考 Use snapshots to migrate instances OpenStack – convert an instance to an image document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"openstack virtualization"},{"title":"使用 Pandoc 来制作 HTML 和 beamer 风格的幻灯片","url":"/2018/Using-Pandoc-to-Make-HTML-and-Beamer-Slides/","text":"Pandoc 是一款开源的文档格式转换工具，能够实现大部分的文件格式互转。这里我们主要用来将 Markdown 文档转换成 HTML 和 PDF 风格的幻灯片。 Markdown 语法、结构简单，只需要写出文档的提纲和结构，就能转换成简洁明了又美观的 Slides。 Pandoc - 格式转换利器 Pandoc 是全平台的转换工具，在命令行能够很方便的进行格式转换。 Pandoc 支持的格式及转换方式如下图所示。 更多的可以打开 这里 查看，可以看到 Pandoc 支持的格式是极多的。 Pandoc 的安装是非常简单的， 可按照 说明 很方便地安装到系统中。 文档语法结构 Pandoc 支持标准的 Markdown 语法，但在结构上与一般的用 Markdown 写博客有点区别。 元数据 为了让 Pandoc 能渲染 Markdown 文件生成 Slides， 需要在文档头部加上几行元数据信息。 % Title % Author % Tate 这样就能生成幻灯片的标题页了。 文档结构 一般情况下，一个 Markdown 文档只有一个一级标题。 Pandoc 中，默认情况下，一级标题代表 一个章节， 三个一级标题就是把幻灯片分为三部分；二级标题代表一张幻灯片； 三级标题则一般是幻灯片内部的块。 一个基本的文档结构如下所示 % Title % Author % Tate # Part I ## slide I - list 1 - list 2 ## slide II 1. list 1 2. list 2 # Part II ## slide III [amito](amito.me) ## slide IV ![picture](pic.png) 数学公式 Pandoc 支持转换 TeX 数学公式， 能够将数学公式转换成 MathML， 或者通过 MathJax 显示。 $\\frac{a}{b^2+c^3}$ $$\\frac{a}{b^2+c^3}$$ 显示效果为：$\\frac{a}{b2+c3}$ $$\\frac{a}{b2+c3}$$ 幻灯片样式 在 /usr/share/pandoc-xxx/data 下面有 Pandoc 自带的转换模版， html 幻灯片模版有 dszlides, html, html5, revealjs, s5, slideous, slidy 等等。下面简单介绍几种。 s5 s5 是很流行的 HTML 幻灯片模版，可以从网站上下载后使用，也可以在线使用。本地使用主要是其中的 ui/default, 将其复制到 markdown 所在目录。在线使用需要指定 s5-url #### local $ pandoc -s -t s5 -o s5.html s5.md #### online $ pandoc -s -t s5 -V s5-url=https://cdn.docbook.org/release/xsl-nons/current/slides/s5/ui/default/ -o s5.html s5.md reveal.js reveal.js 是非常火的 HTML 幻灯片库，本身支持 Markdown 语法。 离线使用 reveal.js 需要将其下载下来，在线使用可以指定 revealjs-url。 下载 reveal.js $ git clone https://github.com/hakimel/reveal.js 渲染 markdown 文件生成 HTML 幻灯片，在线使用可以通过指定 revealjs-url 来使用。 $ pandoc -s -t revealjs -o h5.html h5.md $ pandoc -s -t revealjs -V revealjs-url=https://cdnjs.com/libraries/reveal.js/3.6.0 -o h5.htmnl h5.md reveal.js 自带了一些模版可供选择，通过 -V css=/path/to/css 来指定自带的或自己定制的模版， #### local $ pandoc -s -t revealjs -V css=reveal.js/css/theme/night.css -o h5.html h5.md #### online url=\"https://cdnjs.com/libraries/reveal.js/3.6.0/\" $ pandoc -s -t revealjs -V revealjs-url=$url -V css=$url/css/theme/night.css -V -o h5.html h5.md 也可以修改 reveal.js 模版 /usr/share/pandoc-xxx/data/templates/default.revealjs。 $if(css)$ $for(css)$ $endfor$ $else$ $endif$ 改为 $if(css)$ $for(css)$ $endfor$ $endif$ $if(theme)$ $else$ $endif$ 这样就可以简单指定 theme 了。 $ pandoc -s -t revealjs -V theme=night -o h5.html h5.md Beamer 样式 Pandoc 能非常方便地将 markdown 转换为独立的 tex 文档，然后通过 tex 命令 可以生成 pdf 幻灯片。 $ pandoc -s -t beamer -o slide.tex slide.md $ latexmk -pdf slide.tex Beamer 模版 Pandoc 有默认的 beamer 模版，使用 platex 编译，如果不喜欢，可以参考默认模版 /usr/share/pandoc-xxx/data/templates/default.beamer 自己定制一个 beamer 模版，网上也有一些现成的模版，可以参考。 下面是默认模版生成的幻灯片样式。 中文字体 首先查看系统中文字体： $ fc-list :lang=zh 转换时指定字体： $ pandoc -s -t beamer --pdf-engine=xelatex -V CJKmainfont=KaiTi srs.md -o srs.pdf Markdown 文件中加入设置。默认的 beamer 模版中有 CJKmainfont 变量，指定后会使用 xeCJK 包： --- CJKmainfont: STSong CJKoptions: - BoldFont=STHeiti - ItalicFont=STKaiti --- 定制模版，这是比较方便的一种方式，能更改的地方也更多，这里 是我根据 网上 模版修改过的。 参考 Markdown+Pandoc→HTML 幻灯片速成 pandoc does not recognize Chinese characters Pandoc Demo Talks with LaTeX Beamer, written in Markdown PDF slides and handouts using Pandoc and Beamer User contributed templates 用 pandoc 把 markdown 转化为 pdf 文档 纯文本做笔记 — 使用 Pandoc 与 Markdown 生成 PDF 文件 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"latex beamer pandoc"},{"title":"为 Nginx 创建自签名的 SSL 证书","url":"/2018/Generating-A-Self-Signed-SSL-Certificate-for-Nginx/","text":"SSL 及其继任者 TLS 能够加密客户端与服务端的网络连接，保证网络传输信息的安全及完整性，不被他人窃取。有时我们会想要在本机测试加密网络连接，SSL 加密需要有一个 SSL 证书，我们可以为本机生成一个证书来测试。 自签名的 SSL 证书 使用 OpenSSL 创建一个自签名的单域名和泛著名的 SSL 证书很容易，一行命令主就可以。假设我们为本地的多个项目添加了多个 .local 结尾的域名。 $ openssl req -x509 -nodes -days 1024 -newkey rsa:4096 -keyout local.key -out local.crt 运行后的结果如下： $ openssl req -x509 -nodes -days 1024 -newkey rsa:4096 -keyout local.key -out local.crt Generating a 4096 bit RSA private key ..........................................++ ..............++ writing new private key to 'local.key' ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter '.', the field will be left blank. ----- Country Name (2 letter code) [XX]:CN State or Province Name (full name) []:Beijing Locality Name (eg, city) [Default City]:Beijing Organization Name (eg, company) [Default Company Ltd]:local Organizational Unit Name (eg, section) []:local Common Name (eg, your name or your server's hostname) []:local,*.local Email Address []:me@amito.me 各选项含义比较明显，比较重要的是 Common Name， 这是与要用 SSL 加密的域名或服务器 IP 相关的。如果是单域名， 就填 local；要是有多个项目，就填 local,*.local。 知道创建的过程后，就可以一行命令自动生成了。 $ echo -e \"CN\\nBJ\\n\\Beijing\\nlocal\\nlocal\\nlocal,\\*.local\\nme@amito.me\"|openssl req -x509 -nodes -days 1024 -newkey rsa:4096 -keyout local.key -out local.crt 这样我们就在当前目录生成了一个 ssl 证书 local.crt 和对应的密钥 local.key。 创建 RootCA 及自签名证书 生成 Root SSL 证书 生成 Root Key $ openssl genrsa -des3 -out RootCA.key 4096 生成 RootCA 证书 $ openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem 信任 Root SSL 证书 生成 网站 SSL 证书 配置 Nginx 使用 SSL 参考 如何为 Nginx 创建自签名 SSL 证书 https 学习笔记三 ----OpenSSL 生成 root CA 及签发证书 How to get HTTPS working on your local development environment in 5 minutes document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ssl nginx"},{"title":"LVM 调整分区大小","url":"/2018/LVM-Resize-Partitions/","text":"最近在三台服务器上安装了系统，安装的时候选择的是自动分区，创建了一个 LVM 卷组， 结果大部分空间都分给了 HOME ， root 很小。 现在要求要将大部分空间重新分给 ROOT。 常用的文件系统如 ext4，xfs 都能很方便地在线调整大小的。 这里就简单记录一下如何调整分区大小的。 制作一个 Linux LiveCD 启动盘 对系统的 root 分区和 home 分区进行调整大小，可以直接线上调整大小。但是文件系统是 XFS 格式的，不好在线减少空间，就选取了这一个比较稳妥的方法 - 离线调整大小。 下载一个 LiveCD 镜像，然后刻录到 U 盘上， 或者通过 grub2 引导镜像启动。 减小 HOME 分区大小 系统自动分区时创建了一个 LVM 卷组 /dev/centos， 分为 swap、home、root 三个卷。 在 Live 系统中，挂载 home 和 root 卷到： $ sudo mkdir -p /mnt/{home,root} $ sudo mount /dev/centos/root /mnt/root $ sudo mount /dev/centos/home /mnt/home 默认分区格式为 XFS，无法减小大小。需要减小后重新格式化。减小之前做好数据备份。 还好 home 卷内容很少。-_-… $ sudo xfsdump -f /mnt/root/home/home.dump /mnt/home $ sudo umount /mnt/home 减小 home 卷大小 $ sudo lvreduce -L 25G /dev/centos/home $ sudo mkfs.xfs -f /dev/centos/home 恢复 home 数据 $ sudo mount /dev/centos/home /mnt/home $ sudo xfsrestore -f /mnt/root/home/home.dump /mnt/home $ sudo rm -f /mnt/root/home/home.dump 若是 ext4 分区，则要简单的多。 $ sudo lvreduce -L 25G /dev/centos/home $ sudo resize2fs /dev/centos/home ### or simply and safely $ sudo lvresize --resize --size 25G /dev/centos/home 增大 ROOT 分区大小 扩大 root 分区就比较简单了。 LVM 调整大小。 $ sudo lvextend -l +100%FREE /dev/centos/root xfs 分区确认大小调整 $ sudo xfs_growfs /dev/centos/root ext4 分区确认大小调整 $ sudo resize2fs /dev/centos/root 确认调整后的分区大小 $ sudo lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 931G 0 disk ├─sda1 8:1 0 200M 0 part /boot/efi ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 929.8G 0 part ├─centos-root 253:0 0 900.8G 0 lvm / ├─centos-swap 253:1 0 4G 0 lvm [SWAP] └─centos-home 253:2 0 25G 0 lvm /home 重启系统 调整好之后，就可以重启系统了。 在线调整大小 一般分区只能在线扩容，不能减少。鉴于 home 分区没有使用，可以不用重启系统到 Live 系统。 确保没有程序在使用 home 分区后， 备份 home 分区，然后可以按照上面的步骤进行调整大小了。 不过在线调整大小有风险，最好提前做好数据备份。 在 这篇 回答中，提到了一种不重启减小 root 分区的方法，不过，如无必要，建议使用 Live 系统减小 root 分区容量。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"lvm xfs ext4"},{"title":"Ceph 运行中的故障分析与修复","url":"/2018/Ceph-Troubleshooting/","text":"在运行管理 Ceph 集群时，会遇到各种问题。这里记录了一些问题的分析与修复过程。 Ceph HEALTH_WARN clock skew detected 这种问题一般有两个原因： 一是 mon 节点上 ntpd 服务没有启用； 二是 Ceph 设置的时间偏差阈值过小。 如果是 CentOS 7， 确定 systemd-timesyncd 没有启动。 $ sudo ystemctl status systemd-timesyncd ### disable it if enabled $ sudo systemctl disable --now systemd-timesyncd 检查 nptd 服务有没有启用，没有启用或安装就安装并启用。 $ sudo yum install -y ntpd ntpdate $ sudo systemctl enable --now ntpd.service $ sudo systemctl status ntpd.service 有必要的话，修改 /etc/ntp.conf, 然后重启 ntpd 服务。 server your-ntp-server1 server your-ntp-server2 重启 相关的 Ceph 服务。 如果仍有这样的问题，可考虑适当增大 Ceph 的时间偏差阈值。 ### /etc/ceph/ceph.conf mon clock drift allowed = 2 mon clock drift warn backoff = 30 将修改后的 /etc/ceph/ceph 推送给所有节点。 $ for i in {mon list}; do scp /etc/ceph/ceph.conf $i:/etc/ceph/ ssh $i sudo systemctl restart ceph-mon@$i.service done Openstack P: 连不上集群. S: 检查 /etc/ceph/ceph.conf 权限, 设置 ceph 或 cinder 可访问. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph"},{"title":"通过网络远程安装 CentOS 7","url":"/2018/Install-CentOS-7-Remotely-through-Internet/","text":"最近工作中需要将几台服务器重装系统, 将 CentOS 6 换成最新的 CentOS 7.5. 于是研究了一下怎么从 grub lagecy 和 grub2 通过网络来升级或安装 CentOS 7. 从 grub 通过网络安装很容易, 还可以设置 VNC 远程安装. 从 CentOS 6 安装 CentOS 7, 和对 CentOS 7 重装比较类似. 从 CentOS 6 安装 CentOS 7 在启动之前, 我们需要下载 vmlinuz 和 initrd.img 到 /boot/netinstall 文件夹下, 如果哦已经无法进入系统, 可以放在一个可以访问的 U 盘上. CentOS 6 的启动方式是 grub lagecy, 配置文件是 /boot/grub/grub.conf, 我们需要在启动项开始或结束加入一个新的启动项. 如果已经无法进入系统, 在 grub 没有毁坏的情况下, 在启动选择界面 按 c 进入手动输入模式, 输入以下启动命令. title CentOS 7 Install root (hd0,2) kernel /netinstall/vmlinuz ro ip=192.168.1.10 netmask=255.255.255.254 gateway=192.168.1.1 dns=1.1.1.1 method=http://mirrors.ustc.edu.cn/centos/7/os/x86_64 lang=en_US keymap=us ksdevice=eth0 initrd /netinstall/initrd.img 其中, ip, netmask, gateway, dns 是网络连接信息, method 是网络安装包位置. 根据需要修改网络信息和安装位置. 根据 /boot 所在位置和所下载文件位置, 修改相应路径. 如果要利用 vnc 通过远程拖动进行安装, 需要在 kernel 语句后面加上 vnc vncpassword=MyPassword 就可以了. 另外, 如果默认网卡 不是 eth0, 比如eno1 的话, 需要在 kernel 命令行加上 ksdevice=eno1 如果一切没有问题, 之后便会进入图形化安装界面. 如果想自动进行安装, 你可以指定 Kickstart 安装文件. 并在 kernel 语句后面加上文件位置如 ks=http://localfile/centos7.ks, 这样就会自动进行安装了, 当然需要实现准备好 ks 文件. 从 CentOS 7 重装 CentOS 7 重装 CentOS 7 的过程与 从 CentOS 6 安装 CentOS 7 类似. CentOS 7 使用的引导器是 grub2, 启动项参数相应有些改变. menuentry \"CentOS-7-Install\"{set root=(hd0,msdos2) kernel16 /netinstall/vmlinuz ro ip=192.168.1.10::192.168.1.1:255.255.255.254:my_hostname:eth0:none nameserver=1.1.1.1 inst.repo=http://mirrors.ustc.edu.cn/centos/7/os/x86_64/ inst.vnc inst.vncpassword=MyPassword ksdevice=eth0 ks=\"http://where.my.ks.file.is\" inst.lang=en_US inst.keymap=us initrd16 /netinstall/initrd.img } 其中 ip 的格式为 ip::gateway:netmask:hostname:interface:none, 或者 ip=dhcp. 如果 硬盘是 GPT 格式的, 则需要将 (hd0,msdos2) 改成 (hd0,gpt2), 且要加上 inst.gpt 选项. 同样 的，需要注意网卡名 interface 和 ksdevice 一定要正确。 之后重启进入 CentOS 7 安装： grub2-reboot CentOS-7-Install 这样重启后一般就会进入自动化安装 CentOS 7 的程序啦. Kickstart 配置文件 一般安装好 CentOS 或 Fedora 后, 会在 /root 下有一个 anaconda-ks.cfg, 这就是一个 Kickstart 配置文件, 可以参考这个修改成适合自己的安装脚本。RHEL/CentOS 有一个图形化的配置工具 system-config-kickstart 可以简化 Kickstart 文件的生成配置。 下面是一个简单的 ks 配置文件。 #version=DEVEL # System authorization information auth --enableshadow --passalgo=sha512 # Use network installation url --url=\"http://mirror.ihep.ac.cn/centos/7/os/x86_64\" # Use graphical install graphical # Run the Setup Agent on first boot firstboot --enable ignoredisk --only-use=sda # Keyboard layouts # old format: keyboard us # new format: keyboard --vckeymap=us --xlayouts='us' # System language lang en_US.UTF-8 # Network information network --bootproto=static --device=eno1 --gateway=202.122.33.1 --ip=202.122.33.38 --nameserver=202.122.33.44 --netmask=255.255.255.128 --ipv6=auto --activate network --bootproto=dhcp --device=eno2 --onboot=off --ipv6=auto network --bootproto=dhcp --device=eno3 --onboot=off --ipv6=auto network --bootproto=dhcp --device=eno4 --onboot=off --ipv6=auto network --hostname=localhost.localdomain # Root password rootpw --iscrypted $6$7Xg630Hqe96PUCMu$mUzleXYhsnNBxHl.U1AZTGGVSqJ6U/7n2.psJ8nlXg0.MMFcEjngSAytOZhD8yM52d2s3PF7vpojhiF6s0iT2/ # System services services --enabled=\"chronyd\" # System timezone timezone Asia/Harbin --isUtc user --groups=wheel --name=eos --password=$6$VWhRMFl9A7LKcekg$0zZOGfO2ezcfbGPRLbNVsrzylddEmlsFw5WTkZSa/0Oh5IGxVXvljRAn8O/sSFxdtHKiqioSYXXZpJ/ipqOaA/ --iscrypted --uid=1000 --gecos=\"eos\" --gid=1000 # System bootloader configuration bootloader --append=\" crashkernel=auto\" --location=mbr --boot-drive=sda # Partition clearing information clearpart --all --initlabel --drives=sda # Disk partitioning information part /data --fstype=\"ext4\" --ondisk=sda --size=123775 --label=data part swap --fstype=\"swap\" --ondisk=sda --size=8191 part / --fstype=\"ext4\" --ondisk=sda --size=51200 --label=root part /var --fstype=\"ext4\" --ondisk=sda --size=102400 --label=var part /boot --fstype=\"ext4\" --ondisk=sda --size=500 --label=boot %packages @^developer-workstation-environment @base @core @debugging @desktop-debugging @development @dial-up @directory-client @file-server @fonts @gnome-apps @gnome-desktop @guest-desktop-agents @hardware-monitoring @input-methods @internet-applications @internet-browser @java-platform @load-balancer @mariadb @multimedia @network-file-system-client @performance @perl-runtime @print-client @python-web @ruby-runtime @system-admin-tools @virtualization-client @virtualization-hypervisor @virtualization-tools @web-server @x11 chrony kexec-tools %end %addon com_redhat_kdump --enable --reserve-mb='auto' %end %anaconda pwpolicy root --minlen=6 --minquality=1 --notstrict --nochanges --notempty pwpolicy user --minlen=6 --minquality=1 --notstrict --nochanges --emptyok pwpolicy luks --minlen=6 --minquality=1 --notstrict --nochanges --notempty %end 需要特别注意的是，安装位置选择硬盘一定要慎重，一定要确定好所选 硬盘 和分区 方案后，才能进行安装，否则容易造成数据丢失。 参考 How to install CentOS 7 remotely using VNC Redhat Enterprise Linux 6 - Installation Guide - BOOT OPTIONS document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux centos"},{"title":"Linux 运维管理中实的用小工具","url":"/2018/Useful-Tools-for-Linux-Operations/","text":"本文会介绍一些在日常 Linux 运维过程中比较实用的一些小工具。 性能测试类 监控类工具 multitail - 监控多个日志 Multitail 是类似于 tail 的工具，可以在一个 consle 中监控多个文件 $ yum install -y multitail $ alias mtail='multitail' ## -l: 监控命令 $ multitail -l \"ping baidu.com\" ## -i: 监控文件 $ multitail -i /var/log/nginx/access.log -i /var/log/nginx/error.log ## -e: 关键字筛选 $ multitail -e \"Error\" /var/log/eos/mgm/xrd.log.mgm /var/log/ceph/*.log ## -a: 将输出同时复制到文件中 $ multitail -a out.log -e \"Error\" /var/log/eos/mgm/xrd.log.mgm 系统进程 进程实时监控 - htop nmom 网络流量 iptraf 磁盘 IO iotop - 实时监控磁盘 IO $ sudo yum install -y iotop ## usage: $ sudo iotop document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux"},{"title":"Ceph 集群停机维护一般过程","url":"/2018/Ceph-Cluster-Maintenance-Procedures/","text":"Ceph 集群运行久了，需要对其进行停机维护。 而停止 Ceph 服务需要按照一定的流程来进行，以避免 Ceph 集群出现性能问题。 停机维护 关掉所有使用 Ceph 集群的客户端 保证集群的健康状态 (healthy state) 为 OK 设置 OSD flags ## osd no out $ ceph osd set noout ## not backfill data to other osd $ ceph osd set nobackfill ## not recover $ ceph osd set norecover ## 上面一些 flags 对于关掉集群应该已经足够安全了， ## 还可以设置如下的 flags 以完全停止集群 $ ceph osd set norebalance $ ceph osd set nodown $ ceph osd set pause 依次关掉 Ceph 的客户端节点 查看 Ceph Service $ ceph -s cluster: id: 1272c14d-2389-48db-a313-02600a50212f health: HEALTH_OK services: mon: 3 daemons, quorum ceph01,ceph02,ceph03 mgr: ceph03(active), standbys: ceph02, ceph01 mds: cephfs-1/1/1 up {0=ceph03=up:active}, 2 up:standby osd: 18 osds: 18 up, 18 in 依次关掉 Ceph MDS/FUSE 服务 ### MDS && FUSE for i in ceph0{1..3}; do ssh $i \"systemctl stop ceph-fuse@-ceph.service; systemctl stop ceph-mds@${i}.service\" done ### 依次关停 Ceph 的 OSD 服务 ### OSD for i in $(ceph osd status | awk '/ceph/{print $2 \":\" $4}');do idx=$(echo $i|awk -F : '{print $1}') host=$(echo $i | awk -F : '{print $2}') ssh $host \"systemctl stop ceph-osd@${idx}.service\" done 依次关停 Ceph 的 Mon 服务 for i in ceph0{1..3};do ssh $i \"systemctl stop ceph-mon@${i}.service\" done 依次关停 Ceph 的 MGR 服务 for i in ceph0{1..3}; do ssh $i \"systemctl stop ceph-mgr@${i}.service\" done 进行关机等维护工作 开机启动 Ceph 集群 启动 MGR 节点 启动 MOM 节点 启动 OSD 节点 在所有节点启动上线后，确认所有服务都已正常 取消之前设置的 flags $ ceph osd unset noout $ ceph osd unset nobackfill $ ceph osd unset norecover $ ceph osd unset norebalance $ ceph osd unset nodown $ ceph osd unset pause 检查 Ceph 健康状态，确定为 OK。 检查 Ceph 客户端是否能连接上 Ceph 集群 参考 How to do a Ceph cluster maintenance/shutdown [ceph-users] Steps to stop/restart entire ceph cluster document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph linux"},{"title":"EOS 中增加 FST 节点和 FS","url":"/2018/Adding-FST-Nodes-and-Filesystems-on-EOS/","text":"EOS 是 CERN 开发的大规模存储系统， 这篇介绍了怎样在 EOS 中增加 FST 节点和 FS。 增加 FST 节点 在新的 FST 节点上，配置 EOS ### edit /etc/sysconfig/eos[_env] XRD_ROLES=\"fst\" export EOS_BROKER_URL=root://your_eos_mgm_master1:1097//eos/ export EOS_MGM_MASTER1=your_eos_mgm_master1 export EOS_MGM_MASTER2=your_eos_mgm_master2 export EOS_MGM_ALIAS=your_eos_mgm_master_alias 启动 EOS 服务 $ sudo systemctl enable --now eos.service 在 MGM 上添加 FST EOS Console [root://localhsot]|/> node set your_new_fst_addr on EOS Console [root://localhsot]|/> node ls 增加 FS 添加新的 FST 后， 就可以增加新的 FS 了 在新 FST 节点上创建新目录并更改权限 $ sudo mkdir -p /data01 /data02 $ sudo chown -R daemon:daemon /data01 /data02 在 MGM 节点上添加 FS ### Usage: $ eos fs add [-m fsid] : [] [] ### e.g. $ eos fs add -m 13 $(uuidgen) your_new_fst_addr:1095 /data01 default.0 rw 配置启动 FS。如果没有在上一步加上 default.0 rw, 需要手动设置一下 ### Usage: $ eos fs config fsid configstatus=rw $ eos fs boot 1 设置调度群组为 on ### before citrine $ eos group set default on ### after citrine $ eos group set your_group on ### e.g. $ eos group set default.0 on 删除一个 FS $ eos fs config fsid configstatus=empty $ eos fs rm fsid 移动一个 FS $ eos fs config fs-id configstatus=empty $ eos fs mv fs-id target-group ### or $ eos fs mv --force fs-id target-group document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"eos"},{"title":"EOS Geo 异地副本与 XCache 配置与测试","url":"/2018/EOS-GEO-Replicas-and-XCache-Configuration-and-Testing/","text":"EOS 中有基于 GeoTag 的存取策略，可以对 fs 和 client 根据 IP 或强行设定 GeoTag, 令 client 根据 GeoTag 匹配原则选择相应 fs 进行读写副本。 地理位置标签 存储节点标签 ### file: /etc/sysconfig/eos[_env] export EOS_GEOTAG=\"beijing\" ### or export EOS_GEOTAG=\"daocheng\" 客户端标签 $ eos vid -h ... vid set geotag : ... ### default tag $ eos vid set geotag default beijing ### 1.1.*.* daocheng $ eos vid set geotag 1.1. daocheng Geo 调度策略 文件写入 EOS 调度器会随机选择一个 FS 群组，然后尽量根据客户端的 GetTag 匹配一个存储位置。如果要强制要求匹配最近的位置进行写入，需要设置如下 tag： ### enforce location matching geotag when writing $ eos space config default space.geo.access.policy.write.exact=on 注意的是，EOS 会先随机选择一个 fs group。如果该 group 中没有具有与 client 相同 GeoTag 的 fs，EOS 并不会 重新选择 group, 而是直接在该 group 写入文件，具体可看 Geoscheduling test on /eulake。 所以，要想实现 Exact 的异地写入，最好的是所有的 group 都有所有的 GeoTag 的 fst，最好有相同的 geo layout。 文件读取 调度器会尽量将客户端与具有相同 GeoTag 的 replica 位置相匹配。 不保证距离最近的副本会一直被选择 (95%)。 如果要强制进行位置匹配，可以设置如下 tag： ### enforce location matching geotag when reading $ eos space config default space.geo.access.policy.read.exact=on EOS 异地副本的访问性能测试 下面就对 GeoTag 的读写访问性能进行一些简单的测试。 测试准备 EOS 测试环境: @beijing (MGM &amp; FST), @daocheng (FST) EOS 副本设置：beijing &amp; beijing, beijing &amp; daocheng , daocheng &amp; daocheng 双副本 EOS GeoTag 设置：基于 IP， beijing &amp; daocheng 两个 GeoTag EOS 客户端： 一个 @beijing，一个 @daocheng 每种读写测试均进行 10 次，取读写速度的平均值。 测试方案 文件写入 在 beijing 客户端写入测试文件，记录写入速度，检查是否在 daocheng 有副本 在 daocheng 客户端写入测试文件，记录写入速度，检查是否在 beijing 有副本 文件读取 在 beijing 客户端访问测试文件，记录读取速度，检查是否是访问的 beijing 的副本 在 daocheng 客户端访问测试文件，记录读取速度，检查是否是访问的 daocheng 的副本 测试结果 文件写入 客户端以 FUSE 的方式挂载 EOS 到 /eos/ 目录。分别在 beijing 和 daocheng 以 dd 方式写入文件。 $ cd /eos/user/c/ceph/ $ for i in {0..9}; do dd if=/dev/urandom of=test${i}.img bs=1M count=5120;sync; eos file info /eos/user/c/ceph/test${i}.img --fullpath;rm -f test${i}.img; done 测试结果如下，unit MB/s： location Replica Max Min Mean beijing beijing+daocheng 8.0 6.8 7.5 daocheng beijing+daocheng 8.6 7.5 8.0 beijing beijing daocheng daocheng 320? 文件读取 $ for i in {0..9};do xrdcp -f -d 1 ${EOS_MGM_URL}//eos/user/c/ceph/test${i}.img .; rm -f test${i}.img done 测试结果如下，unit MB/s： location Replica target Max Min Mean beijing beijing+daocheng beijing 113.8 111.3 112.55 daocheng beijing+daocheng daocheng 232.7 146.3 166.11 daocheng beijing beijing 10.30 10.08 10.16 beijing daocheng daocheng 测试结论 基于 GeoTag 的调度暂时不会完全匹配相同的 tag，即使设定了强制匹配规则，这可能是 EOS 设置的原因 FST 设置成 1 beijng + 1 daocheng 的 group 模式， 在写入文件时，会保证 beijing &amp; daocheng 双副本 从 daocheng 访问 beijing &amp; daocheng 时，有可能访问到 beijing 的副本， 可能能是 EOS 设置的原因 XrootD Cache Proxy 的访问性能测试 测试准备 EOS 测试环境: @beijing (MGM &amp; FST)，@daocheng (XrootD Cache Proxy) EOS 副本设置： beijing 双副本 EOS GeoTag 设置：基于 IP， beijing &amp; daocheng 两个 GeoTag EOS 客户端：两个 @daocheng， 一个做 XrootD Cache Proxy, 一个用于读写访问 XrootD Cache Proxy 两种模式： Disk Cache and Memory Cache 测试方案 每种读取测试均进行 10 次，取读写速度的平均值，每个文件大小为 512MB 和 5G。 文件读取 在 daocheng 客户端通过代理访问测试文件，记录读取速度 在 daocheng 客户端再次通过代理访问测试文件，记录读取速度 测试结果 使用 Disk Cache 模式第二次读取的时候，对代理和客户端进行了内存缓存清理。 $ sync; echo 3 > /sync; echo 3 > /proc/sys/vm/drop_caches 文件读取 从 daocheng 读取 beijing 的文件 (500MB)： mode 1st Max 1st Min 1rs Mean 2nd Max 2nd Min 2nd Mean Disk Cache 11.38 10.24 10.95 512 512 512 Mem Cache 512? 从 daocheng 读取 beijing 的文件 (5GB) mode 1st Max 1st Min 1rs Mean 2nd Max 2nd Min 2nd Mean Disk Cache 5.095 ? 5.095 ? 5.095? 341.3 301.2 320 Mem Cache 320? 测试结论 Disk Cache 模式是基于路径而不是 md5 或 sha256 等哈希值的文件代理缓存 大文件会缓存到硬盘上，小文件同时会缓存到内存中 文件重命名或移动后，访问新的文件会重新进行缓存 文件被修改或删除后，若有缓存则会直接返回缓存的文件，可能一定时间后才会检查文件状态 不确定是否有自动清理功能，或者要手动进行清理空间 使用 pfc.diskusgage 指定使用空间警戒线 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"eos xcache"},{"title":"Bash 中的输入输出重定向","url":"/2018/STDIN-and-STDOUT-Redirect-in-Bash/","text":"Bash 重定向 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux bash"},{"title":"为鼠须管制作新世纪五笔输入法词库","url":"/2018/Make-Custom-Wubi-06-Word-Stock-For-Rime/","text":"鼠须管 (RIME) 是一个非常好用的跨平台的输入法，支持多种输入方案， 由大神 佛振 开发。RIME 上已经有一些新世纪五笔的词库，但是都有一些不满意的地方。就想着根据已有的词库来制作自己的词库。注意，新世纪五笔是有版权的，归王永民先生所有。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"rime 五笔 鼠须管"},{"title":"DocDB 介绍，安装与配置维护","url":"/2018/DocDB-Introduction-Installation-And-Managment/","text":"DocDB 是一个功能强大且灵活的协作文档服务器。 最初是由 Fermilab 的 BTeV 合作组开发的，现在已用于其他 20 多个实验的文档协作。 DocDB 非常适合用于在几百人的合作组间管理和分享文档。 安装 DocDB DocDB 的安装包括三部分：1). 一个关系数据库，用来存储文档信息, 2). 一个文件系统，用来存储文档本身 和 3). 一个合适的 CGI 脚本，用来提供文档和其信息的一致访问。 DocDB 在 MySQL 数据库中维护着一份文档的版本列表。 存储的信息包括 作者 , 标题 ， 主题 , 事件 , 创建与修改时间 , 修订版本号 , 简介 , 关键字 , 文档类型 , 指向实际文件的指针 , 访问限制信息 等等。 对 DocDB 的访问由 运行在 Web 服务器上的 CGI 脚本控制。 当一个文件上传到 DocDB 时， 文档 (从本地硬盘或 url) 被复制到 Web 服务器下的一个目录， 文档可能由多个文件构成。 文档会被集中复制到一个位置，这样当有人整理或删除文档时，它们就不会消失。这也有助于集中化备份。 对文档的更改会生成一个新版本的文档。此时旧的文档仍可以访问，提供历史存档功能。不同版本有着不同的访问限制，因此文档可以在私有状态进行修改，然后正式发布。 DockDB 包含一个健壮的事件和议程管理系统，可以允许全大小的文档与会议结合起来。 下面就简单介绍一下如何安装 DocDB。 安装配置 MySQL Fedora/CentOS 下可以使用 MariaDB，也可以下载 MySQL 安装 ### Fedora/CentOS $ sudo yum install -y mariadb mariadb-server mariadb-devel ### Ubuntu/Debian $ sudo apt install -y mariadb mariadb-client mariadb-server mariadb-common ### start mariadb server $ sudo systemctl enable --now mariadb ### set root password $ sudo mysql_secure_installation ... 为 DocDB 创建新的 MySQL 用户和数据库： $ mysql -u root -p Enter password: ... ### User MariaDB [(none)]> create user 'docdbadmin'@'localhost' identified by 'admin_password'; MariaDB [(none)]> create user 'docdbrw'@'localhost' identified by 'rw_password'; MariaDB [(none)]> create user 'docdbro'@'localhsot' identified by 'ro_password'; ### Database MaraiDB [(none)]> create database 'docdb'; ### Grant Privileges MariaDB [(none)]> grant all on docdb.* to 'docdbadmin'@'localhost' identified by 'admin_password'; MariaDB [(none)]> grant select,insert,update,delete,create on docdb.* to 'docdbrw'@'localhost' identified by 'rw_password'; MariaDB [(none)]> grant select on docdb.* to 'docdbro'@'localhost' identified by 'ro_password'; 安装 Perl 模块 下载 DocDB 现在 DocDB 主要在 Github 上发布，在 这里 下载最新的版本： $ wget https://github.com/ericvaandering/DocDB/archive/8.8.9.tar.gz -O DocDB-8.8.9.tar.gz document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"docdb mysql"},{"title":"Linux 客户端自动挂载 EOS 文件系统","url":"/2018/Automatically-Mount-EOS-File-System-on-Linux-Clients/","text":"EOS 是 CERN 开发的 PB 级的文件存储系统。在 Linux 客户端上我们可以通过 Autofs (Automount) 来自动挂载 EOS。 Autofs 介绍与结构 Autofs 介绍 Autofs 或者 Automount 是 Linux 下一个非常友好的自动按需挂载文件系统的软件。在 Linux 下， 我们一般地是用 /etc/fstab 来设定挂载点的，这两种方法都可以挂载文件系统，那它们有什么区别呢？ Autofs 与 /etc/fstab /tec/fstab 是用来永久挂载文件系统的，在挂载目录很少时会很有用，但是如果挂载目录多了会对整个系统的性能有影响，所以一般只挂载一些必要的目录。 autofs 则是按需挂载文件系统。 默认地，autofs 挂载的文件系统是处于未挂载状态，只有用户尝试去访问挂载点时，它才会自动挂载该文件系统。当用户一定时间不用该文件系统后，其会被自动卸载。 Autofs 结构 下图展示了 Autofs 的结构： /etc/autofs.conf 是 autofs 的配置文件 /etc/auto.master 是 autofs 的主要挂载点配置文件 /etc/autofs_ldap_auth.conf 是 LDAP 认证文件 /etc/auto.misc 则是 Linux 设备的挂载模板 安装配置 Autofs 安装 Autofs ### CentOS/Fedora $ sudo yum install -y autofs ### Debian/Ubuntu $ sudo apt install -y autofs 配置 /etc/auto.master /etc/auto.master 中记录了自动挂载的目录和配置文件，我们要挂载 EOS， 就在里面添加一项： /eos /etc/auto.eos --timeout=10 其中， /eos 是自动挂载点，我们将 EOS 文件系统挂载到 /eos 目录 /etc/auto.eos 是挂载配置文件，指定了文件系统类型等等 --timeout 是超时设置，如果超过指定时间如 10s 没有使用 该文件系统，它就会被自动卸载 创建 /etc/auto.eos /etc/auto.eos 是我们自己创建的， 配置文件格式一般如下： [host]: 下面是 EOS 文件系统的配置： dev --fstype=eos :dev 对于 NFS 等系统，这时就可以通过启动或重启 autofs 服务来挂载共享目录了，但是对于 EOS， 我们还要做额外的工作。 客户端挂载 EOS 安装 EOS-client 客户端要挂载 EOS， 必须要安装 EOS-client: $ sudo yum install -y eos-client eos-fuse eos-fusex eos-fuse-core eos-fusex-core 配置 /etc/sysconfig/eos.dev 挂载 EOS 的配置文件大致如下： export EOS_FUSE_CACHE=1 export EOS_FUSE_CACHE_PAGE_SIZE=32768 export EOS_FUSE_CACHE_SIZE=268435456 export EOS_FUSE_KERNELCACHE=1 export EOS_FUSE_DEBUG=0 export EOS_FUSE_LOGLEVEL=4 export EOS_FUSE_MGM_ALIAS=YOUR_MGM_HOST export EOS_FUSE_MOUNTDIR=/eos export EOS_FUSE_REMOTEDIR=/eos/user/a/amito export EOS_FUSE_NEG_ENTRY_CACHE_TIME=1.0e-09 export EOS_FUSE_ATTR_CACHE_TIME=0.0000000000000001 export EOS_FUSE_ENTRY_CACHE_TIME=0.0000000000000001 export EOS_FUSE_SYNC=1 export EOS_FUSE_NOPIO=1 export EOS_FUSE_PIDMAP=1 export EOS_FUSE_RDAHEAD=1 export EOS_FUSE_RDAHEAD_WINDOW=262144 export EOS_FUSE_RMLVL_PROTECT=2 export EOS_FUSE_SHOW_SPECIAL_FILES=0 export EOS_FUSE_USER_KRB5CC=0 export EOS_LOG_SYSLOG=0 export XRD_APPNAME=eos-fuse export XRD_CONNECTIONRETRY=4096 export XRD_CONNECTIONWINDOW=10 export XRD_DATASERVERTTL=300 export XRD_LOADBALANCERTTL=1800 export XRD_LOGLEVEL=Info export XRD_REDIRECTLIMIT=5 export XRD_REQUESTTIMEOUT=60 export XRD_STREAMERRORWINDOW=60 export XRD_STREAMTIMEOUT=60 export XRD_TIMEOUTRESOLUTION=1 export XRD_WORKERTHREADS=16 export EOS_FUSE_XATTR_ENOSYS=system.posix_acl_access test -e /usr/lib64/libjemalloc.so.1 && export LD_PRELOAD=/usr/lib64/libjemalloc.so.1 其中需要修改的是 EOS_FUSE_MGM_ALIAS: EOS 的 MGM 节点主机名 EOS_FUSE_MOUNTDIR: 挂载到本机的目录 EOS_FUSE_REMOTEDIR：需要挂载的目录 挂载 EOS 配置完成后，我们就可以挂载 EOS 文件系统了： $ sudo systemctl enable --now autofs.service $ sudo systemctl restart autofs.service document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux eos"},{"title":"配置 EOS 使用 QuarkDB 作为 NS","url":"/2018/Configure-EOS-to-Use-QuarkDB-for-Namespace/","text":"QuarkDB 被设计用来作为 EOS 下一代的 Namespace. 要使用 QuarkDB, EOS 版本必须是 4.4.0 以上. 目前 (2018.11.1) EOS 稳定版本是 4.3.12, github 上的版本则到了 4.4.10. 要安装 4.4.0 以上版本, 可以从 EOS 的 commit 源安装. 本文主要参考 Namespace in QuarkDB configuration. 搭建一个 QuarkDB 集群 为了使用 QuarkDB 作为 EOS MGM 的 namespace, 我们首先要搭建一个 QuarkDB 集群, 可以参考 搭建并运行管理 QuarkDB 集群) 安装好 QuarkDB 并搭建好一个有三个 quardb node 如 qdb-1:7777, qdb-2:7777, qdb-3:7777 的集群. 配置 EOS MGM 使用 QuarkDB 要将 MGM 服务与 QuarkDB 整合到一起, 我们要对 /etc/xrd.cf.mgm 做一些修改. 修改 mgm.nslib 指令, 导入 Namespace 的 QuarkDB 实现. mgm.nslib /usr/lib64/libEosNsQuarkdb.so 列出 MGM 进程连接的 QuarkDB 集群的端点. mgmofs.qdbcluster qdb-1:7777 qdb-2:7777 qdb-3:7777 如果 QuarkDB 启用了密码认证， 则要修改以下选项： mgmofs.cfgtype quarkdb mgmofs.qdbpassword_file /etc/qdb.passwd 其中 /etc/qdb.passwd 是 QuarDB 的密码文件. 4. 以 master 模式运行 MGM 服务. $ systemctl start eos@master $ systemctl start eos@mgm 如果一切正常, 可以看到如下一些进程. $ ps aux|grep xrootd ... 在生产环境下, 为了有更好的性能, 应该: QuarkDB 集群服务 和 MGM 服务运行在 不同 的节点上. 至少 QuarkDB 主进程的 /var/lib/quarkdb/ 文件夹应存储到 SSD 分区. 内存 NS 到 QuarkDB NS 的转化 使用 eos-log-compact 工具压缩 Changelog 文件: $ eos-log-compact /var/eos/md/file.mdlog /var/eos/md/compacted_file.mdlog $ eos-log-compact /var/eos/md/directory.mdlog /var/eos/md/compacted_directory.mdlog 压缩需要将 namespace 整体加载到内存中, 因此压缩 log 的机器必须有 足够大 的内存来保存 namespace 数据结构. 为了更好的性能, 建议 changelog 文件也应该存储在 SSD 分区. 为了加速初始的导入, QuarkDB 有一个特殊的 bulkload 配置模式, 在这种模式下, 我们期待只做的就是向后端写入, QuarkDB 中的数据压缩只发生在导入最后阶段. 因此减少了 I/O 次数, 加速了整个过程. 使用 quarkdb-create 工具创建一个压缩用 QuarkDB 集群, 下面是一个 QuarkDB 配置文件的例子: xrd.port 7777 xrd.protocol redis:7777 /usr/lib64/libXrdQuarkDB.so redis.mode bulkload redis.database /var/lib/quarkdb/convert/ 启动 QuarkDB 集群后, 我们可以使用 eos-ns-convert 工具来进行实际的 namespace 转换过程: $ eos-ns-convert /var/eos/md/compacted_file.mdlog /var/eos/md/compacted_directory localhost 7777 eos-ns-convert 必须使用压缩后的 changelog 文件作为输入. 等 bulkload 完成后, 就可以关掉 QuarkDB 实例, 然后使用 quarkdb-create 在不同的位置建一个新的 QuarkDB 目录, 列出组建 QuarkDB 集群的节点. 新建的 QuarkDB raft-journal 目录可以删除掉. 存储在 /var/lib/quarkdb/convert 的 raft 日志可以利用 cp/scp/rsync 等等复制到新的 QuarkDB 集群的所有实例上. 确保 QuarkDB 集群是运行在 raft 模式而不是 bulkload 模式. 复制完成后, QuarkDB 集群所有实例就可以启动了, 系统会很快达到一个主节点多个副本节点的稳定配置状态. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"eos quarkdb"},{"title":"Redis 5.0 中引入的集群管理新特性","url":"/2018/New-Features-About-Cluster-in-Redis-5/","text":"Redis 5.0 最近 (17/10/2018) 刚刚发布, 其中引入了一些非常令人期待的特性, 其中就有集群管理的新命令。 而在之前, 是由 Ruby 写的 redis-trib.rb 提供了集群管理的相关命令. Redis 5 中的新特性 下面是 Redis 5.0 发行日志中提及的更新和改进: 新的 Stream 数据类型, https://redis.io/topics/streams-intro 新的 Redis 模块 APIs: Timers, Cluster 和 Dictionary RBD 现已支持存储 LFU 和 LRU 信息 redis-cli 中的集群管理功能现从 Ruby (redis-trib.rb) 转换到 C 实现. 通过 redis-cli --cluster help 查看更多信息 新的 有序集合 (sorted set) 命令: ZPOPMIN/MAX 和 阻塞变量(blocking variant) 主动碎片整理 (Active defragmentation) 更新到第 2 版 增强 基数 (HyperLogLog) 实现. 更好的内存报告能力. 许多有子命令的命令现在有了 help 子命令. 在客户端频繁连接和断开连接时有更好的性能 许多错误修复和其他一些改进 Jemalloc 更新到 5.1 版 新引入 CLIENT UNBLOCK 和 `CLIENT ID 加入了 LOLWUT 命令. 见 http://antirez.com/news/123 在不用 API 向后兼容的地方弃用了 slave 一词 网络层不同程度的优化 Lua 解释器改进: 更好地将 lua 脚本传递给 replicas/AOL Lua 脚本现在也可以在 replicas 中超时, 进入 -BUSY 状态 动态的 HZ以平衡闲置 CPU 使用率, 提高响应能力 Redis 核心现已重构并有多处改进 许多其他的改进优化… Redis 中的集群管理 运行 redis-cli --cluster help 可以查看集群管理相关的子命令: $ redis-cli --cluster help Cluster Manager Commands: create host1:port1 ... hostN:portN --cluster-replicas check host:port info host:port fix host:port reshard host:port --cluster-from --cluster-to --cluster-slots --cluster-yes --cluster-timeout --cluster-pipeline rebalance host:port --cluster-weight --cluster-use-empty-masters --cluster-timeout --cluster-simulate --cluster-pipeline --cluster-threshold add-node new_host:new_port existing_host:existing_port --cluster-slave --cluster-master-id del-node host:port node_id call host:port command arg arg .. arg set-timeout host:port milliseconds import host:port --cluster-from --cluster-copy --cluster-replace help For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"sql redis"},{"title":"将 Quda 从 CUDA 转码到 HIP","url":"/2018/Transcode-Quda-from-CUDA-to-HIP/","text":"HIP 是 开源的 AMD 上的异构并行计算框架, 其类似于 Nvidia 的 CUDA. QUDA 则是 Lattice QCD 中广泛使用的 cuda 库. 我们需要将 quda 从 CUDA 平台移植到 AMD 的 HIP 平台. 方便的是, HIP 提供了几个脚本来帮助我们实现这一过程. QUDA from cuda to hip There are tools for transcode quda from cuda to hip - hipify-perl and hipify-cmakefile for source codes and cmake files respectively. Code Portion Details $ sed -i \"s/curand_uniform/hiprand_uniform/g\" include/*.h $ sed -i \"s/curand_normal/hiprand_normal/g\" include/*.h $ sed -i \"s/cuCdiv/hipCdiv/g\" include/*.h $ sed -i \"s/cuCdivf/hipCdivf/g\" include/*.h $ sed -i \"s/curandStateMRG32k3a/hiprandStateMRG32k3a/g\" include/*.h $ sed -i \"s/cuDoubleComplex/hipDoubleComplex/g\" include/*.h $ sed -i \"s/cuFloatComplex/hipFloatComplex/g\" include/*.h $ sed -i \"s|cuComplex.h|hip/hip_complex.h|g\" include/*.h $ sed -i \"s|cudaHostRegisterDefault|hipHostRegisterDefault|g\" include/*.h $ sed -i \"s|CUDA_CUSSESS|hipSuccess|g\" lib/*.cpp $ sed -i \"s|CUresult|hipError_t|g\" lib/*.cpp $ sed -i \"s|cudaHostRegisterDefault|hipHostRegisterDefault|g\" lib/*.cpp $ sed -i \"s|cuMemAlloc|hipMalloc|g\" lib/*.cpp $ sed -i \"s|cuMemFree|hipFree|g\" lib/*.cpp $ sed -i \"s/cuMeMFreeHost/hipFreeHost/g\" lib/*.cpp $ sed -i \"s|CUdeviceptr|hipDeviceptr_t|g\" lib/*.cpp $ sed -i \"s/cudaHostRegisterDefault/hipHostRegisterDefault/g\" lib/*.cpp $ sed -i \"s/cudaIpcEventHandle_t/hipIpcEventHandle_t/g\" lib/*.cpp $ sed -i \"s/cudaEventInterprocess/hipEventInterprocess/g\" lib/*.cpp $ sed -i \"s/cudaIpcOpenEventHandle/hipIpcOpenEventHandle/g\" lib/*.cpp $ sed -i \"s/cudaIpcGetEventHandle/hipIpcGetEventHandle/g\" lib/*.cpp $ sed -i \"s/cuMemcpyDtoH/hipMemcpyDtoH/g\" lib/*.cpp $ sed -i \"s/cuMemcpy/hipMemcpy/g\" lib/*.cpp $ sed -i \"s/cuMemcpyDtoHAsync/hipMemcpyDtoHAsync/g\" lib/*.cpp $ sed -i \"s/cuCtxSynchronize/hipCtxSynchronize/g\" lib/*.cpp $ sed -i \"s/cuEventSynchronize/hipEventSynchronize/g\" lib/*.cpp $ sed -i \"s/cuStreamSynchronize/hipStreamSynchronize/g\" lib/*.cpp $ sed -i \"s/cuStreamWaitEvent/hipStreamWaitEvent/g\" lib/*.cpp $ sed -i \"s/cuEventQuery/hipEventQuery/g\" lib/*.cpp $ sed -i \"s/cuEventRecord/hipEventRecord/g\" lib/*.cpp $ sed -i \"s/CUDA_MEMCPY2D/hip_Memcpy2D/g\" lib/*.cpp $ sed -i \"s/cudaLaunchKernel/hipLaunchKernel/g\" lib/*.cpp $ sed -i \"s/CUDA_ERROR_NOT_READY/hipErrorNotReady/g\" lib/*.cpp sed -i \"s/COMPILE_LANGUAGE:CUDA/COMPILE_LANGUAGE:HIP/g\" lib/CMakeLists.txt adding rocRAND and hipRAND library to CMakeLists.txtinclude_directories(SYSTEM ${rocRAND_HOME}/include/) FIND_LIBRARY(rocrand_LIB rocRAND ${rocRAND_HOME}/lib/) include_directories(SYSTEM ${hipRAND_HOME}/include/) FIND_LIBRARY(hiprand_LIB hipRAND ${hipRAND_HOME}/lib/) cuComplex.h to hip/hip_complex.h: include/complex_quda.h hipEventCreate to hipEventCreateWithFlags: lib/lattice_field.cpp line 369 lib/tune.cpp QUDA_HASH to “” lib/quda_cuda_api.cpp: - line 33 “const void *” to “void *” - line 92 add kind to the end - line 116 remove const - WidthInBytes to widthInBytes - Height to height lib/lattice_field.cpp: comment hipIpcGetEventHandle and hipIpcOpenEventHandle rocRAND download googletest $ git clone https://github.com/google/googletest.git $ mv googletest googletest-src && tar cvjf googletest-src.tar.bz2 googletest-src $ tar xf rocRAND-1.8.0.tar.gz && cd rocRAND-1.8.0 $ mkdir build && cd build $ CC=gcc CXX=g++ cmake3 .. -DCMAKE_HIP_COMPILER_ENV_VAR=hipcc -DHIP_TOOLKIT_INCLUDE=/opt/rocm/hip/include/hip -D__HIP_PLATFORM_HCC__=hcc -DCUDA_cuda_LIBRARY=/opt/rocm/hip/lib/ ### error may occur when download googletest sed -i \"6,9d \" googletest-download/googletest-download-prefix/tmp/googletest-download-gitclone.cmake touch googletest-download/googletest-download-prefix/src/googletest-download-stamp/googletest-download-gitclone-lastrun.txt ### then re-run cmake $ CC=gcc CXX=hcc cmake3 .. -DCMAKE_HIP_COMPILER_ENV_VAR=hipcc -DHIP_TOOLKIT_INCLUDE=/opt/rocm/hip/include/hip -D__HIP_PLATFORM_HCC__=hcc -DCUDA_cuda_LIBRARY=/opt/rocm/hip/lib/ $ make -j4 && make DESTDIR=$HOME/ install cublas include/blas_cublas.h : &lt;cublas_v2.h&gt; -&gt; &lt;rocblas.h&gt; include/cub_helper.cuh: adding #include&lt;rocblas.h&gt; include/cub_helper.cuh: adding #include&lt;thrust/system/cuda/detail/cub/cub.cuh&gt; hip_helpers/forwarder.hpp - device/dispatch/dispatch_histogram.cuh - device/dispatch/dispatch_reduce_by_key.cuh adding hip_helpers/forwarder.hpp from here uncomment line 125 of cub_helper.cuh:// shared bool isLastBlockDone; shared bool -&gt; bool thrust download thrust cub The corresponding version is rocPRIM: $ tar xf amdgpu-target Not finished: -DQUDA_GPU_ARCH=gfx801 $(TOP_DIR)/CMakeLists.txt --arch -> --amdgpu-target PTX quda compiling source /work/soft/profile.d/rocm-1.8.5.sh source /work/soft/profile.d/gcc-8.1.0.env.sh export LD_LIBRARY_PATH=$LD_LIBRAY_PATH:/opt/rocm/rocrand/lib/:/opt/rocm/hiprand/lib:/opt/rocm/lib export C_INCLUDE_PATH=$C_INCLUDE_PATH:/opt/rocm/rocrand/include:/opt/rocm/hiprand/include:/opt/rocm/include export CPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/opt/rocm/rocrand/include:/opt/rocm/hiprand/include:/opt/rocm/include export PATH=$PATH:$HOME/opt/rocm/rocrand/bin:$HOME/opt/rocm/hiprand/bin:/opt/rocm/bin # CC=gcc CXX=hcc cmake3 .. -DCMAKE_HIP_COMPILER_ENV_VAR=hipcc -DHIP_TOOLKIT_INCLUDE=/opt/rocm/hip/include/ -D__HIP_PLATFORM_HCC__=hcc -DCUDA_cuda_LIBRARY=/opt/rocm/hip/lib/ -DCMAKE_HIP_LINK_EXECUTABLE=/opt/rocm/hcc/bin/ -DCMAKE_HIP_CREATE_STATIC_LIBRARY=/opt/rocm/hcc/bin/hipcc_cmake_linker_helper #rm -rf *; CC=gcc CXX=g++ cmake3 .. -DHIP_TOOLKIT_INCLUDE=/opt/rocm/include -DCUDA_cuda_LIBRARY=/opt/rocm/lib/ -DCMAKE_HIP_LINK_EXECUTABLE=/opt/rocm/bin/hipcc_cmake_linker_helper -DCMAKE_HIP_CREATE_STATIC_LIBRARY=/opt/rocm/bin/hipcc_cmake_linker_helper rm -rf *; CC=gcc CXX=g++ cmake3 .. -DHIP_TOOLKIT_INCLUDE=/opt/rocm/include -DCUDA_cuda_LIBRARY=/opt/rocm/lib/ -DCMAKE_HIP_LINK_EXECUTABLE=/opt/rocm/bin/hipcc_cmake_linker_helper -DCMAKE_HIP_CREATE_STATIC_LIBRARY=\"-L/opt/rocm/bin/hipcc_cmake_linker_helper\" Compiling CL2QCD source /work/soft/profile.d/rocm-1.8.5.sh source /work/soft/profiled./gcc-8.1.0.env.sh rm -rf *;CC=gcc CXX=g++ cmake3 .. -DOpenCL_LIBRARIES=\"-L/opt/rocm/opencl/lib/x86_64 -lOpenCL -lcltrace\" -DOpenCL_INCLUDE_DIR=/opt/rocm/opencl/include -DGMP_INCLUDE_DIR=/usr/include -DMPFR_INCLUDE_DIR=/usr/include/ -DLIBXML2_INCLUDE_DIR=/usr/include/libxml2 -DNettle_INCLUDE_DIR=/usr/include -DGMP_LIBRARIES=\"-L/usr/lib64 -lgmp\" -DMPFR_LIBRARIES=\"-L/usr/lib64 -lmpfr\" -DLIBXML2_LIBRARY=/usr/lib64/libxml2.so.2 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"cuda quda hip"},{"title":"Ceph 文件系统 CephFS 的介绍与配置","url":"/2018/CephFS-Introduction-Installation-and-Configuration/","text":"Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server - MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问. Ceph 文件系统简介 CephFS 会在任何可能的情况下使用 POSIX 方法. 例如, CephFS 会在所有客户端之间保持缓存强一致性, 但在某些情况下, CephFS 会与严格的 POSIX 语义有些区别. CephFS 架构 下图展示了 CephFS 的层次结构: 底层是核心集群所依赖的, 包括: OSDs (ceph-osd): CephFS 的数据和元数据就存储在 OSDs 上 MDS (ceph-mds): Metadata Servers, 管理着 CephFS 的元数据 Mons (ceph-mon): Monitors 管理着集群 Map 的主副本 Ceph 存储集群的协议层是 Ceph 原生的 librados 库, 与核心集群交互. CephFS 库层包括 CephFS 库 libcephfs, 工作在 librados 的顶层, 代表着 Ceph 文件系统. 最上层是能够访问 Ceph 文件系统的两类客户端. CephFS 主要特性 CephFS 的限制 配置 CephFS MDS 要使用 CephFS， 至少就需要一个 metadata server 进程。可以手动创建一个 MDS， 也可以使用 ceph-deploy 或者 ceph-ansible 来部署 MDS。 ceph-deploy 部署 MDS $ ceph-deploy mds create mds [mds2 ...] 手动部署 MDS 建立 MDS 元数据文件夹 $ sudo mkdir -p /var/lib/ceph/mds/ceph-{$id} 为提高性能， 可以将该文件夹挂载到 SSD 上。 2. 在 /etc/ceph/ceph.conf 中加入下面一节 [mds.{$id}] host = {hostname} 创建认证密钥 $ sudo ceph auth get-or-create mds.{$id} mon 'profile mds' mgr 'profile mds' mds 'allow *' osd 'allow *' > /var/lib/ceph/mds/ceph-{$id}/keyring 启动 MDS 服务 $ sudo systemctl enable --now ceph-mds@{$id} 查看集群状态： $ ceph -s ... mds: cephfs_a-1/1/1 up {0=c=up:active}, 3 up:standby ... 移除一个 MDS 如果想要移除集群中的一个 MDS，可以通过如下步骤 先创建一个 MDS 停止旧的 MDS, 启用新的 MDS $ ceph mds fail 移除旧 MDS 的数据文件夹 $ sudo rm -rf /var/lib/ceph/mds/ceph-{$id} 部署 Ceph 文件系统 部署一个 CephFS, 步骤如下: 在一个 Mon 节点上创建 Ceph 文件系统. 若使用 CephX 认证, 需要创建一个访问 CephFS 的客户端 挂载 CephFS 到一个专用的节点. 以 kernel client 形式挂载 CephFS 以 FUSE client 形式挂载 CephFS 创建一个 Ceph 文件系统 CephFS 需要两个 Pools - cephfs-data 和 cephfs-metadata, 分别存储文件数据和文件元数据: $ ceph osd pool create cephfs-data 256 256 $ ceph osd pool create cephfs-metadata 64 64 一般 metadata pool 可以从相对较少的 PGs 启动, 之后可以根据需要增加 PGs. 因为 metadata pool 存储着 CephFS 文件的元数据, 为了保证安全, 最好有较多的副本数. 为了能有较低的延迟, 可以考虑将 metadata 存储在 SSDs 上. 2. 安装 ceph-common 包: $ sudo yum install -y ceph-common 创建一个 CephFS, 名字为 cephfs: $ ceph fs new cephfs cephfs-metadata cephfs-data 验证至少有一个 MDS 已经进入 Active 状态: $ ceph fs status cephfs 创建访问 CephFS 和客户端 在 Monitor 上, 创建一个用户: ### usage: $ ceph auth get-or-create client. ### e.g.: $ ceph auth get-or-create client.cephfs mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs-data, allow rw pool=cephfs-metadata' 验证生成的 key： ### usage: $ ceph auth get client. ### e.g.: $ ceph auth get client.cephfs 将 client keyring 复制到 client 节点的 /etc/ceph 目录下，并修改权限： $ sudo scp root@mon1:/etc/ceph/ceph.client.cephfs.keyring /etc/ceph/ceph.client.cephfs.keyring $ sudo chmod 644 /etc/ceph/ceph.client.cephfs.keyring 以 kernel client 形式挂载 CephFS 可以手动用 mount 命令挂载 CephFS 或者通过 /etc/fstab 自动挂载 CephFS. 手动挂载 建立挂载点, 例如 /cephfs: $ sudo mkdir -p /cephfs 挂载 CephFS. 列出多个 Monitors 的地址, 指定 CephX 所需的密钥文件和客户端名, 注意不是 keyring file: $ sudo mount -t ceph mon1:6789,mon2:6789,mon3:6789/ /cephfs -o name=cephfs,secretfile=/etc/ceph/cephfs.secret 验证 CephFS 已经成功挂载: $ stat -f /cephfs 自动挂载 创建挂载点: $ sudo mkdir -p /cephfs 编辑 /etc/fstab 文件: $ echo \"mon1:6789,mon2:6789,mon3:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,_netdev,noatime 0 0\" | sudo tee -a /etc/fstab 重启或者: $ sudo mount -a 以 FUSE client 形式挂载 CephFS 同样地, 可以手动通过 ceph-fuse 挂载或者通过向 /etc/fstab 添加挂载项自动挂载. 将 ceph.conf 从 Monitor 节点复制到客户端节点, $ sudo scp root@mon1:/etc/ceph/ceph.conf /etc/ceph/ceph.conf $ sudo chmod 644 /etc/ceph/ceph.conf 客户端上安装 ceph-fuse: $ sudo yum install ceph-fuse -y 手动挂载 创建挂载点, 例如 /cephfs: $ sudo mkdir -p /cephfs 使用 ceph-fuse 挂载 CephFS, 如果 对应的 keyring 不在 /etc/ceph 下, 则要显示指定: $ sudo ceph-fuse -n client.cephfs [--keyring /etc/ceph/client.cephfs.keyring] /cephfs 验证 CephFS 已经成功挂载: $ stat -f /cephfs 自动挂载 创建挂载点: $ sudo mkdir -p /cephfs 向 /etc/fstab 添加一项: $ echo \"none /cephfs fuse.ceph ceph.id=cephfs[,ceph.conf=/etc/ceph/ceph.conf],_netdev,defaults 0 0\"| sudo tee -a /etc/fstab ### or $ echo \"id=cephfs,conf=/etc/ceph/ceph.conf /mnt/ceph2 fuse.ceph _netdev,defaults 0 0\"| sudo tee -a /etc/fstab 挂载 CephFS: $ sudo mount -a ## or $ sudo reboot 管理 Ceph 文件系统 卸载 Ceph 文件系统 卸载以 kernel clients 形式挂载的 CephFS 跟卸载其他挂载的磁盘一样： ### usage: $ sudo umount ### e.g.: $ sudo umount /cephfs 卸载以 FUSE client 形式挂载的 CephFS ### usage: $ fusermount -u ### e.g.: $ sudo fusermount -u /cephfs CephFS 管理命令 文件系统 创建一个 CephFS。 ceph fs new Ceph 默认只能创建一个 CephFS，若要创建多个，需要设置可创建多个文件系统： ceph fs flag set enable_multiple true 列出所有文件系统： ceph fs ls 导出文件系统元数据。 ceph fs dump 删除文件系统。 ceph fs rm [--yes-i-really-mean-it] 获取文件系统设置。 ceph fs get 更改文件系统设置。 ceph fs set 增加 / 删除数据 Pool ## add data pool ceph fs add_data_pool ## del data pool ceph fs rm_data_pool 设置 更改最大文件大小限制： ceph fs set max_file_size 如果大小设为 0， 不意味着大小没有限制，而是只能创建大小为 0 的文件。最大文件大小过大 兼容性设置。设置客户端最小兼容版本 ## usage ceph fs set min_compat_client ## e.g. ceph fs set cephfs min_compat_client naultilus 上线 / 下线 文件系统 下线 CephFS ceph fs set down true 上线 CephFS ceph fs set down false document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph storage"},{"title":"手动部署一个 Ceph 存储集群","url":"/2018/Deploy-A-Ceph-Cluster-Manually/","text":"Ceph 提供了 ceph-deploy 工具可以很方便地部署一个 Ceph 集群，为了更好理解和管理 Ceph 集群， 我们也可以手动部署一个 Ceph 集群。 所有的 Ceph 集群都心须有一个 monitor，然后有尽可能多的 OSDs 来存储对象。 Monitors 自举是部署 Ceph 集群的第一步, Monitors 的部署也为整个集群设定了非常重要的条件，比如 Pools 副本数量， 每个 OSD 上的 PG 数， 心跳间隔、是否要求认证等等。 大多数的选项都被设置成默认值。 接下来我们会建立一个有 2 个 Monitor nodes 同时作为 OSDs nodes 的 Ceph 集群。 手动部署 Ceph 集群 Monitor 自举 一个 Monitor 自举需要以下几个方面： 唯一标志符 ： fsid 是集群的的一个唯一标志符， 当 Ceph 用做 Ceph FileSystem 时，也代表着 文件系统 ID。现在 Ceph 支持原生接口，块设备，以及对象存储网关接口等等。所以 fsid 现有点用词不当。 集群名字 : Ceph 集群有一个集群名字，是一个没有空格的字符串。 默认的名字是 ceph， 可以指定一个不同的名字。当在管理多个集群时，给集群起一个名字是非常有用的，你需要明确指定你要操作的是哪个集群。 这里我们使用 amito 作为集群名字。 Monitor 名字 : 每一个 monitor 实例都有一个唯一的名字，一般地， Ceph Monitor 名字就是 host 名字。官方推荐 Ceph 的 OSD 实例不要和 Monitor 在同一个 node 上。 Monitor Map： 初始的 Monitors 自举要求我们生成一个 monitor map。 Monitor map 需要 fsid， 集群名字，还有至少一个节点名字与 IP 地址。 Monitor Keyring: Ceph 中 Monitors 是使用 keyring 来进行认证的来互相通信的，所以需要一个 keyring, 我们需要生成提供一个 monitor secret。 Administrator Keyring: 管理节点也需要一个 keyring 来与集群交互, 使用的用户为 client.admin。 前面的要求并不是指 Ceph 配置文件的生成。但为了方便，官方建议创建一个 包含 fsid，mon initial members 和 mon host 的 Ceph 配置文件。步骤如下： 登陆到 初始 monitor 节点上 $ ssh mon-host 确保 /etc/ceph 文件夹存在, 安装 ceph 时会创建这一个文件夹。 $ ls /etc/ceph 创建一个 Ceph 配置文件， 默认为 /etc/ceph/ceph.conf， 这里为 /etc/ceph/amito.conf $ sudo vim /etc/ceph/amito.conf 为集群生成一个唯一 ID (fsid) $ uuidgen > uuid.txt $ cat uuid.txt 17e4176b-29c6-4479-bb8c-72e0918d908c 将这唯一 ID 加到 amito.conf 中。 fsid = 17e4176b-29c6-4479-bb8c-72e0918d908c 添加初始 monitors 到 Ceph 配置文件中 mon initial members = node1, node2 添加初始 monitors 的 IPs 到 Ceph 配置文件中 mon host = 192.168.10.1, 10=92.168.10.2 为集群生成一个 keyring, 并生成一个 monitor secret key $ ceph-authtool --create-keyring /tmp/amito.mon.keyring --gen-key -n mon. -cap mon 'allow *' 生成一个 administrator keyring， 一个 client.admin 用户， 并加到 keyring 中 $ sudo ceph-authtool --create-keyring /etc/ceph/amito.client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' 创建一个 bootstrap－osd keyring, 生成一个 bootstrap-osd 用户，并加到 keyring 中 sudo ceph-authtool --create-keyring /var/lib/ceph/bootstrap-osd/amito.keyring --gen-key -n client.bootstrap-osd --cap mon 'profile bootstrap-osd' 将生成的 keyrings 加到 amito.mon.keyring 中 $ sudo ceph-authtool /tmp/amito.mon.keyring --import-keyring /etc/ceph/amito.client.admin.keyring $ sudo ceph-authtool /tmp/amito.mon.keyring --import-keyring /var/lib/ceph/bootstrap-osd/amito.keyring 使用 节点名 ，IP 地址 ， 和 fsid 生成一个 monitor map， 保存到 /tmp/monmap $ monmaptool --create --add {hostname} {ip-address} --fsid {uuid} /tmp/monmap ### e.g. $ monmaptool --create --add node1 192.168.10.1 --fsid {uuid} /tmp/monmap $ monmaptool --add node2 192.168.10.2 --fsid {uuid} /tmp/monmap 在 monitor 节点上创建默认的数据文件夹 $ sudo mkdir -p /var/lib/ceph/mon/{cluster-name}-{hostname} ### e.g. $ sudo mkdir -p /var/lib/ceph/mon/amito-node1 利用 monitor map 和 keyring 配置 Mon 节点. ### usage: $ sudo ceph-mon [--cluster cluster-name] --mkfs -i {hostname} --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring ### e.g. $ sudo ceph-mon --cluster amito --mkfs -i 192.168.10.1 --monmap /tmp/monmap --keyring /tmp/ceph.mon.keyring 设置 Ceph 配置文件. 如下是一个例子. [global] fsid = 17e4176b-29c6-4479-bb8c-72e0918d908c mon initial members = node1 mon host = 192.168.10.1 public network = 192.168.0.0/24 auth cluster required = cephx auth service required = cephx auth client required = cephx osd journal size = 1024 osd pool default size = 2 osd pool default min size = 1 osd pool default pg num = 256 osd pool default pgp num = 256 osd crush chooseleaf type = 1 创建完成文件. 标记 monitor 已经创建完并可以启动. ### usage: $ sudo touch /var/lib/ceph/mon/{cluster-name}-{hostname}/done ### e.g. $ sudo touch /var/lib/ceph/mon/amito-node1/done 启动 Ceph Monitor. ### systemd $ sudo su -c 'echo \"CLUSTER={cluster-name}\" > /etc/sysconfig/ceph' $ sudo systemctl enable --now ceph-mon@{hostname} ### older debian, centos or rhel $ sudo /etc/init.d/ceph start mon.{hostname} ### e.g. $ sudo systemctl enable --now ceph-mon@node1 查看 Ceph 集群状态. $ ceph -s cluster: id: 17e4176b-29c6-4479-bb8c-72e0918d908c health: HEALTH_OK services: mon: 2 daemons, quorum node1, node2 osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 bytes usage: 0 kB used, 0 kB / 0 kB avail pgs: 设置 Ceph Manager 生成一个认证 Key $ name={hostname} $ ceph --cluster amito auth get-or-create mgr.$name mon 'allow profile mgr' osd 'allow *' mds 'allow *' 将 生成的 Key 放在 mgr data 文件夹, 一般是在 /var/lib/ceph/mgr/{cluster-name}-{name} 2. 启动 ceph-mgr: $ sudo ceph-mgr -i $name 查看 mgr 状态: $ ceph -s ... mgr status: ${name} 一般为了高可用, 每个 mon 节点都应该运行一个 mgr 进程. 添加 OSD 在初始 monitors 开始运行后, 我们就该添加 OSDs 到 集群中了. 集群只有在有足够的 OSDs 来处理对象副本数量时才会达到 active+clean 状态. 自动模式 BLUSTORE 创建 OSD: 联合模式 $ ssh node1 $ sudo ceph-volume lvm create --data {data-path} ### e.g. $ sudo ceph-volume lvm create --data /dev/sdb1 创建 OSD: 分裂模式 #### 1. prepare osd $ sudo ceph-volume lvm prepare --data {data-path} $ sudo ceph-volume lvm list ### e.g. $ sudo ceph-volume lvm prepare --data /dev/sdb2 #### 2. activate osd $ sudo ceph-volume lvm activate {ID} {FSID} ### e.g $ sudo ceph-volume lvm activate 1 17e4176b-29c6-4479-bb8c-72e0918d908c FILESTORE 创建 OSD: 联合模式 $ ssh node1 $ sudo ceph-volume lvm create --filestore --data {data-path} ### e.g. $ sudo ceph-volume lvm create --filestore --data /dev/sdb3 创建 OSD: 分裂模式 #### 1. prepare osd $ sudo ceph-volume lvm prepare --filestore --data {data-path} $ sudo ceph-volume lvm list ### e.g. $ sudo ceph-volume lvm prepare --filestore --data /dev/sdb4 #### 2. activate osd $ sudo ceph-volume lvm activate {ID} {FSID} ### e.g $ sudo ceph-volume lvm activate 4 17e4176b-29c6-4479-bb8c-72e0918d908c 手动模式 登陆到 OSD 节点: $ ssh node1 $ sudo bash 生成一个 UUID $ uuidgen > osd.uuid.txt $ cat osd.uuid.txt 创建一个 CephX key: $ OSD_SECRET=$(ceph-authtool --get-print-key) 创建 OSD. ID=$(echo \"{\\\"cephx_secret\\\": \\\"$OSD_SECRET\\\"}\" | \\ ceph osd new $UUID -i - \\ -n client.bootstrap-osd -k /var/lib/ceph/bootstrap-osd/ceph.keyring) 创建 OSD 目录文件. $ mkdir /var/lib/ceph/osd/ceph-$ID 添加 MDS 创建 MDS 数据文件夹. id 为任意名字, 一般取为 hostname. $ mkdir -p /var/lib/ceph/mds/{cluster-name}-{id} 生成 Keyring. $ ceph-authtool --create-keyring /var/lib/ceph/mds/{cluster-name}-{id}/keyring --gen-key -n mds.{id} 导入 Key 并设置 caps. ceph auth add mds.{id} osd \"allow rwx\" mds \"allow\" mon \"allow profile mds\" -i /var/lib/ceph/mds/{cluster}-{id}/keyring 将设置写入 Ceph 配置文件 [mds.{id}] host = {id} 手动启动 mds ### usage: $ ceph-mds --cluster {cluster-name} -i {id} -m {mon-hostname}:{mon-port} [-f] ### e.g. $ ceph-mds --cluster ceph -i node1 -m {mon-hostname}:{mon-port} -f 或者使用 systemd 或者 sysinitv 启动: ### usage: sudo systemctl enable --now ceph-mds@id 7. 安装 Ceph FileSystem 总结 在设置了 monitor, manager 和 OSDs 后, 可以查看 Ceph 集群的状态: $ ceph -s $ ceph -w $ ceph osd tree 重命名 Ceph 集群 通过上面的手动创建过程可以看到, Ceph 集群名字更多是一个标志名, 没有进行硬编码. 所以可以对 Ceph 集群进行重命名. 具体过程就是将所有的文件夹和文件重命名, 例如从 ceph 到 remote. 改完后重启整个集群. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph linux"},{"title":"在 Linux 中使用 Cron 执行定时任务","url":"/2018/Using-Crontab-in-Linux/","text":"工作中我们常常需要在某些时候执行一些任务，Windows 下有定时任务管理， Linux 下则有 Crontab， 接触过 Linux 的童鞋一定不陌生。 Cron 是 Unix，Solaris， Linux 等系统下的可以自动在后台执行定时任务的工具，比如在某一天 9 点提醒你开会， 某一天 6 点去给女票买礼物等等， 这是由 cron 守护进程定期执行的。 如果启用了 cron（一般默认启用 ), cron 守护进程 会在系统启动后自动启动， 并按时执行任务。 cron 也能手动启用、关闭、重启等。 认识 crontab Crontab 是 cron 的任务库文件，它存储了 cron 要执行的计划任务项目。 不同系统下该文件的位置可能不同。 macOS: /usr/lib/cron/tabs/ BSD Unix: /var/cron/tabs/ Solaris, Debian: /var/spool/cron/crontabs/ RHEL, CentOS, Fedora: /var/spool/cron/ cron 任务是一个包含特定日期，特定时间和要执行的命令的集合, 存储在 crontab 里。一个 crontab 条目如下图所示，指定了在每个月的所有周日凌晨 2 点 36 分 执行 /usr/local/sbin/ 下的 backup.sh 脚本。 Crontab 命令 crontab 命令很简单， [root@localhost ~]$ crontab [-u username] [-l|-r|-e] [-n |-c] [-s] [-x ] 各选项含义如下： -u: 指定 crontab 的用户名, 此选项只能 root 使用 -e: 编辑 crontab 文件， 如果没有，会新建一个 -l: 打印出 crontab 文件内容，列出计划任务 -r: 移出 crontab 文件 -n : 设定在 集群中的 host 主机上执行用户 crontabs -c: 获取用户在集群上的 crontabs -s: selinux 相关内容 -x : 开启 debug 执行 crontab -e 后就会进入 crontab 编辑环境。 # # Example of job definition: .---------------- minute (0 - 59) | .------------- hour (0 - 23) | | .---------- day of month (1 - 31) | | | .------- month (1 - 12) OR jan,feb,mar,apr ... | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat | | | | | * * * * * command to be executed 时间格式为 min hour day_of_month month day_of_week, 每一项都是一定范围的整数或者数组或者范围。 一些辅助的字符有特别的意义。 特殊字符 代表含义 * 代表任何可能的值 , 代表分隔时段， 0 3,6 * * * command 表示在每天 3:00， 6:00 执行任务 - 代表时间范围， 0 10-20 * * * command 表示在每天 10:00 到 20:00 执行任务 /n 代表 每隔 n 单位时间 重复一次 下面举几个 Crontab 的使用例子。 Crontab 例子 备份系统 每天 00:00 准时备份系统 0 0 * * * /usr/local/bin/backup.sh 会议提醒 提醒每周五下午 2:30 开会 20 14 * * 5 /path/to/alert.sh 记录 IP 每隔 3 小时记录 IP 并发送到指定邮箱 0 */3 * * * echo \"$(date) : $(hostname -I)\" | mutt -s \"your ip\" your_email_address > /dev/null 2>&1 Crontab 使用补充 系统设定 如果你要做的是系统的例行性任务，则可以使用 /etc/crontab, 在里面加入你的定时任务就好。 [root@localhost ~]$ cat /etc/crontab SHELL=/bin/bash /path/to/your/log/file 2>&1 环境变量 cron 会在用户 HOME 文件夹下执行命令，默认的环境变量有： HOME=user-home-directory LOGNAME=user-login-id PATH=/usr/bin:/usr/sbin SHELL=/usr/bin/sh 要是你想要任务开始时执行 ~/.profile 里的命令， 需要显示地在脚本里执行 source ~/.profile 安全检测 有可能服务器会被人入侵，植入的木马一般都会以定时任务的形式运行，可以检查 /var/spool/cron/ 有没有问题。 周与日 day_of_month 与 day_of _week 一般不能同时设定，若同时设定了，系统会在所有符合一个或两个的日期都执行命令。 执行关机期间的任务 如果关机有任务没有执行， 开机后 cron 也不会执行过期的任务。想要执行过期任务，可以使用 anacron。 它作为定时会每小时被执行一次， 执行 cron 没有执行的任务。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux shell"},{"title":"使用 Ceph 作为 QEMU KVM 虚拟机的存储","url":"/2018/Using-Ceph-RBD-as-A-Backend-for-QEMU-KVM-Instances/","text":"Ceph RBD 块设备存储是十分适合作为虚拟化平台的存储后端的, 这也是其最常用的情景之一. 这篇主要介绍如何将 Ceph RBD 作为 QEMU 等的存储端. RBD 具有快照的特性, 我们又可以对 RBD 快照进行克隆, 创建一系列的写时复制克隆复本. RBD 的这一特性可以让 Ceph 能够很快地给虚拟机提供块设备, 因为客户端不需要每次在新建虚拟机时都下载整个镜像. libvirt 为 OpenStack, CloudStack 等云计算平台提供 Ceph 块设备服务, 云计算平台使用 libvirt 与 QEMU/KVM 等交互, QEMU/KVM 等使用 librbd 与 Ceph RBD 块设备交互. Ceph RBD 可以作为 VMs 的数据盘和系统盘来使用. 下面我们就来看看如何实现这一目标. 配置 Ceph 要配置 Ceph 来配合 libvirt, 可依照如下步骤. 创建一个 Pool. 专门为 libvirt 创建一个 Pool, 并初始化. [ceph@ceph0 ~]$ ceph osd pool create libvirt-pool 128 128 [ceph@ceph0 ~]$ ceph osd lspools [ceph@ceph0 ~]$ rbd pool init libvirt-pool 创建与 Ceph 交互的 User. 这里使用 client.libvirt 作为用户名, libvirt 会使用 libvirt 而不是 client.libvirt 与 Ceph 交互. [ceph@ceph0 ~]$ ceph auth get-or-create client.libvirt mon 'profile rbd' osd 'profile rbd pool=libvirt-pool' [ceph@ceph0 ~]$ ceph auth ls 使用 QEMU 新建 images. 比如新建一个 linux-image, 一个 windows-image. [ceph@ceph0 ~]$ qemu-img create -f rbd rbd:libvirt-pool/linux-image 60G [ceph@ceph0 ~]$ qemu-img create -f rbd rbd:libvirt-pool/windows-image 80G [ceph@ceph0 ~]$ rbd -p libvirt-pool ls 添加如下部分到 /etc/ceph/ceph.conf 中, 以便调试错误. (可选) [client.libvirt] log file = /var/log/ceph/qemu-guest-$pid.log admin socket = /var/run/ceph/$cluster-$type.$id.$pid.$cctid.asok 配置 VM Managers libvirt 可以直接创建 VMs, 但是使用 virt-manager 可能麦加方便一些. 安装 VM manager. [ceph@ceph0 ~]$ sudo yum install -y virt-manager 准备好 OS images. 这里我们用 CentOS 7.5 和 Windows 7 64 bit 两个系统. 启动 VMs manager. [ceph@ceph0 ~]$ virt-manager 接下来我们就开始安装 VMs 了. 创建 VMs 创建 Linux VMs 点击 Create New Virtual Machine 按钮. 命名 虚拟机 的域. 例如 centos. 导入 OS image. 配置启动 VM 要使用 Ceph RBD 作为系统盘, 先不要安装系统, 要使用 Ceph RBD 作为数据盘, 可以先安装系统.VM. 接下我们先关掉 VM. 配置 Linux VMS 打开 VM 配置文件. sudo virsh edit centos 在 &lt;devices&gt; 下应该有一个 &lt;disk&gt; 项. /usr/bin/kvm 如果使用 RBD 作为数据盘, 在其下面加上下面项. 如果用作系统盘, 去掉上面的一项, 再加上下面一项. 将其中的 {monitor-host} 替换成真实的 hosts. 保存并退出. 3. 配置 Ceph Authentication. 一般 Ceph 集群都设置了登录认证. 我们需要生成一个 secret. cat > secret.xml { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph kvm qemu"},{"title":"解决 SSH 连接提示 Too Many Authentication Failures 问题","url":"/2018/Solving-SSH-Too-Many-Authentication-Failures/","text":"最近在使用 ssh 连接远程服务器时, 出现了 “Too many authentication failures for xxxx” 的问题, 然而之前是可以正常登录的. 貌似服务器也没有做什么更改. 那是什么原因呢? 问题 在登录远程服务器时, 会出现如下提示: Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks. Received disconnect from 172.16.22.11 port 22:2: Too many authentication failures for xxx Disconnected from 172.16.22.11 port 22 显示指定所用的公钥后, 仍旧不行. 在加上 -vvv 输出 debug 信息后, 发现 ssh 在登录时, 尝试了 ~/.ssh 下的所有密钥, 很不幸的是, 服务器用的密钥很靠后, 还没有尝试到该密钥就已经超出限制了. ssh th2 -vvv OpenSSH_7.8p1, OpenSSL 1.1.0i-fips 14 Aug 2018 debug1: Reading configuration data /home/user/.ssh/config debug1: /home/user/.ssh/config line 4: Applying options for * debug1: /home/user/.ssh/config line 7: Deprecated option \"cipher\" debug1: /home/user/.ssh/config line 111: Applying options for th2 debug1: Reading configuration data /etc/ssh/ssh_config ... SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx. Please contact your system administrator. Add correct host key in /home/user/.ssh/known_hosts to get rid of this message. Offending ECDSA key in /home/user/.ssh/known_hosts:121 Password authentication is disabled to avoid man-in-the-middle attacks. Keyboard-interactive authentication is disabled to avoid man-in-the-middle attacks. ... debug2: key: /home/user/.ssh/ceph_rsa (0x55d8ec4dd860), agent debug2: key: /home/user/.ssh/amito_rsa (0x55d8ec4d8c30), agent debug2: key: aaaa (0xxxxxxxxxxxxx), agent debug2: key: bbbb (0xxxxxxxxxxxxx), agent debug2: key: cccc (0xxxxxxxxxxxxx), agent debug2: key: dddd (0xxxxxxxxxxxxx), agent debug2: key: /home/user/.ssh/xxxx ((nil)), explicit ... debug2: we did not send a packet, disable method debug3: authmethod_lookup publickey debug3: remaining preferred: debug3: authmethod_is_enabled publickey debug1: Next authentication method: publickey debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx /home/user/.ssh/eeee_rsa debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 51 debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx /home/user/.ssh/aaaa_rsa debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 51 debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx bbbb debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 51 debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx cccc debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 51 debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx eeee debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 51 debug1: Authentications that can continue: publickey,gssapi-keyex,gssapi-with-mic debug1: Offering public key: RSA SHA256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ffff debug3: send packet: type 50 debug2: we sent a publickey packet, wait for reply debug3: receive packet: type 1 Received disconnect from 172.16.22.11 port 22:2: Too many authentication failures Disconnected from 172.16.22.11 port 22 原来 SSH 在登录时, 会依次尝试密钥, 直到找到一个能成功的. 然而 SSH 服务端会设置几次登录失败后就将其禁掉. 找到了原因, 解决方法也简单… 方法 一个简单的方法是将多的密钥移除, 但是密钥自有其存在的意义, 使用同一个密钥登录所有服务器也不现实. 另一个方法就是在 ~/.ssh/config 中指定使用的密钥, 并且加上 IdentitiesOnly=yes. Host myhost Hostname my_host_ip User user Port port Identitiesonly=yes Identityfile ~/.ssh/id_rsa 为简便, 可以对所有 host 都进行设置. Host * Serveraliveinterval 60 StrictHostKeyChecking no Identitiesonly=yes document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux ssh"},{"title":"使用 CBT 来对 Ceph 集群进行性能测试","url":"/2018/Using-CBT-to-Benchmark-Your-Ceph-Cluster/","text":"CBT document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph linux"},{"title":"在 CentOS 7 下安装与配置 EOS 存储服务","url":"/2018/Install-and-Configure-EOS-on-CentOS-7/","text":"EOS 是欧洲核子中心开发的专门用来提供可靠的 PB 级数据存储技术的软件解决方案, 目前在 LHC 和其他一些场景都有实例在运行. EOS 的核心是 XrootD 框架, 其提供了非常多的远程访问协议。本文是 EOS 的一个简单安装配置过程。 下图展示了 EOS 的发展历史： 准备工作 为了要创建我们的 EOS 服务， 我们至少需要两个服务器节点， 一个作为 EOS MGM 主节点， 一个为 EOS MGM 从节点。这里我们将 CentOS7 作为我们的服务器操作系统。 安装 EOS Cern 提供已经编译好的 EOS 安装包和依赖包， 我们也可以自己编译 EOS 安装包和依赖包。 从源中安装 EOS 从 Cern 的 EOS 源中安装 EOS 是最方便的。 创建一个 yum repo， 内容如下： [eos-citrine] name=EOS binaries from EOS project, citrine branch baseurl=http://cern.ch/storage-ci/eos/citrine/tag/el-$releasever/$basearch/ enabled=1 gpgcheck=0 priority=10 [eos-citrine-deps] name=EOS dependencies from EOS project, citrine branch baseurl=http://cern.ch/storage-ci/eos/citrine-depend/el-$releasever/$basearch/ enabled=1 gpgcheck=0 priority=10 并复制到 /etc/yum.repos.d/ $ sudo yum makecache && sudo yum update $ sudo yum install -y eos-folly eos-server eos-client eos-fuse eos-fusex 目前 EOS 稳定版本是 citrine, 版本号为 4.3.12。 手动编译 EOS RPM 包 安装依赖库 $ sudo yum install -y yum-utils rpm-build rpm-build-libs yum-plugin-priorities $ sudo yum install -y zlib-static zlib-devel bzip2-devel bzip2-devel #### Download from cern $ sudo yum install -y http://linuxsoft.cern.ch/cern/centos/7/cern/x86_64/Packages/protobuf3-3.3.1-2.el7.cern.x86_64.rpm $ sudo yum install -y http://linuxsoft.cern.ch/cern/centos/7/cern/x86_64/Packages/protobuf3-compiler-3.3.1-2.el7.cern.x86_64.rpm $ sudo yum install -y http://linuxsoft.cern.ch/cern/centos/7/cern/x86_64/Packages/protobuf3-devel-3.3.1-2.el7.cern.x86_64.rpm #### eos-server dependence $ sudo yum install -y openldap-devel e2fsprogs-devel libmicrohttpd libmicrohttpd-devel #### libmicrohttpd dependence $ sudo yum install -y graphviz graphviz-devel libgcrypt-devel gnutls-devel 安装 devtoolset-7 #### centos $ sudo yum install -y centos-release-scl.noarch #### scientific linux $ sudo yum install -y yum-conf-repos $ sudo yum install -y yum-conf-softwarecollections ####-------------------------------------------- $ sudo yum install -y devtoolset-7-gcc devtoolset-7-gcc-c++.x86_64 devtoolset-7-gcc-plugin-devel.x86_64 $ sudo yum install -y devtoolset-7-gcc-gfortran.x86_64 devtoolset-7-gdb.x86_64 devtoolset-7-strace.x86_64 devtoolset-7.x86_64 #### enable devltoolset $ scl enable devtoolset-7 bash 编译 eos-folly $ git clone https://gitlab.cern.ch/eos/eos-folly && cd eos-folly $ mkdir -p ~/rpmbuild/SOURCES $ cp SConstruct.double-conversion ~/rpmbuild/SOURCES/ $ sudo yum install python2-scons openssl-devel libevent-devel cmake3 cmake devtoolset-6 $ sed -i \"s|devtoolset-6|devtoolset-7|g\" eos-folly.spec $ rpmbuild --undefine=_disable_source_fetch -ba eos-folly.spec 编译 eos-rocksdb $ git clone https://gitlab.cern.ch/eos/eos-rocksdb && cd eos-rocksdb $ rpmbuild --undefine=_disable_source_fetch -ba eos-rocksdb.spec 编译 EOS 下载源码包： #### Download eos from github $ git clone https://gitlab.cern.ch/eos/eos $ mv eos eos-citrine-4.3.12-1 $ tar cvzf eos-citrine-4.3.12-1.tar.bz2 eos-citrine-4.3.12-1 $ mv eos-citrine-4.3.12-1.tar.gz ~/rpmbuild/SOURCES/ $ cd eos-citrine-4.3.12-1 && cp eos.spec.in eos.spec $ sed -i -e \"s|@CPACK_PACKAGE_RELEASE@|1|g\" -e \"s|@CPACK_PACKAGE_VERSION_MAJOR@|4|g\" eos.spec $ sed -i -e \"s|@CPACK_PACKAGE_NAME@|eos-citrine|g\" -e \"s|@CPACK_PACKAGE_VERSION|4.3.12|g\" eos.spec #### depends $ sed -i \"s|2017.09.18.00-4|2017.09.18.00-5|g\" eos.spec #### without server $ rpmbuild --undefine=_disable_source_fetch --with server -ba eos.spec #### with server ## build libmicrohttpd $ cd microhttpd $ cp vmem.patch epoll.patch fdshift.patch ~/rpmbuild/SOURCES/ $ rpmbuild --undefine=_disable_source_fetch -ba libmicrohttpd.spec $ sudo rpm -ivhU ~/rpmbuild/RPMS/x86_64/libmicrohttpd-* && cd .. ## build eos with server $ rpmbuild --undefine=_disable_source_fetch --with server -ba eos.spec 编译好后, 安装相应的包即可. 配置 EOS 服务 EOS 有三个基本守护进程, 作为 OFS 插件跑在 xrootd 服务内. MGM - 管理服务器 (名字空间, 存储池配置, 布局与读取调度) MQ - 无状态异步消息传递服务器 FST - 文件存储服务器, 根据文件 ID 存储文件 对于 名字空间和配置有副本的情况, 还有第四个守护进程 4. SYNC - 变更日志与配置文件同步服务器 复制一份配置文件到 /etc/sysconfig/eos[_env] ### for SL 6 $ cp /etc/sysconfig/eos.example /etc/sysconfig/eos ### for SL 6 $ cp /etc/syconfig/eos_env.example /etc/sysconfig/eos_env 配置 MGM 更改 /etc/sysconfig/eos[_env] 修改如下相应的量变: ### define roles of daemons to run on this node ### here for management, message broker, sync and fed serfice XRD_ROLES=\"mq mgm fed sync global-mq\" ### EOS instance name EOS_INSTANCE_NAME=eostest ######################## EOS_MGM_HOST=localhost EOS_BROKER_URL=root://localhost:1097//eos/ EOS_MGM_MASTER1=eos01.amito.me EOS_MGM_MASTER2=eos02.amito.me EOS_MGM_ALIAS=eos01.amito.me EOS_FUSE_MGM_ALIAS=eos01.amito.me 修改 /etc/xrd.cf.mgm 按需修改 /etc/xrd.cf.mgm 中如下的配置 # sec.protocol krb5 # sec.protocol gsi sec.protocol unix sec.protocol sss -c /etc/eos.keytab -s /etc/eos.keytab # sec.protocol krb5 -exptkn:/var/eos/auth/krb5# host/@CERN.CH # sec.protocol krb5 host/@CERN.CH sec.protbind localhost.localdomain unix sss sec.protbind localhost unix sss sec.protbind * unix sss ### if using quarkdb as ns mgmofs.cfgtype quarkdb mgmofs.nslib libEosNsQuarkdb.so mgmofs.qdbcluster ceph0:7777 ceph1:7777 ceph2:7777 ceph3:7777 ###### if quarkdb using password ###### /etc/quarkdb.passwd.eos permission 400 mgmofs.qdbpassword_file /etc/quarkdb.passwd.eos 生成 eos.keytab 不然会出问题: $ test -f /etc/eos.keytab && rm -f /etc/eos.keytab $ yes | xrdsssadmin -k eos -u daemon -g daemon add /etc/eos.keytab >& /dev/null $ chown daemon:daemon /etc/eos.keytab >& /dev/null ; chmod 400 /etc/eos.keytab 设置 Master/Slave ### Master $ systemctl enable --now eos@master.service ### Slave $ systemctl enable --now eos@slave.service ### on master and slave nodes $ systemctl enable --now eos.service $ systemctl enable --now eos@mgm.service ### enable compact $ eos -b ns compact on 1 86400 启动 EOS ### start $ systemctl start eos ### status $ systemctl status eos@* ### stop $ systemctl kill eos NTP 时间同步 增加定时任务, 每半个小时同步一次时间: */30 * * * * /usrsbin/ntpdate -s pool.ntp.org >/dev/null 2>&1 配置 FST FST 节点的配置相对简单一点. 修改 /etc/sysconfig/eos[_env] ### define roles of daemons to run on this node ### here for management, message broker, sync and fed serfice XRD_ROLES=\"fsd\" ######################## EOS_BROKER_URL=root://eos01.amito.me:1097//eos/ EOS_MGM_MASTER1=eos01.amito.me EOS_MGM_MASTER2=eos02.amito.me EOS_MGM_ALIAS=eos01.amito.me EOS_FUSE_MGM_ALIAS=eos01.amito.me 启动 FST $ systemctl enable --now eos $ systemctl enable --now eos@fst $ systemctl status eos@fst 设置存储位置 $ mkdir /data01 && chown -R daemon:daemon /data01 启用 FST 在 MGM 上, 进入 EOS 控制台 $ eos EOS Console [root://localhost]|/> node set fst01.amito.me on ### make sure fst01.amito.me is on EOS Console [root://localhost]|/> node ls ### a3bxxx is uuid of the partition EOS Console [root://localhost]|/> fs add -m 1 a3b0d210-4cce-413f-91ed-13ee9a5b9dc4 fsd01.amito.me:1095 /data01 #### EOS 认证 EOS Console [root://localhost]|/> vib enable unix EOS Console [root://localhost]|/> vib enable sss #### FUSE 配置 EOS Console [root://localhost]|/> vid set map -tident \"*@fs.amito.me\" vuid:0 EOS Console [root://localhost]|/> vid set map -tident \"*@fs.amito.me\" vgid:0 EOS Console [root://localhost]|/> vid add gateway login* unix EOS Console [root://localhost]|/> vid add gateway vm* unix #### 启动 FST EOS Console [root://localhost]|/> fs boot 1 EOS Console [root://localhost]|/> fs config 1 configstatus rw EOS Console [root://localhost]|/> group set default on 配置 FED $ mkdir -p /var/spool/xrootd/fed $ sudo chown -R daemon:daemon /var/spool/xrootd/fed 配置 QuarkDB NS 参考 配置 EOS 使用 QuarkDB 作为 NS。 EOS 故障分析与解决 配置 Unable process keytable /etc/eos.keytab; No such file or directory $ chmod 600 /etc/eos.keytab 2>/dev/null; unlink /etc/eos.keytab 2>/dev/null; yes | xrdsssadmin -k eos -u daemon -g daemon add /etc/eos.keytab >& /dev/null; chown daemon:daemon /etc/eos.keytab >& /dev/null ; chmod 400 /etc/eos.keytab;\" 主从 MGM 节点通信出现 Decrypt key not found, 可能是 /etc/eos.keytab 不一致导致的。 [root@mgm-master-node ~]$ scp /etc/eos.keytab mgm-slave-node:/etc eos vid ls 出现如下权限错误的问题： error: errc=3010 msg=\"Unable to execute proc command - you don't have the requested permissions for that operation (2) /proc/admin/; Operation not permitted\" 通过查看用户 ID， $ eos whoami Virtual Identity: uid=99 (99) gid=99 (99) [authz:unix] host=ceph1 发现 host 不是 localhost， /etc/hosts 中对 127.0.0.1 做了修改 $ cat /etc/hosts 127.0.0.1 ceph1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 ceph1 localhost localhost.localdomain localhost6 localhost6.localdomain6 去掉 ceph1， 将其改回来，就可以了。 4. /var/log/eos/tx/transfer-archive.log not found $ mkdir -p /var/log/eos/tx && chown -R daemon:daemon /var/log/eos document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"storage eos"},{"title":"Ceph 集群的性能测试","url":"/2018/Ceph-Cluster-Performance-Benchmark/","text":"在 Ceph 集群搭建好后，我们想要知道我们的集群的性能到底怎样， 有没有以最好性能运行。以下几种方法可以粗略地测试集群的性能。 获取基准性能统计数据 硬盘性能基准 最常见的也最简单的测试就是 dd，测试时记得忽略硬盘的 page cache。 $ dd if=/dev/zero of=test bs=100M count=100 oflag=direct 如下图所示，这是在本机上的结果，最终给出的 157MB/s 即是硬盘的写入性能。 在集群上所有的硬盘上进行这一测试， 记录下测试结果。 网络性能基准 网络的带宽和延迟也是影响 Cluster 集群性能的重要因素。 iperf 是一个非常好的测试网络性能的工具，它使用了 client-server 连接来测试 TCP 和 UDP 的带宽。 iperf 需要安装在两个以上的节点上， 其中一个运行 server， 其他运行 client。 [ceph@ceph1 ~]$ iperf -s [ceph@ceph0 ~]$ iperf -c ceph1 可以看到，网络的传输带宽为 941 MB/s，网络所能承受的最大吞吐量. 有了一些基准数据之后， 就可以进行集群的性能测试了。 在进行测试之前，需要将所有的缓存禁掉。 $ sudo echo 3|sudo tee /proc/sys/vm/drop_caches && sudo sync 测试 Ceph 集群存储性能 Ceph 包含了 rados bench 命令， 目的就是为了测试 RADOS 存储集群的性能。 要测试其性能， 先创建一个测试的存储池， 再进行测试。 [ceph@cpeh0 ~]$ ceph rbd pool create rbd_bench 100 100 [ceph@ceph0 ~]$ rados bench -p rbd_bench 10 write--no-cleanup 结果如上图所示， 测试进行了 10s 的写入测试，测试结果给出了集群写入性能的相关指标。读取测试有两种，顺序 seq 和 随机 rand。 [ceph@ceph0 ~]$ rados bench -p rbd_bench 10 seq [ceph@ceph0 ~]$ rados bench -p rbd_bench 10 rand 可以加上 -t 选项来增加并行读写性能， 或者 -b 选项来改变写入对象的大小。可以同时对不同的 Pools 进行多次测试以观察多个客户端时的性能变化。下面是一个测试样例。 在测试完成后，可以将所有的测试数据清理掉。 [ceph@ceph0 ~]$ rados -p rbd_bench cleanup 测试 Ceph RBD 块设备性能 如果你是在用 Ceph 作为 OpenStack 等云计算平台的存储端，你可能比较关心 RBD 块设备的性能。 Ceph 本身已经包含 rbd 的高性能测试命令 rbd bench，也可以用流行的 I/O 性能测试工具 fio， 其现在也内置了对 RADOS 块设备的支持。 使用 RBD 内置性能测试工具 rbd bench 提供了一些选项可以指定测试的类型， 读写的大小，同时读写线程数等等， 下面是命令的简单用法。 $ rbd help bench usage: rbd bench [--pool &lt;pool&gt;] [--image &lt;image&gt;] [--io-size &lt;io-size&gt;] [--io-threads &lt;io-threads&gt;] [--io-total &lt;io-total&gt;] [--io-pattern &lt;io-pattern&gt;] [--rw-mix-read &lt;rw-mix-read&gt;] --io-type &lt;io-type&gt; &lt;image-spec&gt; Simple benchmark. Positional arguments &lt;image-spec&gt; image specification (example: [&lt;pool-name&gt;/]&lt;image-name&gt;) Optional arguments -p [--pool] arg pool name --image arg image name --io-size arg IO size (in B/K/M/G/T) [default: 4K] --io-threads arg ios in flight [default: 16] --io-total arg total size for IO (in B/K/M/G/T) [default: 1G] --io-pattern arg IO pattern (rand or seq) [default: seq] --rw-mix-read arg read proportion in readwrite (&lt;= 100) [default: 50] --io-type arg IO type (read , write, or readwrite(rw)) 创建测试镜像 [ceph@ceph0 ~]$ rbd create -p rbd_bench image01 --size 1024 [ceph@ceph0 ~]$ sudo rbd map image01 -p rbd_bench --name client.admin [ceph@ceph0 ~]$ sudo mkfs.ext4 -m0 /dev/rbd/rbd_bench/image01 [ceph@ceph0 ~]$ sudo mkdir -p /mnt/rbd/rbd0 [ceph@ceph0 ~]$ sudo mount /dev/rbd/rbd_bench/image01 /mnt/rbd/rbd0 写入测试 顺序写入测试 。 --io-threads 16, --io-pattern seq, --io-type write [ceph@ceph0 ~]$ rbd bench --io-pattern seq --io-type write --io-threads 16 rbd_bench/image01 bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 19216 18928.78 77532265.12 2 33840 16885.47 69162868.74 3 49872 16634.56 68135172.10 4 64688 15932.72 65260420.77 5 79696 15891.25 65090544.94 6 94736 15155.24 62075871.22 7 108352 14920.02 61112410.18 8 123728 14770.92 60501690.27 9 140064 15264.19 62522125.05 10 156336 15380.00 62996487.97 11 171088 15270.11 62546374.77 12 183440 15017.32 61510925.55 13 198176 14689.54 60168357.84 14 212144 14355.43 58799859.19 15 226336 13999.74 57342914.85 16 235568 12775.66 52329122.65 18 238512 8493.36 34788809.90 19 240784 6812.79 27905191.13 22 240944 3491.69 14301960.19 23 241392 1705.06 6983942.13 24 247104 1445.77 5921865.13 25 248512 1449.25 5936119.55 26 250928 1399.53 5732483.13 27 255328 2846.57 11659558.50 28 258416 3987.74 16333798.18 elapsed: 29 ops: 262144 ops/sec: 8991.53 bytes/sec: 36829303.18 写入速度为 36829303.18 B/s ~ 25 MB/s. 2. 随机写入测试 。--io-threads 16, --io-pattern rand, --io-type write [ceph@ceph0 ~]$ rbd bench --io-pattern rand --io-type write --io-threads 16 rbd_bench/image01 bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 5984 5899.59 24164734.37 2 6912 3369.59 13801824.44 3 7712 2575.95 10551096.34 4 8544 2118.25 8676341.63 5 9376 1854.63 7596545.03 6 10384 883.16 3617434.56 7 11152 835.78 3423361.20 8 12032 845.39 3462697.68 9 12720 822.68 3369695.10 10 13504 836.29 3425439.24 ... ... ... ... 366 256832 754.22 3089269.17 367 257648 758.82 3108106.35 368 258464 784.48 3213250.05 369 259200 779.69 3193598.51 370 259840 779.38 3192349.72 371 260480 730.61 2992574.34 372 261376 740.70 3033895.78 373 262064 717.12 2937314.62 elapsed: 392 ops: 262144 ops/sec: 668.29 bytes/sec: 2737297.63 随机写入速度为 2737297.63 B/s ~ 2.6 MB/s. 读取测试 顺序读取测试 。--io-pattern seq, --io-type read, --io-threads 16 [ceph@ceph0 ~]$ rbd bench --io-pattern seq --io-type read rbd_bench/image01 bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 11536 11586.54 47458466.83 2 23776 11913.64 48798285.92 3 36560 12203.97 49987468.84 4 48048 12024.79 49253539.23 5 60976 12205.49 49993691.45 6 74800 12652.56 51824882.16 7 88544 12953.35 53056935.50 8 101360 12957.16 53072534.91 9 113376 13065.35 53515678.78 10 126800 13164.55 53921994.24 11 139248 12889.35 52794796.51 12 150336 12358.16 50619042.73 13 163392 12408.65 50825812.14 14 175696 12463.76 51051572.10 15 188432 12326.17 50487973.23 16 201600 12467.67 51067572.49 17 212896 12511.76 51248176.37 18 225136 12346.10 50569607.97 19 237248 12307.70 50412356.02 20 249136 12140.57 49727770.12 21 260448 11771.73 48217007.26 elapsed: 21 ops: 262144 ops/sec: 12396.62 bytes/sec: 50776571.60 顺序读取速度为 50776571.60 B/s ~ 48.4 MB/s 2. 随机读取测试 。 --io-threads 16, --io-type read, --io-pattern rand. [ceph@ceph0 ~]$ rbd bench --io-pattern rand --io-type read rbd_bench/image01 bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/sec: 1 10304 10330.13 42312226.94 2 20976 10501.05 43012303.11 3 32624 10883.42 44578490.48 4 45216 11310.61 46328267.65 5 56896 11384.46 46630748.33 6 69216 11782.18 48259790.95 7 81344 12073.37 49452523.44 8 93648 12204.57 49989908.40 9 105904 12137.37 49714662.45 10 117296 12077.35 49468843.58 11 128624 11881.37 48666106.43 12 140336 11798.18 48325325.73 13 152480 11766.18 48194256.22 14 163936 11606.18 47538908.72 15 174944 11531.69 47233788.66 16 186160 11504.68 47123168.62 17 197376 11407.78 46726277.80 18 209056 11314.98 46346176.24 19 220784 11369.38 46568994.40 20 232192 11449.38 46896668.16 21 243792 11528.49 47220679.10 22 255632 11650.98 47722406.03 elapsed: 22 ops: 262144 ops/sec: 11611.40 bytes/sec: 47560301.56 随机读取速度为 47560301.56 B/s ~ 45.4 MB/s 读写测试 顺序读写测试 。 --io-threads 16, --io-pattern seq, --io-type write [ceph@cpeh0 ~]$ rbd bench --io-type rw --io-pattern seq rbd_bench/image01 bench type readwrite read:write=50:50 io_size 4096 io_threads 16 bytes 1073741824 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 8640 8578.63 35138058.68 2 9600 4793.53 19634290.32 3 10432 3441.30 14095584.16 4 11376 2846.52 11659356.30 5 12304 2462.48 10086300.14 6 13200 912.90 3739220.13 7 14096 899.90 3686001.90 8 14864 891.02 3649603.24 9 15776 878.93 3600091.29 10 17664 1073.05 4395223.66 ... ... ... ... 211 241584 769.89 3153454.38 212 242176 717.08 2937166.44 213 242976 803.49 3291103.50 214 243728 765.24 3134441.78 215 244272 654.07 2679053.50 216 244976 689.42 2823855.15 217 245744 742.39 3040834.29 218 249152 1232.71 5049184.45 219 250624 1282.24 5252041.26 220 252224 1641.93 6725329.65 221 253872 1747.36 7157203.26 222 254000 1637.41 6706848.82 223 254112 960.11 3932592.52 224 255296 999.13 4092418.19 225 256016 776.08 3178824.42 226 256976 626.81 2567395.36 227 257856 782.45 3204925.96 228 258880 963.99 3948515.16 229 259584 866.60 3549578.30 230 260384 864.59 3541367.22 231 261424 881.65 3611231.11 elapsed: 237 ops: 262144 ops/sec: 1105.89 bytes/sec: 4529705.49 read_ops: 131275 read_ops/sec: 553.80 read_bytes/sec: 2268360.4 write_ops: 130869 write_ops/sec: 552.09 write_bytes/sec: 2261345.02 同时读写的性能比较差， 都只有 2 MB/s 的速度。。。 2. 随机读写测试 。 --io-threads 16, --io-pattern seq, --io-type rw [ceph@cpeh0 ~]$ rbd bench --io-type rw --io-pattern rand rbd_bench/image01 bench type readwrite read:write=50:50 io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 4224 4239.92 17366709.15 2 8448 3990.49 16345065.31 3 9072 3028.27 12403778.36 4 9776 2444.29 10011799.28 5 10304 2053.69 8411923.33 6 11232 1381.95 5660465.62 7 12032 733.81 3005688.85 8 12928 770.72 3156880.89 9 13696 777.45 3184453.41 10 14448 832.78 3411073.28 ... ... ... ... 271 253872 911.68 3734249.31 272 254800 924.61 3787217.39 273 255712 926.82 3796260.40 274 256704 930.25 3810312.21 275 257600 920.41 3769988.70 276 258576 934.80 3828938.17 277 259472 925.31 3790086.63 278 260224 904.01 3702825.06 279 260992 858.27 3515475.05 280 261920 861.57 3528995.38 elapsed: 285 ops: 262144 ops/sec: 917.33 bytes/sec: 3757384.15 read_ops: 131548 read_ops/sec: 460.33 read_bytes/sec: 1885514.72 write_ops: 130596 write_ops/sec: 457.00 write_bytes/sec: 1871869.43 随机读写的性能也不好， 都在 1.8 MB/s 左右。。。 使用 fio 对 RBD 进行测试 fio 是一个通用的 I/O 性能测试工具，源码在 http://git.kernel.dk/fio.git。 源码里面带了一个 rbd.fio 的模板文件， 用来测试 RBD 块设备的 4K 随机写入性能。 需要根据 pool 和 image 的名字来修改相应的名字。 [ceph@ceph0 ~]$ git clone git://git.kernel.dk/fio.git && cd fio [ceph@ceph0 ~]$ sudo yum install -y zlib-devel librados librbd-devel librados-devel libaio-devel [ceph@ceph0 fio]$ ./configure && make -j 4 && sudo make install [ceph@ceph0 fio]$ cat examples/rbd.fio [global] #logging #write_iops_log=write_iops_log #write_bw_log=write_bw_log #write_lat_log=write_lat_log ioengine=rbd clientname=admin pool=rbd rbdname=fio_test rw=randwrite bs=4k [rbd_iodepth32] iodepth=32 可以看到， fio 给出的结果与之前的结果相近， 在 1.6 MS/s 左右。 测试 Ceph 对象网关性能 使用 swift-bench 可以测试 Ceph 对象网关的性能。 通过 pip 直接从源里安装 swift-bench： [ceph@ceph0 ~]$ sudo pip install swift-bench 这部分没有涉及。。。来自 Ceph。。 shell> sudo radosgw-admin user create --uid='benchmark' --display-name='benchmark' shell> sudo radosgw-admin subuser create --uid='benchmark' --subuser=bencmark:swift --access=full shell> sudo radosgw-admin key create --subuser=benchmark --key-type=swift shell> radosgw-admin user modify --uid=benchmark --max-buckets=0 shell> cat > /tm/swift.conf < EOF [bench] auth = http://gateway-node/auth/v1.0 user = benchmark:swift key = guessme auth_version = 1.0 EOF shell> swift-bench -c 64 -s 4096 -n 1000 -g 100 /tmp/swift.conf 使用 IOzone 对 RBD 进行测试 …to.be.continued… CBT - Ceph 性能测试框架 CBT 是 Ceph 官方提供的一个系统化的性能测试工具。 将会在 使用 CBT 来对 Ceph 集群进行性能测试 中介绍。。。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph linux"},{"title":"HTCondor - 介绍, 安装与配置","url":"/2018/HTCondor-Introduction-Installation-and-Configuration/","text":"Condor …to.be.continued… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux hpc condor htc"},{"title":"Ceph RBD 镜像功能与异地备份","url":"/2018/Ceph-RBD-Mirror-and-Remote-Backup/","text":"从 2015 年的 Jewel 版本开始, Ceph 引入了 RBD 的 Mirror 功能, 其主要目的是对 Ceph 进行异地防灾备份和故障自动恢复. Ceph RBD 镜像 RBD 的镜像功能是一种在两个及以上 Ceph 集群间异步副本功能。 镜像功能保证了对一个 iamge 做的更改在所有副本上保持时间点的一致性， 包括读写、块设备大小调整、快照、克隆和扁平化等等。镜像功能可以以 主 - 备 或 主 - 主 两种模式运行。使用强制的独占锁和 RBD 的日志功能， RBD 顺序记录了对一个 image 做的所有操作。 这保证了远端的镜像有一个灾难一致性的镜像可以使用。因此， 在镜像一个 image 之前， 必须要启用它的日志功能。 要保证 镜像功能 可用，本地和远端的CRUSH 架构最好要保持一致的容量和性能特征，此外还要两者之间的网络带宽要足够， 网络延迟要尽可能小以避免超时。 rbd-mirror 守护进程 RBD 的镜像功能是由 rbd-mirror 守护进程实现的，其负责将 images 从一个 Ceph 集群同步到另一个。 根据副本类型， rbd-mirror 运行在单个或所有参与镜像的集群上。 单向副本 。 数据从一个主集群同步到一个或多个备份集群上， rbd-mirror 只运行在备份集群上。 在 active-passive 模式下， RBD 镜像有多个备份站点。 双向副本 。 数据在两个 Ceph 集群之间互相同步，不分主次。 两个集群都必须运行 rbd-mirror。 目前，这种双向副本，或者 主 - 主 模式，只支持在两个站点之间同步。 rbd-mirror 由 rbd-mirror 包提供。 每个 Ceph 集群只能有一个 rbd-mirror 在运行。 镜像模式 RBD 镜像是基于 Pool 的。 Ceph 支持两种模式， 由一个 Pool 中的哪些 images 镜像决定。 Pool 模式。 这种模式下，Pool 内的所有 images 都必须启用日志功能。 Image 模式。只有 Pool 中开启日志功能的 images 才会被镜像。 在 主 - 备 模式下，被镜像的 images 有两种状态： 主：可以被修改 备：不可以被修改 image 在被设置成 mirroring 模式时，就自动进入 主 状态。 将 主状态降级或将 备升级都是可能的。 开启日志功能 我们可以在新建一个 image 时或已经存在的 image 上开启 日志 特性， 同时要开启的还有 独占锁 特性。 ## Usage: when creating a new image [ceph@ceph0 ~]$ rbd create {pool-name}/{image-name} --size {image-size} --image-feature {feature-name} ## e.g. [ceph@ceph0 ~]$ rbd create volumes/data --size 1024 --image-feature exclusive-lock,journaling ## Usage: on an existing image [ceph@ceph0 ~]$ rbd feature enable {pool-name}/{image-name} {feature-name} ## e.g. [ceph@ceph0 ~]$ rbd feature enable volumes/data exclusive-lock,journaling 要是想让所有新建的 image 都开启 journaling 特性， 将下面的徽墨加到 Ceph 配置里面： rbd default feature = 125 Pool 配置 以下的操作需要在所有的 同等集群上进行。 启用 Pool 的 镜像功能 ## Usage [ceph@ceph0 ~]$ rbd mirror pool enable {pool-name} {mode} ## e.g. enable pool mode [ceph@ceph0 ~]$ rbd mirror pool enable volumes pool ## e.g. enble image mode [ceph@ceph0 ~]$ rbd mirror pool enable volumes image 禁用 Pool 的 镜像功能 ## Usage [ceph@ceph0 ~]$ rbd mirror pool disable {pool-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror pool disable volumes 在禁用 镜像功能前， 要移除 Peer 集群。 禁用 Pool 的镜像功能，也就禁用了所有 images 的镜像功能。 3. 增加一个集群 Peer ## Usage [ceph@ceph0 ~]$ rbd mirror pool peer add {pool-name} {client-name}@{cluster-name} ## e.g. [ceph@ceph0 ~]$ rbd --cluster local mirror pool peer add data client.remote@remote [ceph@ceph0 ~]$ rbd --cluster remote mirror pool peer add data client.local@local 查看 Peer 信息 ## Usage [ceph@ceph0 ~]$ rbd mirror pool info {pool-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror pool info data Enabled: true Peers: UUID NAME CLIENT peer-uuid ceph-remote client.admin 移除一个 Peer [ceph@ceph0 ~]$ rbd mirror pool peer remove {pool-name} {peer-uuid} 需要明确指定 Pool 名字和 Peer 的 UUID。 6. 获取 Pool 的 镜像状态 ## Usage [ceph@ceph0 ~]$ rbd mirror pool status {pool-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror pool status data health: OK imges: 1 total 想要更多信息， 可加上 --verbose 选项。 Image 配置 image 模式下，下面的操作只在一个集群上运行。 启用 image 镜像 对一个 image 启用镜像： 在所有的 peer 集群上对所在的 Pool 启用镜像功能。 显示指定该 image 启用镜像功能。 ## Usage [ceph@ceph0 ~]$ rbd mirror image enable {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image enable volumes/data 禁用 image 镜像功能 ## Usage [ceph@ceph0 ~]$ rbd mirror image disable {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image disable volumes/data Image 升级与降级 ### DEMOTION ## Usage [ceph@ceph0 ~]$ rbd mirror image demote {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image demote volumes/data ### PROMOTION ## Usage [ceph@ceph0 ~]$ rbd mirror image promote {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image promote volumes/data 这一操作主要是用在 灾难恢复 中. 当降级不能传递到 peer 集群时，利用 --force 选项强制升级一个 image, 例如由于集群错误或者交流超时等等。 4. Image 重新同步 ## Usage [ceph@ceph0 ~]$ rbd mirror image resync {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image resync volumes/data 当两个集群之间数据不一致时， rbd-mirror 不会试图去镜像导致不一致的 image。 这时可用 resync 来重新同步。 5. 获取单个 image 的镜像状态 ## Usage [ceph@ceph0 ~]$ rbd mirror image status {pool-name}/{image-name} ## e.g. [ceph@ceph0 ~]$ rbd mirror image status volumes/data data: global_id: image-uuid state: up+replaying description: some description here last_update: time last update 配置 Ceph 集群之间的镜像 配置单向镜像模式 单向镜像模式即一个主集群同步副本到一个或多个次级集群. 主集群镜像牌 主 (primary) 模式; 次级集群镜像处于 非主(non-primary) 模式, 只能读, 不能写. 单向模式适合维护一个镜像的灾难一致性副本, 但不适合与 OpenStack 结合时次级集群镜像的自动灾难重建等情况, 这时需要双向模式. 两个正在运行的 Ceph 集群: 将镜像从 local 集群备份到 remote 集群. 配置文件分别为 local.conf 和 remote.conf. 若两个集群名字相同, 需要做额外的工作. 见 在相同名字的集群之间配置镜像 一个能够连接到 local 集群的客户端 - client.local. 在两个集群上创建相同的 pool, 比如 volumes. 想要镜像的 pool - volumes 中有想要镜像的 image, 并且 journaling 功能已经启用. 前面说过, 有两种镜像块设备的方式: Pool 镜像模式 Image 镜像模式 配置 Pool 镜像模式 保证 volumes 中的所有 images 都开启了 exclusive lock 和 journaling 特性. 在 remote 集群上的 monitor 节点上, 安装 rbd-mirror. 确保集群中只有一个 rbd-mirror 实例运行. [ceph@ceph0 ~]$ sudo yum install -y rbd-mirror 在两个 集群上, 使用 CLUSTER 特别指定参与镜像的两个集群名字. CLUSTER=local CLUSTER=remote 在两个集群上, 创建能够访问 volumes 池的客户端, 并存储它们的 keyring 到 &lt;cluster-name&gt;.client.&lt;user-name&gt;.keyring. 在 local 集群的 monitor 节点, 创建 client.local. [ceph@ceph0 ~]$ ceph auth get-or-create client.local 'profile rbd' osd 'profile rbd pool=volumes' -o /etc/ceph/ceph.client.local.keyring --cluster local 在 remote 集群的 monitor 节点, 创建 client.remote. [ceph@ceph0 ~]$ ceph auth get-or-create client.remote 'profile rbd' osd 'profile rbd pool=volumes' -o /etc/ceph/ceph.client.remote.keyring --cluster remote 将 local 集群上的 Ceph 配置文件和 新生成的 keyring 文件 拷贝到 remote 集群和 remote 集群的客户端节点上. [ceph@ceph0 ~]$ scp /etc/ceph/local.conf @:/etc/ceph/ [ceph@ceph0 ~]$ scp /etc/ceph/ceph.client.local.keyring @:/etc/ceph/ [ceph@ceph0 ~]$ scp /etc/ceph/local.conf @:/etc/ceph/ [ceph@ceph0 ~]$ scp /etc/ceph/ceph.client.local.keyring @:/etc/ceph/ 在 remote 集群的 monitor 节点上, 启用并启动 rbd-mirror 守护进程. [ceph@ceph-remote ~]$ sudo systemctl enable ceph-rbd-mirror.target [ceph@ceph-remote ~]$ sudo systemctl enable --now ceph-rbd-mirror@ 这里, &lt;client-id&gt; 是指的 rbd-mirror 运行使用的用户, 此处即 remote. 7. 在 local 和 remote 两个集群上启用 pool mirroring. [ceph@ceph0 ~]$ rbd mirror pool enable volumes pool --cluster local [ceph@ceph0 ~]$ rbd mirror pool enable volumes pool --cluster remote 确保 local 和 remote 集群的 mirroring 功能已成功启用. [ceph@ceph0 ~]$ rbd mirror pool info volumes --cluster local [ceph@ceph0 ~]$ rbd mirror pool info volumes --cluster remote 将 local 集群加入到 remote 集群的 peer 中. [ceph@ceph0 ~]$ rbd mirror pool peer add volumes client.local@local --cluster remote [ceph@ceph0 ~]$ rbd mirror info volumes --cluster remote Mode: pool Peers: UUID NAME CLIENT local-cluster-uuid local client.local 配置 Image 镜像模式 确保需要 镜像的 images 开启了 exclusive lock 和 journaling 特性. 依循 配置 Pool 镜像模式 中的 2-5 步. 开启 local 集群上 volumes 的 image 镜像功能. [ceph@ceph0 ~]$ rbd --cluster local mirror pool enable volumes image [ceph@ceph0 ~]$ rbd mirror pool info --cluster local 将 local 集群 加到 remote 集群的 peer 中. [ceph@ceph0 ~]$ rbd --cluster remote mirror pool peer add volumes client.local@local [ceph@ceph0 ~]$ rbd --cluster remote mirror pool info Mode: pool Peers: UUID NAME CLIENT local-cluster-uuid local client.local 在 local 集群上, 显示启用 images 如 image1 的 image 镜像功能. [ceph@ceph0 ~]$ rbd --cluster local mirror image enable volumes/image1 确保镜像功能已经成功启用. [ceph@ceph0 ~]$ rbd mirror image status volumes/image1 --cluster local image1: global_id: image-uuid state: up+replayingn description: replaying, master_position=[xxxxx], mirror_position=[xxxxx], entries_behind_master=0 last_update: last_update_time [ceph@ceph0 ~]$ rbd mirror image status volumes/image1 --cluster remote image1: global_id: image-uuid state: up+replayingn description: replaying, master_position=[xxxxx], mirror_position=[xxxxx], entries_behind_master=0 last_update: last_update_time 配置双向镜像模式 配置双向模式的下列步骤假定: 有两个正在运行的 Ceph 集群 - local 和 remote, 相应的配置文件在 /etc/ceph/local.conf 和 /etc/ceph/remote.conf. 如果两个集群名字相同, 需要做额外的工作, 见 在相同名字的集群之间配置镜像 有一个 块设备 客户端连接到两个集群 - client.local 和 client.remote. 要镜像的 Pool - volumes 在两个集群中都有. 要镜像的 Pool volumes 包含着要镜像的 images, 要开启了 journaling 特性. 配置 Pool 镜像模式 在双方客户端上, 安装 rbd-mirror. [ceph@ceph0 ~]$ sudo yum install -y rbd-mirror 在双方客户端节点上, 在 /etc/sysconfig/ceph 里用 CLUSTER 特别指定集群名字. CLUSTER=local CLUSTER=remote 在双方集群上, 创建能够访问 volumes 的客户端并输出其 keyring. 在 local 集群的 monitor 节点上, 创建 client.local. [ceph@ceph0 ~]$ ceph auth get-or-create client.local mon 'profile rbd' osd 'profile rbd pool=volumes' -o /etc/ceph/local.client.local.keyring --cluster local 在 remote 集群的 monitor 节点上, 创建 client.remote. [ceph@ceph0 ~]$ ceph auth get-or-create client.remote mon 'profile rbd' osd 'profile rbd pool=volumes' -o /etc/ceph/local.client.remote.keyring --cluster remote 拷贝双方的 Ceph 配置文件和生成的 keyring 拷贝到对方的 monitor 节点上. 拷贝 local.conf 和 local.client.local.keyring 到 remote 集群的 monitor 节点上. [ceph@ceph-local ~]$ scp /etc/ceph/local.conf ceph@ceph-remote:/etc/ceph [ceph@ceph-local ~]$ scp /etc/ceph/local.client.local.keyring ceph@ceph-remote:/etc/ceph 拷贝 remote.conf 和 remote.client.remote.conf 到 local 集群的 monitor 节点上. [ceph@ceph-local ~]$ scp ceph@ceph-remote:/etc/ceph/remote.conf /etc/ceph/ [ceph@ceph-local ~]$ scp ceph@ceph-remote:/etc/ceph/remote.client.remote.keyring /etc/ceph/ 如果 local 集群上 monitor 跟 mirroring 守护进程不在 = 同一个节点上, 需要 拷贝 local.client.local.keyring 到运行 rbd-mirroring 的节点上. remote 集群也一样. [ceph@ceph-local ~]$ scp /etc/ceph/local.client.local.keyring ceph@ceph-local-mirror:/etc/ceph/ [ceph@ceph-remote ~]$ scp /etc/ceph/remote.client.remote.keyring ceph@ceph-remote-mirror:/etc/ceph/ 在双方客户端节点上, 启用并启动 rbd-mirror守护进程. [ceph@ceph0 ~]$ sudo systemctl enable ceph-rbd-mirror.target [ceph@ceph0 ~]$ sudo systemctl enable --now ceph-rbd-mirror@ &lt;client-id&gt; 这里即 local, remote. 7. 启用两个集群 volumes 上的 pool mirroring, 并确保启用成功. [ceph@ceph0 ~]$ rbd mirror pool enable volumes pool --cluster local [ceph@ceph0 ~]$ rbd mirror pool enable volumes pool --cluster remote [ceph@ceph0 ~]$ rbd mirror pool status volumes --cluster local [ceph@ceph0 ~]$ rbd mirror pool status volumes --cluster remote 将双方互相加为 peer, 并确保添加成功. [ceph@ceph0 ~]$ rbd mirror pool peer add volumes client.local@local --cluster remote [ceph@ceph0 ~]$ rbd mirror pool peer add volumes client.remote@remote --cluster local [ceph@ceph0 ~]$ rbd mirror pool info volumes --cluster local ... [ceph@ceph0 ~]$ rbd mirror pool info volumes --cluster remote 配置 Image 镜像模式 依循 配置 Pool 镜像模式 中的 1-5 步. 在双方的 volumes pool 上启用 image 镜像模式, 并确保成功. [ceph@ceph0 ~]$ rbd --cluster local mirror pool enable volumes image [ceph@ceph0 ~]$ rbd --cluster remote mirror pool enable volumes image [ceph@ceph0 ~]$ rbd --cluster local mirror pool status volumes [ceph@ceph0 ~]$ rbd --cluster remote mirror pool status volumes. 互相添加为 peer, 并确保成功. [ceph@ceph0 ~]$ rbd mirror pool peer add volumes client.local@local --cluster remote [ceph@ceph0 ~]$ rbd mirror pool peer add volumes client.remote@remote --cluster local [ceph@ceph0 ~]$ rbd mirror pool info --cluster local [ceph@ceph0 ~]@ rbd mirror pool info --cluster remote 在主集群上, 显示启用 images image1 的 image 镜像功能. [ceph@ceph0 ~]$ rbd --cluster local mirror image enable volumes/image1 [ceph@ceph0 ~]$ rbd --cluster local mirror image status volumes/image1 ... [ceph@ceph0 ~]$ rbd --cluster remote mirror image status volumes/image1 ... 在相同名字的集群之间配置镜像 有时候集群管理者会使用相同的名字, 一般是 ceph 创建集群. 当两个集群名字相同时, 目前要执行如下的步骤. 在 rbd-mirror 执行一端的集群上的 /etc/sysconfig/ceph 中更改集群的名字如 master. 同时建立一个指向 ceph.conf 的符号链接. [ceph@ceph0 ~]$ ln -sf /etc/ceph/ceph.conf /etc/ceph/master.conf 使用每次 rbd-mirror 配置过程中都使用 符号链接文件 master.conf. [ceph@ceph0 ~]$ ceph -s [ceph@ceph0 ~]$ ceph -s --cluster master 现在两个命令都指向同一个集群. 延迟副本模式 无论使用单向还是双向副本模式， 我们都可以在 RBD image 镜像时设置延迟，这样我们就有一个时间窗口来撤销一些作用在主要 iamge 上的并不想要的改变。 要实现这样的延迟副本， 目标集群上的 rbd-mirror 守护进程需要设置 rbd mirroring replay delay = {minimum delay in seconds}。这可以通过 ceph.conf 作用在所有 Pools 上， 也可以单独作用在 一些 images 上。 ## Usage [ceph@ceph0 ~]$ rbd image-meta set {pool-name}/{image-name} conf_rbd_mirroring_replay_delay {minimum delay in seconds} ## e.g. [ceph@ceph0 ~]$ rbd image-meta set volumes/data conf_rbd_mirroring_replay_delay 600 利用 Ceph 集群镜像进行灾难数据恢复 从一次正常关机进行故障转移 关掉所有正在使用 主 image 的客户端. 这取决于哪种客户端在使用 image. 将 local 集群上的 主 image 降级. [ceph@ceph0 ~]$ rbd mirror image demote {pool-name}/{image-name} --cluster local 将 remote 集群的 非主 image 升级. [ceph@ceph0 ~]$ rbd mirror image promote {pool-name}/{image-name} --cluster remote 重新让客户端连接新的 主 image. 同样取决于使用的客户端. 从一次非正常关机进行故障转移 确认主集群(local cluster) 已经关闭. 停止所有使用主 image 的客户端. 这取决于哪种客户端在使用 image. 将远端的 非主状态的 image 升级为 主 image, 使用 --force选项. [ceph@ceph0 ~]$ rbd mirror image promote --force {pool-name}/{image-name} --cluster remote 重新让客户端连接新的 主 image. 同样取决于使用的客户端. 故障恢复 当以前的主集群恢复后, 需要行故障恢复. 如果是非正常关机, 将 local 集群上的旧的主 image 降级. [ceph@ceph0 ~]$ rbd mirror image demote {pool-name}/{image-name} --cluster local 只有在非正常关机时才重新同步 image. [ceph@ceph0 ~]$ rbd mirror image resync {pool-name}/{image-name} --cluster local 保证 重新同步 完成, 并处于 up+replaying 状态. [ceph@ceph0 ~]$ rbd mirror image status {pool-name}/{image-name} --cluster local 将次级集群上的 image 降级. [ceph@ceph0 ~]$ rbd mirror image demote {pool-name}/{image-name} --cluster remote 将主集群上的 image 升级. [ceph@ceph0 ~]$ rbd mirror image promote {pool-name}{image-name} --cluster local 利用镜像更新实例 …to.be.continued… OpenStack 实现 …to.be.continued… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph rbd"},{"title":"Ceph 中块设备 RBD 的基本用法","url":"/2018/Using-RBD-in-Ceph/","text":"块设备 (block device) 是一系列的数据块序列。基于数据块的存储接口是最常用的用来存储 硬盘 , CD, 软盘 , 磁盘 等数据的方式。正因为这种普适性，虚拟块设备成了与大容量数据存储系统交互的非常理想的选择。 块设备概述 Ceph 中的 块设备, 即可靠的、自治的、分布式的对象存储(RBDs) 块设备(RBDs), 是稀疏预分配的，可调大小的，将数据条状存储在 Ceph 存储集群上。 Ceph 块设备 使用了 RADOS 的一些功能： 创建快照 副本 一致性 Ceph 块设备使用 librbd 库与 OSDs 交互，其在 KVMs 如 QEMU, 和云计算系统如 OpenStack，CloudStack 等依赖于 libvirt 和 QEMU 功能与之交互的应用中有很高的性能。 块设备基本命令 要使用块设备相关命令， 你需要能够访问正在运行的 Ceph 存储集群，并且安装了 Ceph Block 客户端。 rbd 命令能够创建、列出、自检、移除块设备，还可以对块设备进行克隆、建立快照、回滚快照、查看快照等。 显示帮助 因为 rbd 的命令和子命令非常多, 要记住所有的比较困难, 可以 通过 help 命令查寻相应命令的使用方法. ### Usage [ceph@ceph0 ~]$ rbd help [] ### e.g. [ceph@ceph0 ~]$ rbd help snap llist 创建一个块设备池 在使用块设备客户端前, 必须有一个 rbd 类型的池存在并且被初始化了. 创建一个 rbd Pool, 可以使用如下命令: [ceph@ceph0 ~]$ ceph osd pool create {pool-name} {pg-num} {pgp-num} ### set rbd property for pool [ceph@ceph0 ~]$ ceph osd pool application enable {pool-name} rbd [ceph@ceph0 ~]$ rbd pool init -p {pool-name} 创建块设备镜像 使用如下命令 创建一个块设备镜像: ### Usage: [ceph@ceph0 ~]$ rbd create --pool {pool-name} {image-name} --size {megabytes} ### e.g. create a rbd image named data of size 1GB stored in a pool named volumes [ceph@ceph0 ~]$ rbd create -p volumes data --size 1024 创建的时候有一些选项： --order n, 创建的 RBD 的条带大小为 2^n bytes， 如 –order 24` 为 16M --iamge-format, 默认格式是 1， 原始格式， 不支持 RBD 分层， format 2 支持 RBD 分层， 可以 COW， 可写快照。 罗列块设备镜像 要列出 Pool 内所有的镜像, 可用如下命令 ### Usage: list rbd pool if pool name is absent [ceph@ceph0 ~]$ rbd ls [{pool-name}] ### e.g. list images of pool volumes [ceph@ceph0 ~]$ rbd ls volumes 获取镜像信息 列出 Pool 内的镜像后, 我们可能对某一个镜像感兴趣, 可通过如下命令获取其信息: ### Usage: image-name in rbd if pool-name is absent [ceph@ceph0 ~]$ rbd [{pool-name}] info --image {image-name} ### e.g. retriving information of image data in pool rbd [ceph@ceph0 ~]$ rbd info --image data ### e.g. retriving information of image data in pool volumes [ceph@ceph0 ~]$ rbd info -p volumes --image data ### or simply [ceph@ceph0 ~]$ rbd info volumes/data 调整块设备镜像大小 Ceph 的块设备镜像是稀疏分配的, 直到你开始往其中存入数据, 它们才会使用物理空间. 它们有一个最大可用容量, 由 --size 选项指定, 但你可以通过 resize 命令来更改其大小. ### Usage: resize image-name in rbd pool if pool-name is absent [ceph@ceph0 ~]$ rbd resize --image --size [--pool ] ### e.g. cresize data in volumes pool from original to 60G [ceph@ceph0 ~]$ rbd resize volumes/data --size 60G rbd 大小一般只能扩大, 不能减小, 否则很可能会损坏数据. 若非要减小其大小, 可加上 --allow-shrink 选项 移除块设备镜像 当一个镜像完成了使命, 我们便可以将其删除掉, ### Usage: remove image image-name in pool rbd if pool-name is absent [ceph@ceph0 ~]$ rbd rm [--pool {pool-name}] rm {image-name} ### e.g. remove an image bar from pool volumes [ceph@ceph0 ~]$ rbd rm volumes/bar ### or [ceph@ceph0 ~]$ rbd rm -p volumes bar 将镜像移到回收站 相比于 rm 命令, 我们可以使用 trash 命令将镜像移到回收站里, 过后再将其删除, 防止意外删除镜像. trash 命令提供了比 rm更多的选项. 移动镜像到回收站 ### Usage [ceph@ceph0 ~]$ rbd trash move [--pool {pool-name}] image-name ### e.g. [ceph@ceph0 ~]$ rbd trash move -p volumes data 从回收站彻底删除镜像. 利用镜像的 image-id: ### Usage [ceph@ceph0 ~]$ rbd trash remove [--pool {pool-name}] {image-id} ### e.g. [ceph@ceph0 ~]$ rbd trash remove volumes/data-image-id 延时回收镜像. 通过 --delay {time-in-second} 选项来指定一定时间内镜像不可从回收站删除. ### Usage [ceph@ceph0 ~]$ rbd trash move [--delay {time-in-second}] [--pool {pool-name}] {image-name} ### e.g. [ceph@ceph0 ~]$ rbd trash move --delay 360 -p volumes data 从回收站恢复镜像. 只要一个镜像还在回收站中未被删除, 且没有指定 --delay 选项, 就可以回收站中恢复. ### Usage [ceph@ceph0 ~]$ rbd trash restore [--pool {pool-name}/]{image-id} ### e.g. [ceph@ceph0 ~]$ rbd trash restore {pool-name}/data-image-id 启用 / 禁用镜像特性 我们可以启用或禁用镜像的某些特性, 比如 fast-diff, exclusive-lock, object-map 或者 journaling 等等. 启用特性 ### Usage [ceph@ceph0 ~]$ rbd feature enable {pool-name}/{image-name} {feature-name} ### e.g. enable journaling on volumes/data [ceph@ceph0 ~]$ rbd feature enable volumes/data journaling 禁用特性 ### Usage [ceph@ceph0 ~]$ rbd feature disable {pool-name}/{image-name} {feature-name} ### e.g. disable journaling on volumes/data [ceph@ceph0 ~]$ rbd feature disable volumes/data journaling 在启用 fast-diff 和 object-map 特性后, 要重新建立 object map: [ceph@ceph0 ~]$ rbd object-map rebuild {pool-name}/{image-name}ss deep flatten 特性只能在 镜像建立时 启用, 不能在 已经建立而没有启用 的镜像上启用. 使用镜像元数据 Ceph 支持向镜像添加 key-value 对形式的元数据. key-value 对没有什么严格的形式. 使用 rbd image-meta 命令可以添加删除元数据. 设置镜像元数据 ### Usage [ceph@ceph0 ~]$ rbd image-meta set {pool-name}/{image-name} {key} {value} ### e.g. [ceph@ceph0 ~]$ rbd image-meta set volumes/data last-update 2018-09-19:10:51 获取一个键的值 ### Usage [ceph@ceph0 ~]$ rbd image-meta get {pool-name}/{image-name} ### e.g. [ceph@ceph0 ~]$ rbd image-meta get volumes/data last-update 移除镜像元数据 ### Usage [ceph@ceph0 ~]$ rbd image-meta remove {pool-name}/{image-name} {key} ### e.g. [ceph@ceph0 ~]$ rbd image-meta remove volumes/data last-update 列出镜像的元数据 ### Usage [ceph@ceph0 ~]$ rbd image-meta list {pool-name}/{image-name} ### e.g. [ceph@ceph0 ~]$ rbd image-meta list volumes/data 重设镜像默认配置 ### Usage [ceph@ceph0 ~]$ rbd image-meta set {pool-name}/{image-name} conf_{parameter} {value} ### e.g. [ceph@ceph0 ~]$ rbd image-meta set volumes/data conf_rbd_cache false RBD 块设备镜像功能 RBD 的镜像功能是一种在两个及以上 Ceph 集群间异步副本功能。 镜像功能保证了对一个 iamge 做的更改在所有副本上保持时间点的一致性， 包括读写、块设备大小调整、快照、克隆和扁平化等等。 具体可参考 Ceph RBD 镜像功能与异地备份 内核模块操作 要使用 内核模块操作， 必须有一个正在运行的 Ceph 集群。 列出镜像 Ceph 块设备的一大应用就是可挂载到虚拟机，我们也可以挂载到本地使用。要挂载一个镜像，需要先知道镜像名字，先将所有镜像列出： [ceph@ceph-client ~]$ rbd list 映射 RBD 到本地使用 需要指定 pool 名字， 镜像名字，用户名字, 若使用 CephX 进行认证，则还要加上相应的 密钥文件(keyring). ### Usage [ceph@ceph-client ~]$ rbd map [--pool {pool-name}] image-name --id {user-name} [--keyring /path/to/keyring] ### e.g. [ceph@ceph-client ~]$ rbd map volumes/data --id admin --keyring /etc/ceph/ceph.client.admin.keyring 显示已映射的设备 要显示已映射的设备，可以使用 rbd 的 showmapped 选项： ## Usage [ceph@ceph-client ~]$ rbd showmapped 一般会映射到 /dev/rbd/xxx/xxx 之类的地方， 比如 /dev/rbd/rbd/rbd1, 然后就就像一般硬盘一样可以对其格式化，挂载使用 [ceph@ceph-client ~]$ sudo mkfs.ext4 /dev//dev/rbd/volumes/data [ceph@ceph-client ~]$ sudo mount -t ext4 /dev/rbd/volumes/data /mnt/volumes/data [ceph@ceph-client ~]$ df -Th 取消设备映射 ## Usage [ceph@ceph-client ~]$ rbd unmap /dev/rbd/{pool-name}/{image-name} ## e.g. [ceph@ceph-client ~]$ rbd unmap /dev/rbd/volumes/data 使用 Python 处理 RBD Python 中的 rbd 模块提供了像处理文件一样处理 RBD 镜像的功能, 核心接口由 librbd 提供。 使用这些功能之前，要先导入 rados 和 rbd 模块 ### in python >>> import rados >>> import rbd 创建和写入镜像 连接到 RADOS (Ceph 集群), 打开一个 IO 上下文 (IO context) >>> cluster = rados.Rados(conffile = 'my_ceph.conf') >>> cluster.connect() >>> ioctx = cluster.open_ioctx('my_pool') 实例化一个 class:rbd.RBD 对象，可以用来创建镜像： >>> rbd_inst = rbd.RBD() >>> size = 4*1024**3 # 4 GiB >>> rbd_inst.create(ioctx, 'my_image', size) 实例化一个 class:rbd.RBD 对象，对镜像进行读写: >>> image = rbd.Image(ioctx, 'my_image') >>> data = 'foo' * 200 >>> image.write(data, 0) 关闭镜像， I/O 上下文，和与 RADOS 的连接: >>> image.close() >>> ioctx.close() >>> cluster.shutdown() 为了安全，可以使用 finally: 或 with 语句: ### with with rados.Rados(conffile='my_ceph.conf') as cluster: with cluster.open_ioctx('my_pool') as ioctx: rbd_inst = rbd.RBD() size = 4*1024**3 # 4 GiB rbd_inst.create(ioctx, 'my_image', size) with rbd.Image(ioctx, 'my_image') as image: data = 'foo' * 200 image.write(data, 0) ### try and finally cluster = rados.Rados(conffile = 'my_ceph.conf') try: ioctx = cluster.open_ioctx('my_pool') try: rbd_inst = rbd.RBD() size = 4*1024**3 # 4 GiB rbd_inst.create(ioctx, 'my_image', size) image = rbd.Image(ioctx, 'my_iamge') try: data = 'foo' * 200 image.write(data, 0) finally: image.close() finally: ioctx.close() finally: cluster.shutdown() document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph openstack rbd"},{"title":"解决 Hexo 无法生成 index.html 等静态文件的问题","url":"/2018/Solving-Hexo-Not-Generating-Static-Files/","text":"Hexo 是非常棒的一个静态博客生成软件，使用也非常方便。但有时候会有一些小问题，比如不会生成 index.html 文件等。 Hexo 无法生成 index.html 在刚初始化一个项目后， 你运行 hexo g，有时候 hexo 并不会生成 index.html 和其他一些静态文件。 这一般是没有初始化完全的原因, 有些插件没有安装 查看 npm 插件缺失情况 $ npm ls --depth 0 这时一般会提醒你有插件没有装。 npm ERROR! missing xxx 安装缺失插件 如果你的插件都在 packages.json 里， 可以简单通过如下命令安装 $ npm install --save 要是没有， 就依次将所有缺失的插件安装上 $ npm install --save jquery jsdom [xxx ...] 重新生成静态文件 安装好后，执行 hexo g 命令应该就可以正常生成完整博客了。 注：本文来自 HuiHut document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux hexo"},{"title":"在 Windows 10 中使用 OpenSSH","url":"/2018/Using-SSH-on-Windows-10/","text":"在 Windows 10 中, 微软添加了 OpenSSH 客户端和服务端功能, 我们可以很方便地用 Windows 的用户名和密码从 Linux 远程登陆到 Windows 10 中. 添加 OpenSSH 功能 在设置中，进入 应用和功能， 选择 管理可选功能 , 然后选择 添加功能， 添加 Openssh 客户端 和 OpenSSH 服务端 两项功能， 这是已经是安装好这两项功能了。 安装好后需要重启 Window。 启用 OpenSSH 服务 OpenSSH 在 Windows 中是以 Windows 的服务存在的。安装后，默认是不会自动启动的。 我们需要进入 服务， 在服务中启用 OpenSSH SSH Server 和 OpenSSH Authentication Agent, 设置为自动并启用 设置 Windows 防火墙 设置好 OpenSSH 服务后， 我们还要在防火墙中允许 OpenSSH 通过防火墙。 与 WSL 配合 如果你安装了 WSL， 登陆 Windows 后，可以 运行 bash.exe 进入 Linux 环境, 搭配 tmux 或其它工具， 非常方便。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"windows openssh"},{"title":"Ceph 集群管理","url":"/2018/Ceph-Administrition/","text":"这篇文章会记录管理 Ceph 集群的方法及在搭建和管理集群过程中遇到的一些问题及解决方案… Ceph 集群管理 管理集群规模大小 增加减少 Monitor 增加 Monitor 减少 Monitor 增加减少 OSD 增加一个 OSD 创建 OSD。 [ceph@ceph0 ~]$ ceph osd create [ []] 创建新 OSD 默认目录，如果不是 root 用户，需要修改目录权限 [ceph@ceph0 ~]$ ssh new-osd-host sudo mkdir /var/lib/ceph/osd/ceph- [ceph@ceph0 ~]$ ssh new-osd-host sudo chown -R ceph:ceph /var/lib/ceph/osd/ceph- 一般如果硬盘不是用作系统盘，可以格式化，并挂载到刚建立的 ceph 目录上。 [ceph@ceph0 ~]$ ssh new-osd-host sudo mkfs -t xfs /dev/{driver-for-ceph} [ceph@ceph0 ~]$ ssh new-osd-host sudo mount -o user_xattr /dev/{driver-for-ceph} /var/lib/ceph/osd/ceph- 初始化 OSD 数据目录，OSD 目录下必须是空的。 [ceph@ceph0 ~]$ ssh new-osd-host ceph-osd -i --mkfs --mkkey 注册新的 OSD。 [ceph@ceph0 ~]$ 添加新的 OSD 到 CRUSH map 中。 [ceph@ceph0 ~]$ 移除一个 OSD 更换 OSD 1 Ceph 的设计就是具有故障容忍性的. Ceph 可以在降级的状态进行操作而不会丢失数据. 例如, Ceph 在一个驱动器损坏的情况下仍能工作, 这时, 损坏的 OSD 上的数据在其他 OSD 上的副本会回填到另外的 OSDs 上, 即降级状态. 然而, 如果一个 OSD 损坏了, 你应该将其替换掉, 并重建该 OSD. 当 OSD 出故障了, 其状态会显示为 down 和 in. Ceph 会提示健康度警告. Ceph 显示 OSD down 不一定就是 OSD 出故障了, 也有可能是心跳包或其他网络问题. Ceph 中 OSD 出故障了, 一般按照如下程序进行替换. 检查集群健康度. [ceph@ceph0 ~]$ ceph -s 如果一个 OSD 显示为 down, 确定其在 **CRUSH ** 层级中的位置, 位于哪个节点上. [ceph@ceph0 ~]$ ceph osd tree | grep -i down 登陆到有故障的 OSD 所在节点上, 尝试去重启 OSD. [ceph@ceph0 ~]$ ssh bad-osd-node sudo systemctl restart ceph-osd@bad-osd-id [ceph@ceph0 ~]$ ssh bad-osd-node sudo systemctl status ceph-osd@bad-osd-id 如果结果显示 OSD 运行正常, 查看其日志, 有可能是网络问题. 如果无法重启, 说明 驱动器很可能是损坏了. 4. 检查故障 OSD 挂载点. 如果没法启动 OSD, 先看看挂载点是否还在, 如果不在, 可以重新挂载它. [ceph@ceph0 ~]$ df -h 如果还是无法挂载. 说明, 很有可能驱动器损坏了. 需要利用工具查看驱动器的健康程度, 检查结果若真是损坏了, 我们就需要将其替换掉, 并重新建立一个 OSD. 5. 执行如下命令来更换 OSD 在的硬盘. ### 1. 标记 OSD 已经不在集群中. [ceph@ceph0 ~]$ ceph osd out osd. ### 2. 确保该 OSD 进程已经停止. [ceph@ceph0 ~]$ ssh bad-osd-host systemctl stop ceph-osd@ ### 3. 确保集群回填数据成功. [ceph@ceph0 ~]$ ceph -w ### 4. 将 OSD 从 CRUSH map 中移除. [ceph@ceph0 ~]$ ceph osd crush remove osd. ### 5. 移除 OSD 认证密钥. [ceph@ceph0 ~]$ ceph auth del osd. ### 6. 将 OSD 从集群中移除. [ceph@ceph0 ~]$ ceph osd rm osd. ### 7. 卸载故障的驱动器. [ceph@ceph0 ~]$ ssh bad-osd-node sudo umount /var/lib/ceph/{daemon}/{cluster}-{daemon-id} ### 8. 不支持热插拔, 需要将节点下线, 更换硬盘. 在此之前, 需要设置集群禁止回填数据. [ceph@ceph0 ~]$ ceph osd set noout ### 9. 更换完毕硬盘后, 将节点启动上线, 移除 `noout` 设置. [ceph@ceph0 ~]$ ceph osd unset noout ### 10. 找到 OSD 驱动器, 格式化硬盘. 例如. [cepp@ceph0 ~]$ ssh bad-osd-host sudo mkfs.xfs /dev/sdd 重建 OSD. 见 增加一个 OSD 检查 CRUSH 层级, 确保没有问题. [ceph@ceph0 ~]$ ceph osd tree 如果对 OSD 在 CRUSH 层级中的位置不满意, 可以将其移动到合适的地方. [ceph@ceph0 ~]$ ceph osd crush move = 确保重建的 OSD 已经上线. [ceph@ceph0 ~]$ ceph -s 更换 OSD 2 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph storage linux"},{"title":"Ceph 中的 Pool 配额与 RBD 在线调整大小","url":"/2018/Pool-Quota-and-RBD-Resizing-Online-in-Ceph/","text":"在 Ceph 中, 我们可以对一个 Pool 的 空间大小 和文件数 设置配额大小, 对 RBD 的 image/volume 也可心进行在线调整大小, 十分方便. Pool 配额设置 设置 Pool 的配额大小 设置 空间大小 配额 ## usage $ ceph osd pool set-quota {pool-name} max_bytes {bytes} ## e.g. $ ceph osd pool set-quota test-pool max_bytes $((100 * 1024 * 1024 * 1024)) 设置 文件数 配额 ## usage $ ceph osd pool set-quota {pool-name} max_objects {file_num} ## e.g. $ ceph osd pool set-quota test-pool max_objects 10000 两个配额可以 同时设置, 既设置空间限制, 又设置文件数限制. 查寻 Pool 配额设置 ## usage $ ceph osd pool get-quota {pool-name} ## e.g. $ ceph osd pool get-quota test-pool 取消 Pool 的配额 要取消一个 Pool 的配额, 只需要将相应的限制设为 0 即可. ## usage $ ceph osd pool set-quota {pool-name} max_bytes|max_objects 0 ## e.g. $ ceph osd pool set-quota test-pool max_bytes 0 $ ceph osd pol set-quota test-pool max_objects 0 RBD 镜像在线调整大小 RBD 镜像调整大小很简单. 若是离线状态, 可以直接对其调整大小, 若是已经将其挂载到系统中, 则需要将其卸载, 调整后再挂载. 对于挂载到虚拟机上的可以通过 virsh 命令在线调整. 利用 libvirt 调整 rbd 镜像 在物理机上调整大小 获取当前镜像大小 ## usage $ qemu-img info -f rbd \"rbd:{pool-name}/{image-name}\" ## e.g. $ qemu-img info -f rbd \"rbd:volume/test-imgage\" image: json:{\"pool\": \"volumes\", \"image\": \"test-image\", \"driver\": \"rbd\"} file format: rbd virtual size: 50G (53687091200 bytes) disk size: unavailable cluster_size: 4194304 将镜像大小调整到一个合适的大小, 一般只能调大, 否则很可能会损坏文件系统. 若要调小容量, 需要加上 --allow-shrink 选项. ## e.g. $ qemu-img resize -f rbd \"rbd:volumes/test-image\" 30G --allow-shrink Image resized. 列出所有挂载的 镜像. ## usage $ virsh domblklist {domain-name} ## e.g. $ virsh domblklist rbd-test 调整 libvirt 块设备 $ virsh blockresize --domain rbd-test --path vdb --size 60G $ virsh info volumes/test-block 在虚拟机上调整大小 重新挂载 rbd, 假定挂载到 /mnt/rbd/data: $ sudo mount -o remount,rw /dev/vdb1 文件系统为 xfs: $ xfs_growfs /mnt/rbd/data 文件系统为 ext3/ext4: $ e2fsck -fy /dev/vdb1 $ resize2fs /dev/vdb1 ## check rbd size $ df -h 利用内核模块调整 RBD 大小 在 内核版本在 3.10+ 以上的系统上, 可以直接使用 rbd 的 resize 功能: 获取 RBD 镜像当前大小: $ rbd info volumes/volume-a737fff5-6426-42ab-8873-4119bbecebbf rbd image 'volume-a737fff5-6426-42ab-8873-4119bbecebbf': size 60 GiB in 15360 objects order 22 (4 MiB objects) id: 2b4301cac20ca block_name_prefix: rbd_data.2b4301cac20ca format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Tue Sep 18 09:00:49 2018 parent: images/c18d8e1d-b4df-42ec-b06f-53bbe53187c1@snap overlap: 1.1 GiB 物理机调整 RBD 大小: $ rbd resize volumes/volume-a737fff5-6426-42ab-8873-4119bbecebbf --size 80G 虚拟机调整 RBD 大小: ### xfs $ xfs_growfs /dev/vdb1 ### ext4 $ e2fsck -fy /dev/vdb1 $ resize2fs /dev/vdb1 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph openstack rbd"},{"title":"QuarkDB 集群数据库的备份与恢复","url":"/2018/Backup-and-Restore-in-QuarkDB-Cluster/","text":"数据库的备份与恢复对数据安全是极为重要的， 特别是对于存储 EOS 元数据的 QuarkDB 数据库。 虽然 QuarkDB 集群有多副本保证元数据的安全， 但按时备份 QuarkDB 集群仍是一个良好的习惯。 假定我们的 QuarkDB 数据文件夹是 /var/lib/quarkdb/, 那我们如何来备份它呢？直接复制运行中的 QuarkDB 节点的 /var/lib/quarkdb 到一个备份文件夹是 不行 的。 因为从开始备份到结束备份期间，QuarkDB 集群很可能会进行写入， 基础的 SST 文件很可能会改变， 这将导致在备份中文件是损坏的。 创建 QuarkDB 快照 QuarkDB 提供了一个 raft-checkpoint 命令， 可以创建一个当前集群的快照，包含了状态机和日志，通过它，我们可以很方便地创建另一个 QuarkDB 实例： 127.0.0.1:7777> raft-checkpoint /path/to/backup 需要注意的是， /path/to/backup 要与 /var/lib/quarkdb 处于同一个物理文件系统中， 这将允许对 SST 文件做硬链接，从而创建快照的时间非常短， 即使 DB 有半 TB，也只会耗时几秒钟，同时也不占额外的空间。 备份 QuarkDB 快照 在做完快照后， 就可以通过 rsync 等将 快照备份到另外的节点上去。 $ rsync -Paz /path/to/backup backup-node:/quarkdb/backup/dir 备份完成后， 记得将 /path/to/backup 删除掉。 否则随着 QuarkDB 持续写入， /var/lib/quarkdb 和 /path/to/backup 会很快分叉， /path/to/backup 也会很快开始占用额外空间。 恢复 QuarkDB 数据 如果 EOS 或 QuarkDB 集群出了问题， 需要恢复数据， 我们可以重新启动一个 QuarkDB 节点或集群， 导入备份的 QuarkDB 快照， 将 EOS 集群指向新的 QDB 集群即可。 可参考 配置 EOS 使用 QuarkDB 作为 NS。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"quarkdb sql"},{"title":"部署并运行 Redis 数据库集群","url":"/2018/Deploy-And-Manage-A-Redis-Cluster/","text":"Redis 是一个开源的、轻量级的、高性能的、基于 键 - 值 对的缓存与存储系统，非常适合用来缓存和存储服务和消息队列、任务队列等。 安装 Redis 目前最新的稳定版是 5.0.0. CentOS 7 的源中最新版为 3.2.12. 如果不需要新特性, 可以使用官方的版本. 这里我们使用 5.0.0 版本. $ wget http://download.redis.io/releases/redis-5.0.0.tar.gz $ tar xf redis-5.0.0.tar.gz $ cd redis-5.0.0 && make -j4 $ make test && sudo make install Redist 集群介绍 Redis 集群简介 Redis 集群所用 TCP 端口 每一 Redis 集群节点都要两个 TCP 连接。 一个 TCP 端口为数据端口，用于服务客户端， 默认为 6379， 一个为 数据端口 +10000, 用于集群节点间的通信，默认就是 16379， 增量 10000 是固定的。 第二个端口是专门用来节点间通信的集群总结 (Cluster bus)，使用一个二进制协议。 Cluster bus 被用来作错误检测，配置更新，故障转移论证等等， 因此客户端应该永远不要使用这一端口，而是使用正常的数据端口。 如果开了防火墙， 需要在防火墙中打开这两个端口， 否则 Redis 集群无法正常工作。 Redis 集群的主－从模型 部署 Redis 集群 要部署 Redis 集群, 我们需要先有已经运行的 Redis 实例, 并且 这些实例处于 Cluster 状态. 这里, 我们会在四个节点上启动 8 个 Redis 实例. 启动 Redis 实例 安装好 Redis 后, 在 /etc/redis/ 下会有一个 redis.conf 的默认配置文件. 我们可以修改这个配置文件. 要部署一个 Redis 集群, 我们要修改的地方如下: port 6379 cluster-enabled yes cluster-config-file node-6379.conf cluster-node-timeout 5000 protected-mode no # bind 127.0.0.1 bind MY_IP ## or bind 0.0.0.0 在 Redis 源码包里有一个脚本 utils/redis_init_script, 可以比较方便地启动和关闭 Redis 服务: REDISPORT=6379 EXEC=/usr/local/bin/redis-server CLIEXEC=/usr/local/bin/redis-cli PIDFILE=/var/run/redis_${REDISPORT}.pid CONF=\"/etc/redis/${REDISPORT}.conf\" case \"$1\" in start) if [-f $PIDFILE] then echo \"$PIDFILE exists, process is already running or crashed\" else echo \"Starting Redis server...\" $EXEC $CONF fi ;; stop) if [! -f $PIDFILE] then echo \"$PIDFILE does not exist, process is not running\" else PID=$(cat $PIDFILE) echo \"Stopping ...\" $CLIEXEC -p $REDISPORT shutdown while [-x /proc/${PID} ] do echo \"Waiting for Redis to shutdown ...\" sleep 1 done echo \"Redis stopped\" fi ;; *) echo \"Please use start or stop as first argument\" ;; esac 在这里, 我们在 4 个节点上的 7000 和 8000 端口各分别启动一个 redis 实例: IP=(192.168.60.94 192.168.60.95 192.168.60.96 192.168.60.97) ports=(7000 8000) $ for ips in ${IP[*]}; do for port in ${ports[*]}; do ssh $ips mkdir -p /var/lib/redis/${port} cp redis_template.conf $port.conf cp redis_init.sh redis_$port sed -i -e \"s/6379/$port/g\" -e \"s/MY_IP/$ips/g\" $port.conf && scp $port.conf $ips:/etc/redis/ sed -i \"s/6379/$port/g\" redis_$port && scp redis_$port $ips:/etc/init.d/ ssh $ips /etc/init.d/redis_${port} start done done 启动好 8 个实例后, 我们就可以部署一个 4 主 4 从 的 Redis 集群了. 创建 Redis 集群 在有了 8 个正常运行的 Redis 实例后, 我们要做一些额外的事情来搭建 Redis 集群. 当前我们的 Redis 版本是 5.0.0, 其已经自带了 cluster 命令, 可以很方便地完成这一任务: $ redis-cli --cluster create 192.168.60.94:7000 192.168.60.95:7000 192.168.60.96:7000 192.168.60.97:7000 192.168.60.94:8000 192.168.60.95:8000 192.168.60.96:8000 192.168.60.97:8000 --cluster-replicas 1 如果使用的是 3 or 4 版本, 我们可以使用旧的工具 redis-trib.rb 来完成: $ gem install redis $ ./redis-trib.rb create --replicas 1 192.168.60.94:7000 192.168.60.95:7000 192.168.60.96:7000 192.168.60.97:7000 192.168.60.94:8000 192.168.60.95:8000 192.168.60.96:8000 192.168.60.97:8000 --cluster-replicas 1 or --replicas 1 表示我们想要每个主 redis 实例都有一个从 redis 实例. Redis-cli 会建议一个 cluster 配置, 如果没问题, 输入 yes, 集群就配置完成了, Redis 实例会进入到集群模式与其他各节点进行通信. 如果一切没有问题, 最后会出现如下的信息: [OK] All 16384 slots covered 这意味着 16384 个 slots 中的每一个都由一个 master 服务着. 使用 Redis 集群 查看节点系统信息 $ redis-cli -p 7000 info # Server redis_version:5.0.0 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:b70c7303801d62f5 redis_mode:cluster ... config_file:/etc/redis/7000.conf # Clients connected_clients:1 client_recent_max_input_buffer:2 client_recent_max_output_buffer:0 blocked_clients:0 # Memory used_memory:2679024 used_memory_human:2.55M used_memory_rss:9224192 used_memory_rss_human:8.80M ... # Persistence loading:0 rdb_changes_since_last_save:0 rdb_bgsave_in_progress:0 rdb_last_save_time:1541496914 ... # Stats total_connections_received:22 total_commands_processed:615 instantaneous_ops_per_sec:1 ... # Replication role:master connected_slaves:1 slave0:ip=192.168.60.94,port=8000,state=online,offset=980,lag=0 master_replid:7f97949f8203a2f9532edcdcbf606b48bab1e045 master_replid2:b69c6ebce2c36f14eb2d35ad78892ab12813c5ba ... # CPU used_cpu_sys:1.407291 used_cpu_user:1.276415 used_cpu_sys_children:0.001923 used_cpu_user_children:0.000961 # Cluster cluster_enabled:1 # Keyspace 查看集群信息 $ redis-cli -p 7000 cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:8 cluster_size:4 cluster_current_epoch:9 cluster_my_epoch:9 ... 查看集群节点信息 $ redis-cli --cluster -p 7000 cluster nodes 770a8abf7c80d4c0051205e4613ee273c8d9985d 192.168.60.95:8000@18000 slave 15b29ddac7bd12b5ce5f61b735b1e3d52e4b17eb 0 1541496916709 6 connected 09020f2b4a8394975c2c0bc6f0b13720a19e1cf4 192.168.60.96:7000@17000 master - 0 1541496916508 7 connected 10240-16383 9a8b796820f1cd81aec0271b585fa2b6d9b89fe7 192.168.60.96:8000@18000 slave c011bfaa93a3ce8fefbe07f9509909c81b57dfc3 0 1541496915000 9 connected 15b29ddac7bd12b5ce5f61b735b1e3d52e4b17eb 192.168.60.94:7000@17000 master - 0 1541496917000 3 connected 1024-4095 c011bfaa93a3ce8fefbe07f9509909c81b57dfc3 192.168.60.95:7000@17000 master - 0 1541496917711 5 connected 5120-8191 cfd2797de67a700f5cdb0add60b7d4d947035a5b 192.168.60.97:8000@18000 slave 09020f2b4a8394975c2c0bc6f0b13720a19e1cf4 0 1541496916000 7 connected 6fa54252c757c1d5f14ffd99d42d028608e41b35 192.168.60.94:8000@18000 slave 24bee81ae3fc08ba9262f4153fd56e5f1be5e795 0 1541496918712 9 connected 24bee81ae3fc08ba9262f4153fd56e5f1be5e795 192.168.60.97:7000@17000 myself,master - 0 1541496916000 9 connected 0-1023 4096-5119 8192-10239 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"sql redis"},{"title":"搭建并运行管理 QuarkDB 集群","url":"/2018/Deploy-And-Run-A-QuarkDB-Cluster/","text":"QuarkDB 是一个由 Cern 的 IT-ST 部门开发的高可用的 键 - 值 存储的内存数据库, 其实现了 redis 命令集的一个小的子集. QuarkDB 建立在 rocksdb 之上, rocksdb 则是一个强一致性的嵌入式的 键 - 值 存储系统. QuarkDB的高可用性是由多副本节点和 raft 分布一致性算法保证的. 下图是 CERN 给出的 EOS 未来 NS 的设计，使用 QuarkDB 作为 EOS 的元数据存储服务器： 下面就简要介绍一下如何搭建一个 QuarkDB 集群。 安装 QuarkDB 通过源安装 安装 QuarkDB 最简单的方式就是从源里进行安装. 如果运行的是 CentOS 7, 将如下内容保存到 quarkdb.repo, 放在 /etc/yum.repos.d下. [quarkdb-stable] name=QuarkDB repository [stable] baseurl=http://storage-ci.web.cern.ch/storage-ci/quarkdb/master/el7/x86_64/ enabled=1 gpgcheck=False [quarkdb-stable-debug] name=QuarkDB repository [stable debug] baseurl=http://storage-ci.web.cern.ch/storage-ci/quarkdb/master/el7/x86_64/ enabled=1 gpgcheck=False 然后通过如下命令安装 sudo yum install -y quarkdb quarkdb-debuginfo 通过源码编译安装 下载源码包 git clone https://gitlab.cern.ch/eos/quarkdb.git && cd quarkdb git submodule update --recursive --init 安装依赖包 sudo yum install -y expect jemalloc libzstd xrootd xrootd-client-libs xrootd-libs xrootd-selinux xrootd-server xrootd-server-libs sudo yum install -y lz4-devel lz4-static lz4 jemalloc jemalloc-devel zstd libzstd libzstd-devel libdwarf-static libdwarf-devel libdwarf-tools 安装 devtoolset-7 yum install centos-release-scl && yum install devtoolset-7 && source /opt/rh/devtoolset-7/enable 编译测试安装 QuarkDB mkdir build && cd build cmake .. make ./test/quarkdb-tests 搭建 QuarkDB 集群 初始化数据库文件夹 假定我们在 qdb-1:7777, qdb-2:7777, qdb-3:7777 三台机器上部署 QuarkDB 集群, 确定集群的名字 --clusterID 可由 uuid 产生, 比如 0c01a140-b3ff-11e8-86bb-9cb6d0e634f1, 一个唯一的 clusterID 能够避免集群节点误与其他集群交流. 首先, 我们要在每一个节点上初始化数据库文件夹. 在每一个节点上运行如下命令: for i in 1 2 3;do ssh qdb-${i} \"sudo quarkdb-create --path /var/lib/quarkdb/node-$i --clusterID 0c01a140-b3ff-11e8-86bb-9cb6d0e634f1 --nodes qdb-1:7777,qdb-2:7777,qdb-3:7777\" ssh qdb-${i} \"sudo mkdir -p /var/log/quarkdb/ && sudo chown -R xrootd:xrootd /var/log/quarkdb\" ssh qdb-${i} \"sudo mkdir -p /var/spool/quarkdb/ && sudo chown -R xrootd:xrootd /var/log/quarkdb\" done 每个节点上的命令都要保持 --clusterID 和 --nodes 一直. 修改数据库文件夹权限: for i in 1 2 3;do ssh qdb-$i \"sudo chown -R xrootd:xrootd /var/lib/quarkdb/node-$i\" done 配置 QuarkDB 实例 QuarkDB 使用 XrootD 作为通信后端, 所以其配置跟 EOS 等类似. 每个节点上的配置文件如下(raft 模式) xrd.port 7777 xrd.protocol redis:7777 libXrdQuarkDB.so redis.mode raft redis.database /var/lib64/quarkdb/node-x redis.myself qdb-x:7777 将其保存到 /etc/xrootd/xrootd-quarkdb.cfg, 然后通过 systemd 启动 quarkdb 实例 for i in 1 2 3;do cat < EOF > xrootd-quarkdb.cfg xrd.port 7777 xrd.protocol redis:7777 libXrdQuarkDB.so redis.mode raft redis.database /var/lib/quarkdb/node-$i redis.myself qdb-$i:7777 EOF cat xrootd-quarkdb.cfg | ssh qdb-$i \"sudo tee /etc/xrootd/xrootd-quarkdb.cfg\" ssh qdb-$i sudo systemctl enable --now xrootd@quarkdb done set mykey myval OK qdb-1.example.com:7777> get mykey \"myval\" 管理 QuarkDB 集群 增加节点 在新节点上运行创建集群的命令, --nodes 选项中不要有新的节点， $ sudo quarkdb-create --path /var/lib/quarkdb/node-4 --clusterID 0c01a140-b3ff-11e8-86bb-9cb6d0e634f1 --nodes qdb-1:7777 qdb-2:7777 qdb-3:7777 ... $ sudo systemctl enable --now xrootd@quarkdb.service 在 leader 节点上使用 raft_add_observer 命令添加新节点为 observer $ redis-cli -h leader -p 7777 raft_add_observer qdb-4:7777 在新节点成为 observer 并上线后，提升其为 follower： $ redis-cli -h leader -p 7777 raft_info $ redis-cli -h leader -p 7777 raft_promote_observer qdb-4:7777 移除节点 假设我们要移除 qdb-4.example.com $ redis-cli -h leader -p 7777 raft_remove_memeber qdb-4:7777 需要注意的是， 不能移除 leader. 密码认证 默认的 QuarkDB 是可以接受所有连接的， 这很不安全。 我们可以为 QuarkDB 启用密码认证。 在原有的配置文件中加入如下一行： redis.password_file /etc/xrootd/quarkdb.passwd 其中 /etc/xrootd/quarkdb.passwd 是密码文件，主属需要设置为 xrootd:xrootd, 权限为 400。 或者加入下面一行： redis.password your-quarkdb-password 选项后面接的就是认证密码。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux quarkdb redis"},{"title":"CentOS 和 Fedora 下连接深信服 SSLVPN","url":"/2018/Connect-SSLVPN-under-CentOS-and-Fedora/","text":"目前很多公司和大学都采用 深信服的 sslvpn 来为校外的用户提供服务访问校内资源. Windows 和 macOS 下有 easyconnect 可以很方便地登陆 vpn. 但是 Linux 下访问却不是那么容易. 这这篇文章会介绍如何在 Fedora, CentOS 下连接深信服的提供的 sslvpn. update 深信服最近更新了其 SSLVPN 服务, 提供了 Linux 下的客户端, 但 RPM 包貌似是 rhel6 版的… 目前的深信服的 sslvpn 只支持 jre-1.6.27, 以浏览器 NPAPI 类插件的形式运行的, 而大部分的现代浏览器早已不支持这种形式插件, 我们需要下载 古老 的 opera-10.60, 这也是深信服推荐的版本. 获取并安装 opera 10.60 wget http://mirror.olnevhost.net/pub/opera/linux/1060/opera-10.60-6386.x86_64.rpm sudo rpm -ivhU opera-10.60-6386.x86_64.rpm 安装 jre-6u27-linux-x64.bin chmod +x jre-6u27-linux-x64.bin sudo ./jre-6u27-linux-x64.bin 关联浏览器与 JRE $ sudo rm -rf /usr/lib/mozila/plugins/libnpjp2.so $ sudo ln -sf /usr/java/jre1.6.0_27/lib/i386/libnpjp2.so /usr/lib/mozila/plugins/ 安装相应的 32 位软件包 $ sudo dnf install libnsl.i686 -y 以 root 权限打开 Opera, 进入 sslvpn 登陆界面, 登陆 sslvpn. sudo opera https://your-ssl-vpn-address >/dev/null 2>&1 & 如果一切正常, 现在就可以访问内网资源啦… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux sslvpn"},{"title":"Ceph 中的数据灾难重建","url":"/2018/Ceph-Data-Disaster-Recovery/","text":"数据的备份和灾难重建是数据存储中非常重要的环节. to be continued… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph storage"},{"title":"Ceph RDB 的快照、增量备份与恢复","url":"/2018/Ceph-RBD-Incremental-Backup-and-Restore/","text":"Ceph 提供了一些特性和接口, 可以让我们很方便地实现 集群的备份与恢复. 一种是 通过集群镜像, 可以做到实时备份, 但是这个对网络要求比较高, 运行一个镜像集群资源消耗也比较大. 还有一种就是通过 RBD 的快照技术, 实现对 RBD 的数据备份与恢复. RBD 的快照功能 对于快照, 熟悉存储系统和虚拟机的童鞋肯定会有印象. 快照一般是基于时间点或事件对系统做一个标记, 然后在需要的时候, 将状态恢复到标记时的点. 快照的实现前提是底层系统没有损坏… Noting that (seeing rbd-backup): 快照 的基本操作 对于镜像快照的操作, 一般是要指定 pool 名字和 镜像名字. Pool 名字通过 --pool 或者 -p 指定. 创建快照 要创建一个快照, 使用 rbd snap create 命令, 基本语法是 ### Usage: [ceph@ceph0 ~]$ rbd -p {pool-name} snap create --sanp {snap-name} {image-name} ### or simply [ceph@ceph0 ~]$ rbd snap create {pool-name}/{image-name}@{snap-name} ### e.g. [ceph@ceph0 ~]$ rbd -p rbd snap create --snap foo.2018.09.12.20.30.30 foo ### or [ceph@ceph0 ~]$ rbd snap create rbd/foo@foo.2018.09.12.20.30.30 列出快照 列出一个镜像的快照, 要指定 Pool 名字和 image 名字. ### Usage: [ceph@ceph0 ~]$ rbd -p {pool-name} snap ls {image-name} ### or [ceph@ceph0]$ rbd snap ls {pool-name}/{image-name} ### e.g. [ceph@ceph0]$ rbd -p rbd snap ls foo ### or [ceph@ceph0]$ rbd snap ls rbd/foo 快照回滚 使用 rbd snap rollback 对 image 进行回滚操作, 恢复数据, ### Usage: [ceph@ceph0]$ rbd -p {poo-name} snap rollback --snap snap-name {image-name} ### or [ceph@ceph0]$ rbd snap rollback {pool-name}/{image-name}@{snap-name} ### e.g. [ceph@ceph0]$ rbd -p rbd snap rollback --snap snapname foo ### or [ceph@ceph0]$ rbd snap rollback rbd/foo@snapname Ceph 建议使用 快照克隆 功能而不是 回滚操作来还原数据, 因为 随着镜像大小增加, 回滚操作时间会越来越长. 相比之下, 快照克隆 要快很多. 删除快照 要删除一个快照, 使用 rbd snap rm 命令. ### Usage: [ceph@ceph0]$ rbd -p {pool-name} snap rm --snap {snap-name} {image-name} ### or [ceph@ceph0]$ rbd snap rm {pool-name}/{image-name}@{snap-name} ### e.g. [ceph@ceph0]$ rbd -p rbd snap rm --snap snapname foo ### or [ceph@ceph0]$ rbd snap rm rbd/foo@snapname Ceph OSDs 删除 快照过程是异步的, 删除一个快照并不能立刻释放空间. 删除所有快照 要删除所有快照, 可以使用 rbd snap purge 命令. ### Usage: [ceph@ceph0]$ rbd -p {pool-name} snap purge {image-name} ### or [ceph@ceph0]$ rbd snap purge {pool-name}/{image-name} ### e.g. [ceph@ceph0]$ rbd -p rbd snap purge foo ### or [ceph@ceph0]$ rbd snap purge rbd/foo 快照层次化 Ceph 支持对 rbd 快照创建很多 写时复制(Copy-on-write, COW) 克隆. 快照层次化能够使 Ceph 块设备客户端非常快速地建立镜像. 比如你可以创建一个块设备, 让一个 Linux VM 对其读写； 然后对镜像建立快照, 保护这一快照, 这样你就可以创建任意多的 COW 克隆体. 图中的 parent 表示 Ceph 块设备(parent), 相应的child) 则表示克隆镜像. 这两个术语非常重要. 每一个克隆镜像(child) 存着一份对 parent 快照的引用, 这使得克隆 image 能够打开 parent 快照读取内容. 一个 COW 的克隆体就像其他正常的块设备 image 一样. 你可以读取, 写入数据, 或者调整大小, 没有什么特别的限制. 然而, COW 克隆 镜像指向 快照, 因此必须要在克隆快照前保护快照. 整个流程如下: 保护快照 克隆体能够访问 父母快照. 如果 parent 快照被删除了, 所有克隆体都会损坏. 因此为了防止数据丢失, 需要保护要克隆的快照. ### Usage: [ceph@ceph0]$ rbd -p {pool-name} snap protect --image {image-name} --snap {snap-name} ### or [ceph@ceph0]$ rbd snap protect {pool-name}/{image-name}@{snap-name} ### e.g. [ceph@ceph0]$ rbd -p rbd snap protect --image test-image --snap test-image.snap ### or [ceph@ceph0]$ rbd snap protect rbd/test-image@test-image.snap 克隆快照 这之后就可以对快照进行克隆了. ### Usage: [ceph@ceph0]$ rbd clone -p {pool-name} --image {image-name} --snap {snap-name} --dest-pool {dest-pool-name} --dest {child-image} ### or [ceph@ceph0]$ rbd clone {pool-name}/{parent-image}@{snap-name} {dest-pool-name}/{child-image-name} ### e.g. [ceph@ceph0]$ rbd -p rbd --image test-image --snap test-image.snap --dest-pool dest-rbd --dest dest-test-image ### or [ceph@ceph0]$ rbd clone rbd/test-image@test-image.snap dest-rbd/dest-test-image 列出快照的克隆体 ### Usage: [ceph@ceph0]$ rbd -p {pool-name} children --image {image-name} --snap {snap-name} ### or [ceph@ceph0]$ rbd children {pool-name}/{image-name}${snapshot-name} ### e.g [ceph@ceph0]$ rbd -p rbd children --image test-image --snap test-image.snap ### or [ceph@ceph0]$ rbd children rbd/test-image@test-image.snap 平坦化克隆镜像 克隆的镜像保持着对 parent 快照的引用, 移除 child 克隆体对 parent 快照的引用实际上就是通过从快照拷贝信息到克隆体来将这个镜像 平坦化(flattening). ### Usage: [ceph@ceph0]$ rbd -p {pool-name} flatten --image {image-name} ### or [ceph@ceph0]$ rbd flatten {pool-name}/{image-name} ### e.g. [ceph@ceph0]$ rbd -p dest-rbd --image dest-test-image ### or [ceph@ceph0]$ rbd flatten dest-rbd/dest-test-image 一个 flattened 的 image 占据的存储空间要比 layered 克隆体大. 取消对快照的保护 在删除一个受保护的快照时, 需要先取消对快照的保护. 除此之外, 你还需要将与之有引用关系的克隆体平坦化(flatten), 再删除快照. [ceph@ceph0]$ rbd -p {pool-name} snap unprotect --image {image-name} --snap {snapshot-name} ### or [ceph@ceph0]$ rbd snap unprotect {pool-name}/{image-name}@{snapshot-name} 基于快照的增量备份 在了解了 Ceph RBD 的快照功能后, 我们就可以来看看如何通过快照来对 RBD image 进行备份. 过程也非常简单. 快照的创建和导出 我们先建一个测试 pool 和 image: [ceph@ceph0 ~]$ rbd mkpool test_pool [ceph@ceph0 ~]$ rbd create test_pool/test_image --size 200 --image-format 2 --order 24 [ceph@ceph0 ~]$ do_some_work 在时间节点 t1 和 t2 分别创建快照: [ceph@ceph0 ~]$ rbd snap create test_pool/test_image@test.snap.t1 [ceph@ceph0 ~]$ do_some_work [ceph@ceph0 ~]$ rbd snap create test_pool/test_image@test.snap.t2 导出 v1 与 v2 ## rbd export-diff --pool {pool name} --image {image name} --snap {snap name} {exported file name} ## or ## rbd export-diff {pool name}/{image name}@{snap name} {exported file name} [ceph@ceph0 ~]$ rbd export-diff test_pool/test_image@test.snap.t1 test_image_test.snap.t1 [ceph@ceph0 ~]$ rbd export-diff test_pool/test_image@test.snap.t2 test_image_test.snap.t2 Full export. 导出 v1 与 v2 的差异数据 ## rbd export-diff --from-snap snap1 --pool {pool name} --image {image name} --snap {snap name} {exported file name} ## or ### rbd export-diff --from-snap snap1 {pool_name}/{image_name}@{snap-name} exported_file_name ### diff from t1 to t2 [ceph@ceph0]$ rbd export-diff --from-snap test.snap.t1 test_pool/test_image@test.snap.t2 test_pool_test_image_snap.t1_to_snap.t2.diff ### diff from initial to t1 [ceph@ceph0]$ rbd export-diff test_pool/test_image@test_snap.t1 test_snap_t1 ### diff from initial to t2 [ceph@ceph0]$ rbd export-diff test_pool/test_image@test_snap.t2 test_snap_t2 导出之后, 就可以将 diff 文件传送 到备份服务器上保存起来. 你可以查看有哪些内容被修改了: [ceph@ceph0]$ rbd diff --from-snap test.snap.t1 test_pool/test_image@test.snap.t2 --format plain 导出快照时，可以使用 **–rbd-concurrent-management-ops** 来 并行化 进行: $ rbd export --rbd-concurrent-management-ops 20 --pool=test_pool test_image@test.snap.t2 快照的导入与数据恢复 为了简单, 我们在 test-pool 下新建一个 image, 再导入 diff 文件: [ceph@ceph0 ~]$ rbd create test-pool/backup_image --size 1 --image-format 2 --order 24 ### import diff from initial to t1 [ceph@ceph0 ~]$ rbd import-diff test_snap_t1 test_pool/backup_image ### import diff from t1 to t2 [ceph@ceph0 ~]$ rbd import-diff test_pool_test_image_snap.t1_to_snap.t2.diff test_pool/backup_image ### or import diff from initial to t2 [ceph@ceph0 ~]$ rbd import-diff test_snap_t2 test_pool/backup_image 这样就可以恢复数据了. 要是运行着两个 Ceph 集群来预防灾难并在灾难后做数据恢复, 可以在线的方式对其做快照备份: [ceph@ceph0]$ rbd export-diff --from-snap test.snap.t1 test_pool/test_image@snap.t2 - | ssh user@second_cluster rbd import-diff - test_pool/test_image 如果想要使用快照覆盖原 image 来恢复数据，可以使用前面提到的 rollback： [ceph@ceph0]$ rbd snap rollback rbd/foo@snapname 备份整个 RBD Pool 上面是对一个 image 的备份和还原, 在实际中, 会有非常多的 image 需要备份. 有了上面的例子, 我们可以很容易地写一个脚本来定时备份 RBD pool. Backup tool: backy2 rbd-backup ceph-backup ceph-rbd-backup ceph-vm-backup document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph rados"},{"title":"在 Linux 下监控程序修改文件","url":"/2018/Monitor-File-Change-under-Linux/","text":"工作中, 有时候我们需要知道某个文件是否被修改了, 被哪个程序修改了. Linux 下可以很方便地监控某个文件被修改的记录. 根据目的不同, Linux 下有不同的监视文件或文件夹的方案. A. Auditd Auditd 可以很方便监控记录哪些程序和用户对指定文件 (即使不存在) 做的操作. 安装 auditd 一般发行版软件仓库里都会有 auditd 安装包. 如果默认没有安装, 可以很方便地利用包管理器来安装 ## Fedora $ sudo dnf install -y audit ## CentOS $ sudo yum install -y audit ## Debian / Ubuntu $ sudo apt install -y audit 启动 audit 服务 $ sudo systemctl enable --now auditd 检查 audit 服务状态 $ sudo systemctl status auditd ● auditd.service - Security Auditing Service Loaded: loaded (/usr/lib/systemd/system/auditd.service; enabled; vendor preset: enabled) Active: active (running) since Fri 2018-09-07 08:47:53 CST; 10min ago Docs: man:auditd(8) https://github.com/linux-audit/audit-documentation Process: 28126 ExecStartPost=/sbin/augenrules --load (code=exited, status=0/SUCCESS) Process: 28121 ExecStart=/sbin/auditd (code=exited, status=0/SUCCESS) Main PID: 28122 (auditd) Tasks: 5 (limit: 4915) Memory: 3.3M CGroup: /system.slice/auditd.service ├─28122 /sbin/auditd ├─28124 /sbin/audispd └─28127 /usr/sbin/sedispatch 这就表示服务正常. 配置 audit 监视规则 为了监视文件的访问操作, 需要在/etc/audit/rules.d/audit.rules 添加如下规则, 其中 perm 是要监控的操作, key 是在查寻时指定的关键字, path 是要监控的文件路径. $ vi /etc/audit/rules.d/audit.rules -a always,exit -F path=/path/to/file -F perm=warx -F key=keyword-for-filter-log 重启 audit 服务 ### Note: use service instead of systemctl $ sudo service auditd restart 检查 audit 规则. $ sudo auditctl -l -a always,exit -F path=/path/to/file -F perm=warx -F key=keyword-for-filter-log 检查文件操作记录 假如我们监控 /root/test.txt 的操作, 规则是 -a always,exit -F path=/root/test.txt -F perm=warx -F key=test-filter 然后对其做一些操作 $ ls /root/test.txt $ cat /root/test.txt $ touch /root/test.txt 然后通过如下命令查询操作记录. $ ausearch -k test-filter time->Fri Sep 7 09:23:24 2018 type=PROCTITLE msg=audit(1536283404.017:39844): proctitle=63617400746573742E747874 type=PATH msg=audit(1536283404.017:39844): item=0 name=\"test.txt\" inode=1587579 dev=103:08 mode=0100644 ouid=0 ogid=0 rdev=00:00 nametype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=CWD msg=audit(1536283404.017:39844): cwd=\"/root\" type=SYSCALL msg=audit(1536283404.017:39844): arch=c000003e syscall=257 success=yes exit=3 a0=ffffff9c a1=7fff145999ed a2=0 a3=0 items=1 ppid=32477 pid=10767 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts4 ses=3 comm=\"cat\" exe=\"/usr/bin/cat\" key=\"test-filter\" ---- time->Fri Sep 7 09:27:22 2018 type=PROCTITLE msg=audit(1536283642.307:40008): proctitle=6C73002D2D636F6C6F723D6175746F type=PATH msg=audit(1536283642.307:40008): item=0 name=\"test.txt\" inode=1587579 dev=103:08 mode=0100644 ouid=0 ogid=0 rdev=00:00 nametype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=CWD msg=audit(1536283642.307:40008): cwd=\"/root\" type=SYSCALL msg=audit(1536283642.307:40008): arch=c000003e syscall=191 success=no exit=-61 a0=7ffcd0a58c40 a1=7f4b30ece210 a2=7ffcd0a58c00 a3=14 items=1 ppid=1617 pid=12314 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts6 ses=3 comm=\"ls\" exe=\"/usr/bin/ls\" key=\"test-filter\" ---- time->Fri Sep 7 09:27:29 2018 type=PROCTITLE msg=audit(1536283649.775:40009): proctitle=746F75636800746573742E747874 type=PATH msg=audit(1536283649.775:40009): item=1 name=\"test.txt\" inode=1587579 dev=103:08 mode=0100644 ouid=0 ogid=0 rdev=00:00 nametype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=PATH msg=audit(1536283649.775:40009): item=0 name=\"/root\" inode=1569793 dev=103:08 mode=040550 ouid=0 ogid=0 rdev=00:00 nametype=PARENT cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=CWD msg=audit(1536283649.775:40009): cwd=\"/root\" type=SYSCALL msg=audit(1536283649.775:40009): arch=c000003e syscall=257 success=yes exit=3 a0=ffffff9c a1=7fff075909e1 a2=941 a3=1b6 items=2 ppid=1617 pid=12367 auid=1000 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts6 ses=3 comm=\"touch\" exe=\"/usr/bin/touch\" key=\"test-filter\" 我们可以看到都有 cat, ls, touch 在相应的时间对其做了什么操作. aureport 可以给出更直观的报告, $ ausearch -k test_filter | aureport -f -i 33. 09/07/2018 09:56:23 test.txt getxattr no /usr/bin/ls amito 41596 34. 09/07/2018 09:56:26 test.txt openat yes /usr/bin/cat amito 41600 35. 09/07/2018 09:56:28 test.txt openat yes /usr/bin/touch amito 41607 Systemtap systemtap 功能非常强大, 可以监控系统调用, 也可检测文件的修改操作. 但是需要安装系统的 debug 模块. 可能需要在相应的 repo 中 设置 debug 为enabled=1 安装 Systemtap ## Fedora $ sudo dnf install -y kernel-debuginfo kernel-debuginfo-common systemtap systemtap-client systemtap-runtime systemtap-devel ## CentOS $ sudo yum install -y kernel-debuginfo kernel-debuginfo-common systemtap systemtap-client systemtap-runtime systemtap-devel 获取文件信息 $ stat -c '%D %i' test.txt 10308 1587579 Systemtap 监控脚本 下面这个脚本可以监控文件的修改记录. 将脚本保存为 inodewatch.stp #! /usr/bin/env stap probe vfs.write, vfs.read { # dev and ino are defined by vfs.write and vfs.read if (dev == MKDEV($1,$2) # major/minor device && ino == $3) printf (\"%s(%d) %s 0x%x/%u\\n\", execname(), pid(), ppfunc(), dev, ino) } 监控文件 $ sudo stap sudo stap inodewatch.stp 0x103 0x08 1587579 其中连个十六进制数字 0x103 0x08 是上面得到的文件设备 ID 10308. 现在对 test.txt 做一些操作. vim test.txt cat test.txt 脚本会给出如下类似的信息 vim(22230) vfs_read 0x10300008/1587579 vim(22230) vfs_read 0x10300008/1587579 vim(22230) vfs_read 0x10300008/1587579 vim(22230) vfs_read 0x10300008/1587579 vim(22230) vfs_read 0x10300008/1587579 vim(22230) vfs_write 0x10300008/1587579 cat(22303) vfs_read 0x10300008/1587579 cat(22303) vfs_read 0x10300008/1587579 我们可以看到 vim, cat 对 test.txt 做了一些读写操作. inotifywait inotifywait 是 inotify-tools 里带的工具可以监控文件的修改建立等等. 安装 inotify-tools $ sudo dnf install -y inotify-tools # or $ sudo apt install -y inotify-tools 监控 文件夹下的文件产生, 修改, 删除 $ inotifywait -m /path/to/watch -r -e create -e moved_to -e delete -e modify | while read path action file; do $ ... $ your action here $ ... $ done 比如, 在特定文件 test.txt 产生时, 给特定邮箱发邮件, 并删除它 $ inotifywait -m /path -e create -e moved_to | while read path action file; do $ echo \"The file '$file' appeared in directory '$path' via '$action'.\" $ if [[$file == \"test.txt\"]]; then $ echo \"The file '$file' appeared in directory '$path' via '$action'. Deleting\"|mutt -s \"Deleting $file\" your_email $ sudo rm -rf /path/test.txt $ fi $ done document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux systemtap"},{"title":"Ceph 中的 Pools 和 PGs","url":"/2018/Pools-and-Placement-Groups-in-Ceph/","text":"在部署完 Ceph 集群后, Ceph 会默认建有一个存储池 Pool. Pool 能够为我们提供一些需要的功能. Ceph 中的 Pool 如果没有创建一个 Pool, Ceph 会默认有一个叫 rbd 的 Pool. Ceph 中的 Pool 可以提供以下功能: 弹性 (Resilience): 可以设置允许多少 OSDs 错误而不丢失数据. 位置群组 (Placement Groups): CRUSH 规则: 快照 (Snapshot): Ceph pool 支持对 Pool 创建快照. 所有权 (Owership): 可以设定一个 user 作为一个 pool 的拥有者. Pool 的基本操作 你可以 列举(list), 建立(create), 移走(remove) pools. 列出所有 Pools 列出集群中的 pools, $ ceph osd lspools 在新部署的集群上, 只有 rbd 一个 pool. 建立一个 Pool 在创建一个 pool 之前, 需要了解 pool 的 Placement Groups 和 Number of Placement Groups 的测略. 每个 osd 建议 设置 100pg, 但 pg 总数 要除以副本数. 比如, 11 个 osd, size 设置为 3, 那么 pg 数应该设置为 (100*11)/3=367 ~ 400. 官方建议重写 ceph 配置文件里默认的 pg 数, 因为默认的不总会是最优的. 修改配置文件中的如下选项 osd pool default pg num = 256 osd pool default pgp num = 256 官方给的推荐是 少于 5 OSDs, pg_num 设为 128 5 到 10 OSDs, pg_num 设为 512 10 到 50 OSDs, pg_num 设为 5096 超过 50 OSDs, 需要自己平衡计算 pg_num Ceph 官方提供了一个工具 pgcalc 来计算 pg_num 同时, 可以设置默认的副本数和最小副本数: osd pool default size = 3 osd pool default min size = 2 创建一个 pool, $ ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] [crush-ruleset-name] [expected-num-objects] $ ceph osd pool create {pool-name} {pg-num} erasure [erasure-code-profile] [crush-rulset-name] [expected-num-objects] 各个参数的意义如下: {pool-name} desc: Pool 的名字, 不能重复. type: string. required: yes. {pg-num} desc: Pool 的 Placement groups 数, 默认为 8, 基本不能满足需要, 一般要重写 type: integer required: yes default: 8. {pgp_num} desc: 为配置目的而设的 pg 总数. 一般等于 pg 数. type: integer. required: yes. 如果没有指定, 则为默认值. default: 8. {replicated|erasure} desc: 标明 Pool 的类型是多副本 (replicated) 的以能够从损坏的 OSDs 中恢复数据, 还是 消除(erasure) 的以获得 广义的 RAID5 兼容性. type: string. require: no. default: replicated. [crush-ruleset-name] desc: crush ruleset 的名字. 指定的 ruleset 必须存在. type: string required: no default: 对于 replicated pools, 其值是由 osd pool default crush replicated ruleset 设定. 对于 erasure pools, 如果 [erasure-code-profile=profile] desc: type: string. required: no. [expected-num-objects] desc: xxx type: integer. -required: no. default: 0, no splitting at the pool creation time. 例如, 创建一个 名叫 test-pool, pg 和 pgp 为 128 的 Pool, $ ceph osd pool create test-pool 128 128 重命名一个 Pool 对一个 Pool 重命名很简单, $ ceph osd pool rename test-pool test-pool-new 但如果有一个 user 对原 pool 配置了权限, 需要先更新 user 的权限到新的 pool, 再重命名 pool. 设置 Pool 配额 可以对一个 Pool 的 Object 个数和容量大小设置限制. ### Object $ ceph osd pool set-quota test-pool max_objects 10000 ### Disk usage limit: 100G $ ceph osd pool set-quota test-pool max_bytes $((100 * 1024 * 1024 * 1024)) ### check pool quota $ ceph osd pool get-quota test-pool #### cancel objects quota $ ceph osd pool set-quota test-pool max_objects 0 ### cancel disk usage quota $ ceph osd pool set-quota test-pool max_bytes 0 删除一个 Pool 除一个 Pool 会同时清空该 Pool 下所有的数据, 是非常危险的操作(想想 rm -rf /). 因此在删除 Pool 时, 需要输入 Pool 名字两次, 再加上 --yes-i-really-really-mean-it $ ceph osd pool delete pool_to_delete pool_to_delete --yes-i-really-really-mean-it 为了能删除一个 Pool, 你还必须在配置文件中设置 mon_allow_pool_delete 为 true, 否则无法删除 pool. mon_allow_pool_delete = true 更改配置文件后, 你需要重启所有 Ceph 服务. 查看 Pool 状态 $ rados df 创建删除快照 Ceph 支持对 整个 Pool 创建快照, 作用于该 Pool 下的所有对象. Ceph 中 Pool 有两种模式: Pool Snapshot, 建立一个 Pool 时的默认模式, 也即下面要讨论的模式. Self Managed Snapshot, librbd 管理的 snapshot. 如果在 Pool 中创建过 rbd 对象, 该 Pool 会自动转化为这种模式. 注意, 这两种模式是互斥的. 若对 Pool 创建了快照, 则不能创建 rbd 对象; 若在 Pool 中创建了(过) rbd 对象, 则不能再对 Pool 做快照. ### create a snapshot $ ceph osd pool mksnap test-pool test-pool-snapshot ### delete a snapshot $ ceph osd pool rmsnap test-pool test-pool-snapshot 从快照里恢复文件 ### 写入文件 [ceph@ceph0 ~]$ rados -p test-pool put testfile /etc/hosts ### 查看文件 [ceph@ceph0 ~]$ rados -p test-pool ls testfile ### 创建快照 [ceph@ceph0 ~]$ ceph osd pool mksnap test-pool test-pool-snapshot001 ### 列出快照 [ceph@ceph0 ~]$ ceph osd pool lssnap test-pool test-pool-snapshot001 xxxx.xx.xx xx:xx:xx #### 删除文件 [ceph@ceph0 ~]$ rados -p test-pool rm testfile #### 从快照恢复文件 [ceph@ceph0 ~]$ rados rollback -p test-pool testfile test-pool-snapshot001 rolled back pool test-pool to test-pool-snapshot001 ### 确认结果 [ceph@ceph0 ~]$ rados -p test-pool ls testfile 设置获取 Pool 属性 Pool 的属性可以通过如下命令语法设置 $ ceph osd pool set {pool-name} {key} {value} 例如, 设置 Pool 的冗余副本数 $ ceph osd pool set test-pool size 3 Pool 的属性可以通过如下命令语法获得, $ ceph osd pool get {pool-name} {key} {value} 例如, 获取 Pool 的 pg_num, $ ceph osd pool get test-pool pg_num Pool 的属性有: size desc: Pool 中对象存储的副本数. replicated pools only. type: integer default: 3 min_size desc: 可以进行读写的最小副本数. replicated pools only. type: crash_replay_interval desc: 允许客户端回应应答的秒数, 除了未授权的请求除外. type: integer pg_num: Pool 建立之时指定, 只能增大?? pgp_num crush_ruleset haspspool desc: (取消)设置 HASHPSPOOL 标志. type: integer, 1 for setting flags, 0 for unsetting flag. nodelete desc: (取消)设置 NODELETE 标志. 标志一个 Pool 是否可以被删除. type: integer. 1 for setting flag, 0 for unsetting flag nopgchange desc: (取消)设置 NOPGCHANGE 标志. 标志一个 Pool 是否可以更改 pg_num type: integer. 1 for setting flag, 0 for unsetting flag. nosizechange: desc: (取消)设置 NOSIZECHANGE 标志. 标志一个 Pool 是否可以改变 副本数. type: integer. 1 for setting flag, 0 for unsetting flag. write_fadvise_dontneed desc: (取消)设置 WRITE_FADVISE_DONTNEED 标志. type: integer. 1 for setting flag, 0 for unsetting flag. noscrub desc: (取消)设置 NOSCRUB 标志. type: integer. 1 for setting flag, 0 for unsetting flag. nodeep-scrub desc: (取消)设置 NODEEP_SCRUB 标志. type: integer. hit_set_type desc: 启用对 cache pools 的 hit set 追踪. type: string. value: bloom, explicit_hash, explicit_object. default is bloom. 具体的含义可参考官方文档。 Placement Groups 一个 Placement Group(PG) 将一系列的对象聚合到一个 群组里, 并将这个群组映射到一系列 OSDs 上去. 为什么要引入 GP 呢? 基于单个对象模式追踪对象的位置和元数据是 非常耗费计算资源 的, 一个有数以百万计对象的系统追踪位置和元数据是 完全不现实 的. 而 placement groups 则比较好地处理了性能和可扩展性的界限. 此外, placement groups 降低了 Ceph 必须存储和获取数据时的处理数和单个对象的元数据量. PG 会额外消耗一部分系统资源. 直接地: 每一个 PG 会需要一定量的内存和 CPU. 间接地: PGs 的总数会增加系统的 Peering 数. 增加 Pool 的 PG 数会减少集群中单个 OSD 的负载变化. 一般地, 只有一个 Pool 的情况下, 可以简单地用如下公式计算 PG 数: (OSDs * 100) Total PGs = ------------ Replicas 但是多个 Pools 的情况下, 你需要确保你平衡了单个 Pool 的 PG 数和 单个 OSD 上的 PG 数, 以找到一个合理的 PGs 总数, 能够在不加重系统负担或使 peer 过程太慢的情况下提供合理的单个 OSD 较低的变动. 设置 Pool 的 PG 数 Pool 的 PG 数必须在 Pool 建立之时就得指定. 获取 Pool 的 PG/PGP 数 ### pg_num $ ceph osd pool get {pool-name} pg_num ### pgp_num $ ceph osd pool get {pool-name} pgp_num 查看 PG 状态信息 [ceph@ceph0 ~]$ ceph pg stat 576 pgs: 576 active+clean; 0 B data, 17 GiB used, 17 GiB / 35 GiB avail 获取 集群的 PG 统计信息 $ ceph pg dump [--format ] format 为 plain 和 json 两者之一. 获取卡住的 PG 的统计信息 $ ceph pg dump_stuck inactive|unclean|stale [--format ] [-t|--threshold ] 处于 STUCK 状态的 pg 有三种 特定的状态: inactive: PGs 无法处理读写请求, 它们在等待拥有最新数据的 OSD 启动并加入进来. unclean: PGs 中有对象存储的副本数没有达到期望的副本数. stale: PGs 处于不可知状态. 存储它们的 OSDs 已经有一段时间没有向 Cluster 的 Monitors 报告状态了. format 有 plain 和 json两种格式. threshold 定义了 pg 进入 stuck 状态的最小秒数. 获取一个 PG 的 映射 获取指定 PG 的 映射(map): $ ceph pg map {pg-id} 如 [ceph@ceph0 ~]$ ceph pg map 1.17c osdmap e131 pg 1.17c (1.17c) -> up [6,5,9] acting [6,5,9] Ceph 会返回 PG map, PG id, 和 对应的 OSD 的状态. 获取一个 PG 的统计信息 $ ceph pg {pg-id} query 如 [ceph@ceph0 ~]$ ceph pg 1.17c query { \"state\": \"active+clean\", \"snap_trimq\": \"[]\", \"snap_trimq_len\": 0, \"epoch\": 131, \"up\": [ 6, 5, 9 ], \"acting\": [ 6, 5, 9 ], \"acting_recovery_backfill\": [ \"5\", \"6\", \"9\" ], ... ... \"scrub\": { \"scrubber.epoch_start\": \"0\", \"scrubber.active\": false, \"scrubber.state\": \"INACTIVE\", \"scrubber.start\": \"MIN\", \"scrubber.end\": \"MIN\", \"scrubber.max_end\": \"MIN\", \"scrubber.subset_last_update\": \"0'0\", \"scrubber.deep\": false, \"scrubber.waiting_on_whom\": []} }, { \"name\": \"Started\", \"enter_time\": \"2018-09-11 21:54:11.827282\" } ], \"agent_state\": {}} 擦洗一个 PG 如果你觉得 一个 PG 不太干净, 可以对这个 PG 进行擦洗: $ ceph pg scrub {pg-id} Ceph 会检查主副节点, 生成一个该 Pool 内的所有对象的日志, 相互比较以确保没有文件遗失或不匹配, 并且对象内容一致. 假定所有副本都匹配, 最后的语法扫描会确保所有与快照相关的元数据也一致. 错误会通过 log 记录下来. 恢复对象 LOST 状态 如果 集群已经失去了一个或多个 对象, 你决定放弃这个对象, 你应该标记这个对象为 lost. 如果所有可能的位置都已经查寻过, 仍没有找到, 你也许应该放弃这些丢失的对象. 现在只有一个选项受支持 - revert, 这会将一个对象回滚到之前的版本, 若新的对象的话则直接舍弃. 要将一个unfound 对象标记为 lost, 可以用下面的命令: $ ceph pg {pg-id} mark_unfound_lost revert 使用须谨慎. 这会让应用搞不清对象是否还在. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph storage linux"},{"title":"使用 Ceph 块存储作为 OpenStack 平台的存储后端","url":"/2018/Using-Ceph-RBD-as-A-Backend-for-OpenStack/","text":"Ceph 提供了很好的接口可以让 OpenStack 使用来作为其 Glance, Cinder, Cinder Backup 和 nova 服务的后端. 本篇主要介绍如何配置 Ceph 和 OpenStack 让两者协同工作, 主要内容搬运自 RedHat Prerequisites 本文档主要介绍配置 Ceph, QEMU, libvirt 和 OpenStack 使用 Ceph 作为后端存储服务. 为了使用 Ceph 块存储作为 OpenStack 后端, 我们需要 一个正常运行的 Ceph cluster 至少一个 OpenStack 节点 OpenStack 服务有三部分是与 Ceph 块存储整合在一起的, 镜像(images): OpenStack Glance 管理 虚拟机的镜像. 卷(volumes): 卷是块设备. OpenStack 使用卷来启动虚拟机, 或者将卷附加到运行的虚拟机上. 访客硬盘 (Guest Disks): 访客硬盘是访客操作系统的硬盘. 一般当虚拟机起动时, 其硬盘会出现在管理系统的文件系统中(通常在 /var/lib/nova/instances/&lt;uuid&gt;/&lt;/uuid&gt; 之下) OpenStack Glance 能够将镜像存储为一个 Ceph 块设备, 能够使用 Cinder 通过一个镜像的写时拷贝克隆镜像来起动虚拟机. Ceph 并不支持 QCOW2 格式的虚拟机硬盘. 为了支持从 Ceph 中起动虚拟机, Glance 镜像必须是 RAW 格式. OpenStack 可以使用 Ceph 作为镜像, 巻, 或访客硬盘的存储, 可以只用作部分的, 也可以用作全部的. 创建 Ceph 池 默认 Ceph 块设备会使用 rbd 池. 你也可心使用任何可用的池. 下面我们将分别为 Cinder, Cinder Backup, Glance 和 Nova 创建存储池. 根据情况更改 pg 值(可参考 Ceph Pools 中 PG 和 PGP 的设置) ### Adjust pg here 128 to your situation $ ceph osd pool create volumes 128; rbd pool init volumes $ ceph osd pool create backups 128; rbd pool init backups $ ceph osd pool create images 128; rbd pool init images $ ceph osd pool creaate vms 128; rbd pool init vms 如果 Ceph 显示 application not enabled on &lt;x&gt; pool(s) 等类似的 Warning，需要启用 Pool 的 rbd 属性 $ for i in volumes backups images vms; do ceph osd pool application enable $i rbd; done 安装和配置 Ceph 客户端 nova-compute, cinder-backup 和 cinder-volume 节点需要 Python bindings 和客户端的命令行工具. $ sudo yum install -y python-rbd $ sudo yum install -y ceph-common glance-api 节点则只需要安装 python-rbd 即可. 拷贝 Ceph 配置文件到 OpenStack 节点 运行着 glance-api, cinder-volume, cinder-backup 和 nova-compute 的节点相当于 Ceph 的客户端, 因而需要 Ceph 的配置文件, 将其拷贝到所有的 OpenStack 节点上. $ scp /etc/ceph/ceph.conf osp:/etc/ceph/ceph.conf 配置 Ceph Client 认证 在一个 Ceph mon 节点上, 为 Cinder, Cinder-Backup, Glance 和 Nova 建立各自的账号. $ ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rx pool=images' $ ceph auth get-or-create client.backup mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=backups' $ ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images' $ ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rx pool=images' 然后分别将 client.cinder, client.backup, 和 client.glance 三个密钥分发到相应的节点上. $ ceph auth get-or-create client.cinder | ssh cinder-volume-server sudo tee /etc/ceph/ceph.client.cinder.keyring $ ssh cinder-volume chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring $ ceph auth get-key client.cinder | ssh {cinder-node} tee client.cinder.key $ ceph auth get-or-create client.backup | ssh cinder-backup-server tee /etc/ceph/ceph.client.backup.keyring $ ssh cinder-backup-server chown cinder:cinder /etc/ceph/ceph.client.backup.keyring $ ceph auth get-or-create client.glance | ssh glance-api-server sudo tee /etc/ceph/ceph.client.glance.keyring $ ssh glance-api-server chown glance:glance /etc/ceph/ceph.client.glance.keyring $ ceph auth get-or-create client.nova | ssh nova-compute-server tee /etc/ceph/ceph.client.nova.keyring $ ssh nova-compute-servers chown nova:nova /etc/ceph/ceph.client.nova.keyring 在控制节点上， $ ssh {cinder-manager-node} $ uuidgen > cinder.uuid.txt #### cinder.xml `cat cinder.uuid.txt` client.cinder secret #### $ sudo virsh secret-define --file cinder.xml $ sudo virsh secret-set-value --secret $(cat cinder.uuid.txt) --base64 $(cat /etc/ceph/client.cinder.key) nova-compute 节点需要 client.nova 的密钥, libvirt 进程需要它在附加一个块设备时访问 Ceph 集群. $ ceph auth get-key client.nova | ssh {your-compute-node} tee client.nova.key 在计算节点上, 为密钥生成一个 UUID, 并保存到文件以便稍后对计算节点进行设置. $ ssh nova-compute-node $ uuidgen > nova.uuid.txt 注意, 为了保持一致性, 最好所有计算节点上的 UUID 一样. #### nova.xml `cat nova.uuid.txt` client.nova secret #### $ virsh secret-define --file nova.xml $ virsh secret-set-value --secret $(cat nova.uuid.txt) --base64 $(cat client.nova.key) 计算节点上也需要 client.cinder 的密钥和 cinder.xml. 将 client.volumes.key, cinder.uuid.txt 和 cinder.xml 复制到计算节点 $ sudo virsh secret-define --file cinder.xml $ sudo virsh secret-set-value --secret $(cat cinder.uuid.txt) --base64 $(cat /etc/ceph/client.cinder.key) 配置 OpenStack 使用 Ceph 配置 Cinder cinder-volume 节点需要 Ceph 块设备驱动, 卷池, 用户, 和 密钥的 UUID 来与 Ceph 块设备来进行交互. 我们依次进行设置. 修改 Cinder 配置文件, 在 [Default] 小节, 设置 Ceph 为 cinder 后端. $ vim /etc/cinder/cinder.conf $ ###### /etc/cinder/cinder.conf # enabled_backends = lvm enabled_backends = ceph 设置 Glance API 版本为 2. 如果有多个 cinder backend, 下面一行设置必须在 [Default] 区. glance_api_version = 2 更改 default_volume_type 为 ceph: # default_volume_type=iscsi default_volume_type = ceph 在/etc/cinder/cinder.conf 配置文件中, 创建 [ceph] 小节. 设定 volume_driver 使用 Ceph 块设备驱动. [ceph] volume_backend_name = ceph volume_driver = cinder.volume.drivers.rbd.RBDDriver 设定 Ceph 集群名字和配置文件名字 rbd_cluster_name = ceph rbd_ceph_conf = /etc/ceph/ceph.conf rbd_store_ceph_conf=/etc/ceph/ceph.conf 设定 OpenStack 存储 volumes 的 Ceph 池. 默认为 rbd. rbd_pool = volumes 设定 OpenStack 与 Ceph 交互的账号与上一节生成的密钥 UUID (cinder.uuid.txt ) rbd_user = cinder rbd_secret_uuid = your_secret_uuid 额外的一些设置. rbd_flatten_volume_from_snapshot = false rbd_max_clone_depth = 5 rbd_store_chunk_size = 4 rados_connect_timeout= -1 建议去掉 配置中 默认的 [lvm] 内容. 设置 Cinder Backup cinder-backup 节点需要一个特别的守护进程. 按照以下步骤对cinder-backup 进行配置. 打开 Cinder 配置文件, 并跳到 [ceph] 小节. $ vim /etc/cinder/cinder.conf 设定 backup_driver 为 Ceph 驱动. backup_driver = cinder.backup.drivers.ceph 设定 backup_ceph_conf 为 Ceph 集群的配置文件. backup_ceph_conf = /etc/ceph/ceph.conf 指定存储备份的 Ceph 池. backup_ceph_pool = backups 指定 cinder_backup 与 Ceph 交互的用户. backup_ceph_user = cinder-backup 其它的一些设置 backup_ceph_chunk_size = 134217728 backup_ceph_stripe_unit = 0 backup_ceph_stripe_count = 0 restore_discard_excess_bytes = true 检查 Cinder Backup 是否已启用, 没有则启用 Cinder Backup $ cat /etc/openstack-dashboard/local_settings*|grep enable_backup ## if False $ sed -i \"s|'enable_backup': False|'enable_backup': True|g\" /etc/openstack-dashboard/local_settings* 配置 GLANCE 修改 GLANCE 的配置文件/etc/glance/glance-api.conf 以使用 Ceph 块设备. 取消下面注释的配置, 并根据实际情况更改相应的值. stores = rbd default_store = rbd rbd_store_chunk_size = 8 rbd_store_pool = images rbd_store_user = glance rbd_store_ceph_conf = /etc/ceph/ceph.conf 启用 copy-on-write(COW) 克隆功能 show_image_direct_url = True 如需要, 关闭 cache management 功能, flavor 应该只设置为 keystone, 而不是 keystone+cachemanagement flavor = keystone 如需要, 可以对镜像设置下面的属性 hw_scsi_model = virtio-scsi hw_disk_bus = scsi hw_qemu_guest_agent = yes os_require_quiesce=yes 配置 NOVA 我们需要修改每一个 nova-compute 节点上的 Ceph 配置文件, 让 Nova 直接从 Ceph 中起动所有虚拟机. 打开 Ceph 配置文件, 在[client] 区加入以下内容. rbd cache = true rbd cache writethrough until flush = true rbd concurrent management ops = 20 admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok log file = /var/log/ceph/qemu-guest-$pid.log 为 admin socket 和日志文件建立相应的目录, 并赋予相应的权限. 文件夹需要在 SELinux 或 AppArmor 中得到允许. $ mkdir -p /var/run/ceph/guests/ /var/log/ceph/ $ chown qemu:libvirt /var/run/ceph/guests /var/log/ceph 为 KVM 访问 Ceph 生成一个密码 $ uuidgen |tee /etc/ceph/nova.uuid.txt $ cat >> /etc/ceph/nova.xml < EOF { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph openstack rados"},{"title":"在 Python 中使用 SH 模块执行 SHELL 命令","url":"/2018/Using-SH-in-Python/","text":"Python 中有一个非常有意思的包 sh, 它是一个将系统程序动态映射到 Python 函数的一个 wrapper. 得益于 Python 的强大和灵活, 它可以让你在 Python 中方便地运行 shell 命令. 开始使用 sh sh是一个非常成熟的子进程接口, 可以让你在 Python 中就像调用函数一样调用任何程序, 比 subproces.Popen 调用更优雅, 也能更好地让你捕获和分析输出结果. 安装 sh sh不是内置的模块, 需要通过 pip或 easy_install 来安装 $ sudo pip install sh ### or $ sudo easy_install sh 简单使用 使用 sh 非常简单, 下面举几个例子 >>> import sh >>> sh.ls(\"-l\",\"-a\") ... >>> from sh import ip,git >>> ip.address() ... >>> git(\"status\") ... 进阶 下面分几部分简单介绍 sh 的几个用法. A. 传递参数 传递给命令的参数必须是单个的字符串, 可以传递多个参数, 或者用 split() 函数将其分成字符串数组, 传递过去 >>> from sh import tar #### multiple args >>> tar(\"cvf\", \"/tmp/test.tar\", \"/my/test/directory/\") #### string splited into array of string >>> tar(\"xf /tmp/test.tar\".split()) sh可以通过关键字参数 **kwargs 的形式传递参数, 但这种形式 不保证 参数顺序, 一般使用上面形式就行. 这种形式支持 -a 和 --arg 长短两种格式参数. >>> sh.curl(\"http://duckduckgo.com/\", \"-o\", \"page.html\", \"--silent\") ### **kwargs >>> sh.curl(\"http://duckduckgo.com/\", o=\"page.html\", silent=True) B. 返回值处理 C. 输出重定向 sh 可以将一个进程或所有进程的 STDOUT 和 STDERR 重定向到不同的目标, 通过使用 _out 和 _err 关键字来指定. 1. 重定向到文件 如果 _out 或 _err 为一个字符串, 通常被认为是一个文件名, 文件以 二进制写 (wb) 的方式被打开. >>> import sh >>> sh.ifconfig(_out = \"/tmp/ifconfig.txt\") 2. 文件对象 我们也可以使用支持 .write(data) 的任何对象作为重定向目标, 例如 io.StringIO: >>> import sh >>> from io import StringIO as io >>> buf = io() >>> sh.ifconfig(_out = buf) >>> print(buf.getvalue()) 3. 回调函数 一个回调函数也可以用作重定向目标, 其至少必须能接受进程的输出数据块. fn(data) D. 异步执行 sh 提供了几种用于异步执行命令的方式. 1. 增量迭代. 我们可以通过 _iter来创建异步命令进程, 最常见的例子就是 tail -f, 可以用在 loop 环境中: >>> from sh import tail >>> for l in tail(\"-f /var/log/som_log_file.log\".split(), _iter=True): print(l) 2. 后台进程 我们可以将一些耗时长但又不需要立即获得结果的命令通过 _bg=True 放在后台运行, 就像 bash 中 some_command &amp;一样. >>> from sh import sleep >>> sleep(3) ### blocks >>> print(\"...3 seconds later\") >>> p = sleep(3, _bg = True) ### doesn't block >>> print(\"print immediately\") >>> p.wait() >>> print(\"...3 seconds later again) 我们需要在适当的时候执行 running_command.wait() 来等待后台运行的程序正常结束. 3. 输出回调 与 _bg=True 结合, sh 可以调用回调函数重定向 out 和 / 或 _err, 回调函数会在命令输出一行或块时被调用. 以 tail -f 为例, >>> from sh import tail >>> def process_output(line): print(line) >>> p = tail(\"-f\", \"/var/log/some_log_file.log\", _out = process_output, _bg=True) >>> p.wait() 4. 交互式回调 5. 完成回调 完成回调(done callback) 是在进程正常结束(成功 / 出错) 或者接收到信号时, 被执行, 通过 _done 关键字传递给命令. >>> import sh >>> from threading import Semaphore >>> pool = Semaphore(10) >>> def done_func(cmd, success, exit_code): pool.release() >>> def do_thing(arg): pool.acquire() sh.your_parallel_command(arg, _bg=True, _done=done_func) >>> procs = [] >>> for arg in range(100): procs.append(do_thing(arg)) # essentially a join >>> [p.wait() for p in procs] E. Baking sh 支持将参数跟命令 烘培 (baking) 在一起, 类似于 bash 中的 别名(alias). >>> from sh import ls,ssh >>> ls = ls.bake(\"-la\") >>> print(ls) /us r/bin/ls -la >>> ls(\"/\") #### identical to sh.ls(\"-la\", \"/\") >>> iam1=ssh(\"myserver\",\"-p 3333\", \"whoami\") >>> myserver=ssh.bake(\"myserver\", p=3333) >>> print(myserver) /us r/bin/ssh myserver -p 3333 >>> iam2=myserver.whoami() >>> asssert(iam1 == iam2) True #### excute commands via myserver >>>myserver.ls(\"/\") F. 管道 基本操作 Bash 风格的管道可以通过函数嵌套来实现. 简单来说, 就是把一个命令当作另一个命令的参数来使用, sh 会把内层的输出传递给外层的命令: >>> import sh >>> print(sh.sort(sh.du(\".\", \"-rn\"))) >>> print(sh.wc(sh.ls(\"/etc\", \"-l\"), \"-l\")) 为了减少出错, 可以结合上面的 baking 来食用. 进阶操作 一般地, 通过管道连接的命令会依次执行, 在大多数情况下是没有问题的. 但是对于持续输出的命令或需要并行化的地方, 这就不适用了. 比如下面的例子: >>> for l in sh.tr(sh.tail(\"-f\", 'test.log'), \"[:upper:]\", \"[:lower:]\", _iter = True): print(l) 由于 tail -f不会结束, tr命令也就无法执行. 这里我们需要在tail 接收到输入时便将其传递给tr, 这可以通过 _piped=True 来实现. >>> for l in sh.tr(sh.tail(\"-f\", 'test.log', _piped=True), \"[:upper:]\", \"[:lower:]\", _iter = True): print(l) 这样就告诉 tail -f 它正处于管道中, 应该将其输出一行一行地发动给tr. 缺省情况, _piped=True 会发送 STDOUT, 如果想发送错误, 可以使用 _piped=\"err\". G. 子命令 许多命令都会有子命令如 git, svn, ip, sudo 等. sh 中可以将子命令当作参数, 也可以想调用函数一样使用. >>> from sh import git, sudo >>> print(git.branch(\"-v\")) >>> print(git(\"branch\", \"-v\")) >>> print(sudo.ls(\"/root\")) >>> print(sudo(\"/bin/ls\", \"/root\")) sh 中的子命令主要是语法糖, 让调用命令更优雅一些. sudo sudo 的情况比较特殊, sh中有三种调用 sudo 的方法. 1. sh.sudo with /etc/sudoers NOPASSWD 通过设定 用户免输入密码, 可以直接使用sh.sudo. $ sudo visudo 在最后添加或修改权限 your_name ALL = (root) NOPASSWD: /path/to/your/program 这是说你可以在 所有 (ALL) 的主机上可以且只能以 root 免密运行 /path/to/your/program, 如果需要运行的程序很多, 可以设置所有程序都免密执行. your_name ALL = (root) NOPASSWD: ALL 2. sh.crontrib.sudo 因为 sudo 使用频率非常高, sh 特别添加了一个 普通版本 的 sudo 使得 sudo更加好用. 它只是对 sh.sudo 做了简单的包装, 但是 bake 了一些特别的关键字参数使得其更加方便. >>> import sh >>> with sh.crontrib.sudo: print(sh.ls(\"/root\")) #### or via subcommand >>>> print(sh.crontrib.sudo.ls(\"/root\")) 然后它会要求你输入密码: [sudo] password for your_name: ******* your_root_file 3. sh.sudo with password passed by 我们可以通过将密码传递给 sh.sudo的方式来使用, 结合 baking 更加方便, 不过不推荐这种方式. >>> import sh >>> my_password='password\\n' >>> my_sudo = sh.sudo.bake(\"-S\", _in=my_password) >>> print(my_sudo.ls(\"/root\")) 4. _fg=True 这种方式无法捕获程序输出, 只会输出到终端. >>> import sh >>> sh.sudo.ls(\"/root\", _fg=True) H. 默认参数 许多时候, 你想覆盖掉所有命令的默认参数. 比如你想将所有的输出都聚合到 一个 io.StringIO 缓冲区 buf 里, 你可以在执行每一个命令的时候显示地传递 _out=buf, 但是太不优雅了. 我们可以对 sh 设定默认的参数并赋值给一个 execution context, 甚至可以从中 导入 sh 中的命令 >>> import sh >>> from io import StringIO as io >>> buf = io() >>> sh2 = sh(_out=buf) >>> sh2.ls(\"/\") >>> from sh2 import ls, whoami, ps >>> ls(\"/\") >>> whoami() >>> ps(\"auxwf\") I. 环境变量 _env 关键字可以以字典类型传递环境变量: >>> import sh >>> sh.google_chrome(_env={\"SOCKS_SERVER\":\"localhost:12345\"}) 需要注意的是, _env 会完全替换进程的环境变量. 只有 _env 里的键值对才会被使用. 如果要在现有的变量中加入新的环境变量, 可以通过 os.environ.copy() 命令来实现. >>> import sh >>> from os import environ.copy as en_copy >>> env = en_copy() >>> env ['SOCKS_SERVER'] = 'localhost:12345' sh.google_chrome(_env = env) J. 输入 STDIN 可以通过 _in 关键字传递给命令. >>> import sh, sys >>> print(sh.cat(_in=\"test\")) >>> print(sh.cat(_in=sys.stdin)) >>> print(sh.tr(\"[:lower:]\", \"[:upper:]\", _in=\"sh is awesome\")) >>> print(sh.tr(\"[:lower:]\", \"[:upper:]\", _in=[\"sh\", \"is\", \"awesom\"])) 可以用 文件对象, queue.Queue, 或者其他任何可以迭代的对象作为参数, 如上面所示. K. With 环境 命令可以运行在 Python 的 with 环境中, 常用的命令可能是 sudo 或者 fakeroot: >>> with sh.contrib.sudo(_with=True): print(ls(\"/root\")) _with=True 关键字告诉命令它正处于 with 环境中, 以便可以正确地运行. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"python shell"},{"title":"在 Python 中使用 JSON 模块处理 JSON 文件","url":"/2018/Using-JSON-in-Python/","text":"Json 是一种非常通用的数据格式, 在 Python 中使用也非常方便. Python 中有内置的 json 模块, 用来编码和解码 json 数据, 其使用也很简单, 在日常运维工作中非常实用. Python 中的 JSON 模块 要使用 json 非常简单, 直接 导入该模块就行. import json ## or simply import json as js 序列化和反序列化 将 python 数据 (list or dict) 编码成 json 数据的过程通常叫 序列化 (serialization), 相反地则叫做 反序列化(deserialization). Python 中还有类似的模块 - pickle 和 marshal, 三者的函数是类似的, 但是后两个是 python 专用的, json 则是通用的. JSON 数据反序列化 json 模块提供了从 json 文件和 json 数据转换到 Python 数据格式的接口: json.load() 和 json.loads(). json 到 python 的数据格式转换如下表所示. JSON Python object dict array array string str number(int) int number(real) float true True false False null None json.load() 接受一个 file-like object (open(\"file\"))作为参数, 而不是文件名, json.loads() 则是接受一个 string 作为输入. 作为开始, 我们先从一个 json 文件中通过 json.load()来读取数据. 下面是一个简单的 json 文件. { \"name\": \"Amito\", \"email\": \"me@amito.me\" } 我们在 Python 中导入 json 文件 >>>import json >>>data=json.load(open(\"simple.json\",\"r\") >>>print(data) {'name': 'Amito', 'email': 'me@amito.me'} 如果你有从别处获取的 json 数据或者 json 格式的 string, 可以通过 json.loads() 来转换到 Python 格式, 多出来的 s 可能就是代表了 string. >>> json_string=\"\"\" { \"name\": \"Amito\", \"email\": \"me@amito.me\", \"note\": \"from json string\" } \"\"\" >>>data = json.loads(json_string) >>>print(data) {'name': 'Amito', 'email': 'me@amito.me', 'note': 'from json string'} Python 数据序列化 下面是 Python 对象序列化时和 json 数据格式之间的一个直观的对应. Python JSON dict object list, tuple array str string int, long,float number True true False false None null json.dump() 会将 python 数据序列化到一个文件对象中, json.dumps() 则会序列化到一个 json string 中. 仍旧以上面的 json 文件作为例子 >>>data=json.load(open(\"simple.json\",\"r\")) >>>type(data) dict #### dump to a file object >>>with open(\"simple.de.json\",\"w\") as out: json.dump(data, out) ### dumps to a string >>>json_string = json.dumps(data) >>>type(str) str >>>print(json_string) {'name': 'Amito', 'email': 'me@amito.me', 'note': 'from json string'} #### beautify json_string and print json pretty >>>json_string=json.dumps(data,indent=4) >>>print(json_string) { \"name\": \"Amito\", \"email\": \"me@amito.me\", \"note\": \"from json string\" } 自定义格式数据序列化和反序列化 todo… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"python json"},{"title":"Redis 基本操作入门","url":"/2018/Introduction-to-Redis/","text":"Redis 是一个开源的、轻量级的、高性能的、基于 键 - 值 对的缓存与存储系统，非常适合用来缓存和存储服务和消息队列、任务队列等。 本文主要介绍一下 Redis 的基本使用方法, 是 《Redis 入门指南 第 2 版》的读书笔记。 .to.be.continued. Redis 简介 开始使用 Redis Redis 类型与命令 Redis 命令不区分 大小写 。主要类型有 字符串 、 散列 、 列表 、 集合 、 有序集合 等。 keys 会遍历所有键, 键多时会影响性能。 基本操作 利用通配符获取键名列表 。bash 中的通配符： ?、*、[]、 转义 keys pattern 判断 key 是否存在。 存在返回 1， 不存在返回 0 exists bar (integer) 1 删除 keys。一次可以删除多个键, 不支持通配符。 del key [key ...] 获取键类型。可能的返回值有 string、hash、list、set、zset。 type key 字符串类型 string string 是 Redis 中最基本的数据类型，可以存储任何形式的字符串， 目前允许的最大容量是 512 MB。 0. 键名规则 。 对象类型: 对象 ID: 对象属性， 多个单词用 . 来分隔。键名要有意义. 赋值与取值。 EX|PX 设置过期时间，分别为秒和微秒数， NX 表示只有在 key 不存在时创建 key，XX 表示只有在 key 存在时赋值 set key value [[EX seconds]|[PX milliseconds]] [NX|XX] get key 键值递增递减。存储的 string 为整数形式时，incr 命令可以让键值加 1，返回增加后的值。 不存在时，会默认为 0，返回 1。不是整数时，会报错。incrby 则会增加 incremnt。 decr 和 decreby 则正好相反。 ## incr key incrby key increment ## decr key decrby key decrement 键值增加指定浮点数。 与 incrby 类似， increment 可正可负。 increbyfloat key increment 键值尾部追加值。返回追加后的字符串总长度。不存在则将 key 的值 设为 value。 append key value 获取字符串长度。不存在则返回 0。 strlen key 同时设置 / 获取多个键值。 ### 获取 MGET key [key ...] ### 赋值 MSET key value [key value ..] 位操作。 ### get bit getbit key offset ### set bit setbit key offset value ### bit count bitcount key [start] [end] ### bit operation bitop operation destkey key [key ...] ### bitops bitpos key 0|1 [start] [end] 位的值为 0 或 1 gitbit 若超出键值长度或不存在则返回 0 setbit 若超出长度则将中间的设为 0，若不存在，则会建立该 key 并将前面的 bit 设为 0 bitcount 可以获得从 start 到 end 之间 值为 1 的位数 bitop 可以对多个字符串类的键进行位运算，保存在 destkey 中。 有 and、or、xor、not 四种 bitposs 可以获取指定范围内键值第一个为 0 或 1 的位置, 从 2.8.7 版本开始引入 位操作可以非常紧凑地存储 布尔值. 100W 个只占 100KB 多空间. 散列类型 hash 散列类型是一种 字典结构 , 存储了 字段 (field) 和 字段值 的映射, 字段值只能是 字符串 , 不能 为其他类型, 不能嵌套 . 其他类型也都 不能 嵌套, 如 集合 、 列表 等. 散列适合存储对象: 对象类别:ID 可作键名, 属性 作为字段, 属性值 作为字段值. Redis 不要求 所有散列具有相同的结构, 这种结构是 人为约定 的, 如 user:1 可以有 car, house, farm, user:2 可以只有 car, house, 外加 tree. 赋值与取值. hset key field value hget key field hmset key field value [field2 value ...] hmget key field [field2 ...] hgetall key hset 和 hmset 用来给一个或多个字段赋值, 不区分插入和更新. 没有的字段会插入, 返回 1, 已有的会更改, 返回 0. 键不存在时会自动创建. hget 和 hmget 用来获取一个或多个字段值. hgetall 会返回所有的字段和值. 判断字段是否存在. 字段存在, 返回 1, 不存在(甚至键也不存在), 返回 0. hexists key field 字段不存在时赋值 . 只有不存在时 , 存在时不做任何操作. 是 原子操作. hsetnx key field value 增加数字 . 健不存在时会自动创建, 字段不存在会插入, 返回 增值后的字段值. hincrby key field value 删除字段. 可以删除一个或多个字段, 返回删除的字段个数. 键或不存在时返回 0, hdel key field [field2 ...] 只获取字段名或字段值 . hkeys 获取所有 字段 , hvals 获取所有 字段值. hkeys key hvals key 获得字段数. 返回字段个数. hlen key 列表类型 list 列表可以存储一个 有序的 字符串列表, 类似于 C 中的双向链表, 内部也是 C 中的双向链表实现的. 首尾插入元素复杂度为 O(1), 获取元素越接近两端越快, 通过索引访问元素相对比较慢. 一个列表类型键最多只能容纳 $2^32 - 1$ 个元素, 元素值可相同. 向两端增加元素. lpush 从左边依次插入, rpush 从右边依次插入. 返回增加元素后列表的长度, 依次增加 value, value2, … lpush key value [value2 ...] rpush key value [value2 ...] 从两端弹出元素. lpop 从左弹出, rpop 从右弹出, 返回弹出的元素. lpop key rpop key lpush 与 lpop 或 rpush 与 rpop 可当作栈, lpush 与 rpop 或 rpush 与 lpop 可以当作队列. 3. 获取列表中元素个数. 键不存在时, 返回 0. 时间复杂度为 O(1). llen key 获取列表片断. 返回从 start 到 stop 之间包含两端的所有元素. 列表起始索引为 0. 支持负索引. lrange key start stop -1 表示最右一个, -2表示最右第二个. start 比 stop 靠后, 则会返回空列表 stop 大于实际索引范围, 则会返回 start 到最右边的列表. 删除列表中指定的值. 删除列表中前 count 个值为 value 的元素, 返回实际删除的元素个数. lrem key count value count &gt; 0, 从左边开始删除前 count 个值为 value 的元素 count &lt; 0, 从右边开始删除前 count 个值为 value 的元素 count = 0, 删除所有值为 value 的元素 获得 / 设置指定位置的元素值. lindex 获得 index 处的元素值, index 可以为负, 返回元素值. lset 设置 index 处的元素值为 value, 成功返回 OK. lindex key index lset key index value 只保留列表指定片断. 跟 lrange 类似. 只保留 start 到 end 的元素. 例如保持元素个数在 100 以内, 可以用 ltrim. ltrim key start end 向列表中插入元素 . 在列表中值为 pivot 的 前面 (before) 或 之后(after) 插入 value. linsert key before|after pivot value 元素在列表间转移. 从 source 右边弹出一个元素, 将其插入 destination 的左边, 并返回该元素. rpoplpush source destination rpoplpush 可以很方便地在多个队列之间传递数据. 源跟目标相同时, 可以源源不断地将队尾的元素移到队首, 可以实现一个网站监控系统. 集合类型 set 集合类型就是数学上的集合, 没有顺序 , 具有唯一性 , 列表没有唯一性 . 一个集合可以最多有 2^32 - 1 个字符串. 常用操作有 插入元素 , 删除元素 , 判断元素是否存在 等等. 内部实现为 值为空的 散列列表(hash table), 操作的时间复杂度为 O(1). 增加 / 删除元素. sadd key member [member2 ...] sdel key member [member2 ...] sadd 插入一个或多个元素, 键不存在自动创建, 忽略已有元素, 返回插入的元素个数. sdel 删除一个或多下元素, 忽略没有的元素, 返回成功删除的元素个数. 获得集合中的所有元素或个数. smembers 返回所有元素, scard 返回元素个数. smembers key scard key 判断元素是否存在. 存在返回 1, 不存在返回 0. 时间复杂度为 O(1). sismember key member 集合间运算. sinter - 交集, sunion - 并集, sdiff - 差集, 返回相应的所有元素. sdiffstore, sinterstore, sunionstore 则将结果集合保存在 destination 中. sdiff key1 key2 [key3 ...] sdiffstore destination key1 key2 [key3 ...] sinter key1 key2 [key3 ...] sinterstore destination key1 key2 [key3 ...] sunion key1 key2 [key3 ...] sunionstore destination key1 key2 [key3 ...] sdiff 先计算 key1 与 key2 的差集, 再将差集与 key3 计算差集, 依次进行. sinter 计算所有 keys 的交集. sunion 计算所有 keys 的并集. 随机获得集合中的元素 . count &gt; 0, 返回 不同的 相应个数的随机元素; count &lt; 0, 则会返回 可能相同的 |count| 个随机元素. srandmember key [count] 从集合弹出一个元素 . 弹出是 随机的. spop key 有序集合 zset Redis 高阶操作 Redis 与 Python Python 与 Redis 交互的客户端官方推荐 redis-py。 安装很简单； ## pip sudo pip install -U redis-py ## easy_install sudo easy_install redis-py Redis 持久化 Redis 集群与运行 Redis 安全与管理 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux sql redis"},{"title":"在 uTorrent 中设置 IP 过滤和去除广告","url":"/2018/Setup-IP-Filters-and-Remove-Ads-in-uTorrent/","text":"uTorrent 是广泛流行的非常好用的 BT 下载工具。我们可以设置 uTorrent 使其屏蔽某些 IP。 默认的 uTorrent 会有广告和升级提示。目前 (v.3.5.4) 还可以通过设置去掉。 IP 地址过滤 比如我们要过滤掉 所有的 IPV4 地址，我们可以通过如下步骤进行。 打开 uTorrent 安装目录。一般在 %APPDATA%\\uTorrent\\ 目录。 打开或新建一个 ipfilter.dat 文本文件。内容为 0.0.0.0-255.255.255.255 如果要过滤某一段， 可添加上 a.b.c.d-e.f.g.h 到该文件中。 3. 打开 uTorrent 选项 (Options) 的 首选项 (Preferences)，在高级 (Advanced) 中，设置 ipfilter.enable 为 true。 在种子的详细信息下面，日志中可以看到有 Loaded ipfilter.dat (1 entries) 类似的信息，表明 IP 过滤已经启用了。 去除广告和升级提示 目前的 uTorrent 可以在设置中除去广告, 这篇 介绍了如何手动去除 uTorrent 中的广告。 屏蔽左下角广告。 在 Options-&gt;Preferences-&gt;Advanced 里， 找到 offers.left_rail_offer_enabled 设置为 false. 屏蔽顶部广告。 找到 offsers.sponsored_torrent_offer_enabled， 设置为 false. 屏蔽左下角升级提示。 找到 gui.show_plus_upsell， 设置为 false. 这样基本就没有烦人的广告和升级提示啦。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"windows utorrent"},{"title":"在 CentOS 7 下安装和配置 Ceph 存储系统","url":"/2018/Install-and-Configure-Ceph-on-CentOS-7/","text":"Ceph 是一个统一的分布式存储系统，设计初衷是提供较好的性能、可靠性和可扩展性。 Ceph 项目最早起源于 Sage 就读博士期间的工作（2004 年），并随后贡献给开源社区。在经过了数年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat 及 OpenStack 都可与 Ceph 整合以支持虚拟机镜像的后端存储。 --from &lt;&lt;Ceph 介绍及原理架构分享&gt;&gt; 准备工作 目前主要有两种 ceph 部署方案: ceph-deploy - https://github.com/ceph/ceph-deploy ceph-ansible - https://github.com/ceph/ceph-ansible 我们主要使用 ceph-deploy 来部署 ceph 集群. 软件版本 ceph-deploy 目前 (03/09/2018) 的版本是 2.0.1: [ceph@localhost ~]$ ceph-deploy --version 2.0.1 Ceph 配置安装源: [ceph] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-mimic/el7/x86_64 enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-mimic/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc [ceph-source] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-mimic/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc Ceph 目前的版本是 13.2.1 (mimic): [ceph@localhost ~]$ ceph --version ceph version 13.2.1 (5533ecdc0fda920179d7ad84e0aa65a127b20d77) mimic (stable) 系统版本采用的是最新的 CentOS 7.5: [ceph@localhost ~]$ lsb release -a LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch [ceph@vm084021 ~]$ lsb_release -a LSB Version: :core-4.1-amd64:core-4.1-noarch Distributor ID: CentOS Description: CentOS Linux release 7.5.1804 (Core) Release: 7.5.1804 Codename: Core 集群架构 作为一个测试集群, 为方便, 我们单独用一个 node 作为部署节点(ceph0), 用来部署集群, 三个节点(ceph1, ceph2, ceph3) 作为监督节点(monitors), 同时这三个 mon 节点也做为osd 节点. 系统准备 建立统一用户 [localhost ~]$ useradd -m -d /home/ceph -r -U -u 1000 ceph [localhost ~]$ usermod -aG wheel ceph [localhost ~]$ sed -i \"s|^# %wheel|%wheel|g\" /etc/sudoers 在下面的安装中都使用 ceph 用户. 安装 ceph-deploy 在部署节点安装 ceph-deploy: [ceph@ceph0 ~]$ sudo pip2 install -U ceph-deploy remoto 集群节点 hosts 设置 #### adding following lines to the end of /etc/hosts #### change IPs according to your servers 10.0.1.10 ceph0 admin 10.0.1.11 ceph1 10.0.1.12 ceph2 10.0.1.13 ceph3 更改主机名(可选) 将主机名更改为 /etc/hosts 中对应的名字: echo ceph0 > /etc/hostname ssh 免密登录 [ceph@ceph0 ~]$ ssh-keygen -t rsa -b 2048 [ceph@ceph0 ~]$ cat .ssh/id_rsa.pub >> .ssh/authorized_keys [ceph@ceph0 ~]$ for i in ceph1 ceph2 ceph3;do ssh-copy-id -i ~/.ssh/id_rsa.pub $i; done [ceph@ceph0 ~]$ for i in ceph1 ceph2 ceph3;do scp ~/.ssh/id_rsa $i:~/.ssh/; done 安装配置 ntp service #### on ceph0 [ceph@ceph0 ~]$ for host in ceph{0..3};do > ssh ceph${host} \"sudo yum install -y ntp ntpdate\" > ssh ceph${host} \"sudo systemctl enable --now ntpd\" > done 关闭 / 设置防火墙 #### disable firewalld [ceph@ceph0 ~]$ for host in ceph{0..3}; do > ssh ceph${host} \"sudo systemctl disable firewalld\" > ssh ceph${host} \"sudo systemctl stop firewalld\" > done 如果不关闭防火墙，需要打开 Ceph 所有节点上各服务相应的端口。 #### firewalld [ceph@ceph0 ~]$ for host in ceph{1..3}; do firewall-cmd --permanent --add-port=3300/tcp --zone=public firewall-cmd --permanent --add-port=6789/tcp --zone=public firewall-cmd --permanent --add-port=6800-7300/tcp --zone=public # or simply firewall-cmd --permanent --add-service=ceph --zone=public firewall-cmd --permanent --add-service=ceph-mon --zone=public #### iptables 安装 ceph [ceph@ceph0 ~]$ ceph-deploy install ceph0 ceph1 ceph2 ceph3 ceph-deploy 会使用 ceph@ceph0 来连接 ceph0, 所以需要做 ssh 免密登录. 部署 Ceph ceph 集群有一个默认的集群名 - ceph, 如果你想运行多个 ceph 集群, 可以在部署时通过--cluster name 指定一个名字. 如果在同一硬件上运行多个实例, 还需要更改默认的端口设置, 以避免冲突. 部署 monitors 创建 三个 monitors: #### hsotname ceph1/2/3 must match the actual `hostname -s` in the remote host [ceph@ceph0 ~]$ ceph-deploy new ceph1 ceph2 ceph3 [ceph@ceph0 ~]$ ceph-deploy mon create ceph1 ceph2 ceph3 #### a ceph.conf and ceph.mon.keyring will be created under current directory 汇集分发 keys #### monitors keys #### wait for a while after \"ceph-deploy mon create\" [ceph@ceph0 ~]$ ceph-deploy gatherkeys ceph1 ceph2 ceph3 #### distribute keys to admin nodes in cluster [ceph@ceph0 ~]$ ceph-deploy admin ceph0 ceph1 ceph2 ceph3 #### change permissions for i in ceph{1..3}; do ssh ceph$i \"sudo chown -R ceph:ceph /etc/ceph\"; ssh ceph$i \"sudo chown -R ceph:ceph /var/lib/ceph\"; ssh ceph$i \"sudo chown -R ceph:ceph /var/log/ceph\"; done 检查集群状态 [ceph@ceph0 ~]$ ceph -s cluster: id: 3cdc0d3a-6f09-4f06-977e-017e273bbb07 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 usage: 0 B used, 0 B / 0 B avail pgs: 部署 managers [ceph@ceph0 ~]$ ceph-deploy mgr create ceph1 ceph2 ceph3 [ceph@ceph0 ~]$ ceph -s cluster: id: 3cdc0d3a-6f09-4f06-977e-017e273bbb07 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2, ceph3 mgr: ceph1(active), standbys: ceph2, ceph3 osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 usage: 0 B used, 0 B / 0 B avail pgs: 部署 MDS (Optional) 使用 ceph-deploy 创建 mds 非常简单: ### Usage: [ceph@ceph0 ~]$ ceph-deploy mds create {host-name}[:{daemon-name}] [{host-name}[:{daemon-name}] ...] ### e.g. [ceph@ceph0 ~]$ ceph-deploy mds create ceph1 ceph2 ceph3 daemon-name 是可选的, 如果要在一个节点上运行多个 mds 实例, 可能需要为每个 mds 实例设定 daemon-name. 部署 Ceph OSD #### ceph-deploy osd create usage [ceph@ceph0 ~]$ ceph-deploy osd create -h usage: ceph-deploy osd create [-h] [--data DATA] [--journal JOURNAL] [--zap-disk] [--fs-type FS_TYPE] [--dmcrypt] [--dmcrypt-key-dir KEYDIR] [--filestore] [--bluestore] [--block-db BLOCK_DB] [--block-wal BLOCK_WAL] [--debug] [HOST] positional arguments: HOST Remote host to connect optional arguments: -h, --help show this help message and exit --data DATA The OSD data logical volume (vg/lv) or absolute path to device --journal JOURNAL Logical Volume (vg/lv) or path to GPT partition --zap-disk DEPRECATED - cannot zap when creating an OSD --fs-type FS_TYPE filesystem to use to format DEVICE (xfs, btrfs) --dmcrypt use dm-crypt on DEVICE --dmcrypt-key-dir KEYDIR directory where dm-crypt keys are stored --filestore filestore objectstore --bluestore bluestore objectstore --block-db BLOCK_DB bluestore block.db path --block-wal BLOCK_WAL bluestore block.wal path --debug Enable debug mode on remote ceph-volume calls 在 Luminous 12.2.2 之后, Ceph 中创建一个 OSD 就开始使用 ceph-volume 了. 创建一个 bluestore 的 OSD, 有如下几种选择: A single block device A block device and a block.db device A block device and a block.val device A block device ,a block.val and a block.db device block device 也有几种选择: 整块磁盘 磁盘分区 逻辑巻(A logical Volume of LVM) 在使用整块磁盘和磁盘分区时, ceph-volume 默认会自动创建一个 logical volume 使用. 简单块设备 整块磁盘建立 OSD 使用一整块磁盘建立一个 OSD, 可用如下命令: ceph-deploy osd create [host] --data [/path/to/device] [--zap-disk] --zap-disk 是为了销毁已有的文件系统. 比如使用 ceph1 上的 /dev/sdb 来创建一个 ext4 系统的 OSD [ceph@ceph0 ~]$ ceph-deploy osd create ceph1 --data /dev/sdb --fs-type ext4 --zap-disk 查看 OSD 信息: [ceph@ceph0 ~]$ ssh ceph1 \"df -h\" ... [ceph@ceph0 ~]$ ssh ceph1 \"ls /var/lib/ceph/osd/ceph-1\" ... [ceph@ceph0 ~]$ ssh ceph1 \"lsblk /dev/sdb\" ... 磁盘分区 使用一个分区来创建一个 OSD 与使用整个磁盘类似: ceph-deploy osd create [host] --data [/path/to/device_part] [--zap-disk] 注意, 磁盘格式须为GPT 分区格式(???). 例如 [ceph@ceph0 ~]$ ceph-deploy osd create ceph1 --data /dev/sdc1 逻辑巻 命令格式为: ceph-deploy osd create [host] --data [vg/lv] 我们可以使用 loop 文件来创建 逻辑巻, 再创建 OSD. [ceph@ceph0 ~]$ ssh ceph@ceph1 [ceph$ceph1 ~]$ dd if=/dev/zero of=ceph.0.img bs=1M count 2048 [ceph@ceph1 ~]$ sudo losetup /dev/loop0 /home/ceph/ceph.0.img [ceph@ceph1 ~]$ sudo pvcreate /dev/loop [ceph@ceph1 ~]$ sudo vgcreate sdavg /dev/loop0 [ceph@ceph1 ~]$ sudo lvcreate sdalv -l 100%FREE sdavg [ceph@ceph1 ~]$ exit [ceph@ceph0 ~]$ ceph-deploy osd create ceph1 -data sdavg/sdalv 若是指定--data /dev/sdavg/sdalv, ceph-volume 会认为是块设备而报错. 这一种只是自己手动创建了 PV, VG, LV, 其他与前面两种方式一样. 带有 block.db 或 / 和 block.wal 的块设备 当指定 block.db 或 / 和 block.wal 时, 对应的设备只能为 磁盘分区 或逻辑巻. --data 的情况与简单块设备情况类似, 我们选择整个磁盘为例. block.db 和 block.wal 大小 默认情况下, block.db 和 block.wal 都比较小, block.db=0, block.wal=100663296. [ceph@ceph0 ~]$ ceph-config --show-config|grep bluestore_block bluestore_block_create = true bluestore_block_db_create = false bluestore_block_db_path bluestore_block_db_size = bluestore_block_path bluestore_block_preallocate_file = false bluestore_block_size = 10737418240 bluestore_block_wal_create = false bluestore_block_wal_path bluestore_block_wal_size = 100663296 所以, 用来存储 block.db 和 block.wal 的分区或 逻辑巻容量不用太大. 这里我们选择 10G. 磁盘分区 基本命令格式为 ceph-deploy osd create [host] --data [/path/to/device] --block-db [/path/to/device-partition] --block-wal [/path/to/device-partition] 例如 [ceph@ceph0 ~]$ ceph-deploy osd create ceph1 --data /dev/sdd --block-db /dev/sde1 --block-wal /dev/sde2 与前面相比, 只是多指定了 block-db 和 block-wal的位置. 逻辑巻 ceph-deploy osd create [host] --data [/path/to/device] --block-db [vg/lv] --block-wal [vg/lv] 例如 [ceph@ceph0 ~]$ ssh ceph1 [ceph@ceph1 ~]$ pvcreate /dev/sdh [ceph@ceph1 ~]$ vgcreate sdhvg /dev/sdh [ceph@ceph1 ~]$ lvcreate -n db-lv-0 -L 4G sdhvg [ceph@ceph1 ~]$ lvcreate -n wal-lv-0 -L 8G sdhvg [ceph@ceph1 ~]$ exit [ceph@ceph0 ~]$ ceph-deploy osd create ceph1 --data /dev/sdg --block-db sdhvg/db-lv-0 --block-wal sdhvg/wal-lv-0 与前面一样, 也是多指定了--block-db 和 --block-wal 的位置. 需要注意的是, --block-db 和 --block-wal 的格式为 vg/lv, 而不能是 /dev/vg/lv, 否则会报错. 移除 OSD 如果某些 OSD 在建立的时候有问题, 要移除这些 OSD, 可按照如下命令进行. for id in {osds_to_be_removed}; do ## mark osd out ceph osd out $i ## remove osd from crush map ceph osd crush remove osd.${i} ## delete osd authencation key ceph auth del osd.${i} ## remove osd finally ceph osd rm ${i} done 自动部署 基于 ceph-deploy, 我们可以开发一个自动部署 Ceph 的脚本。 设定好 Ceph 集群的配置，比如下面这样 { \"cluster\": \"ceph\", \"install\": [\"ceph0\", \"ceph1\", \"ceph2\", \"ceph3\"], \"mon\": [\"ceph1\", \"ceph2\", \"ceph3\"], \"mgr\": [\"ceph1\", \"ceph2\", \"ceph3\"], \"admin\": [\"ceph1\", \"ceph2\", \"ceph3\"], \"osd\":[ { \"host\": \"ceph1\", \"data\": [\"sdavg/sdalv\", \"sdbvg/sdblv\", \"sdcvg/sdclv\"] }, { \"host\": \"ceph2\", \"data\": [\"sdavg/sdalv\", \"sdbvg/sdblv\", \"sdcvg/sdclv\"] }, { \"host\": \"ceph3\", \"data\": [\"sdavg/sdalv\", \"sdbvg/sdblv\", \"sdcvg/sdclv\"] } ], \"pool\":[ { \"name\": \"rbd\", \"pg_num\": 256, \"pgp_num\": 256 }, { \"name\": \"images\", \"pg_num\": 256, \"pgp_num\": 256 } } 就可以用 Python 写一个简单脚本来自动部署 Ceph。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ceph linux"},{"title":"在 Windows Linux 子系统中安装 CentOS","url":"/2018/Install-CentOS-7-on-Windows-Subsystem-for-Linux/","text":"微软在 Windows 10 中引入了 Windows Subsystem Linux(WSL) 功能, 这使得我们可以在 Windows 中就能使用 Linux 的各种命令. 目前 Windows 商店中有 ubuntu, kali, debian等发行版. 但是 Fedora, CentOS 等发行版还没有上架. 那如何安装这些发行版呢? WSL-Distribution-Switcher 项目. 利用它我们可以很方便地在 Windows 中安装 CentOS 7 等其他的 Linux 发行版. 准备工作 首先我们要在 Windows 中启用 WSL 功能. 在控制面板的 添加功能 里能够找到并启用它. 项目是 Python 写的, 所以我们需要安装 Python. 下载或克隆 WSL-Distribution-Switcher 项目. 使用管理员权限打开 CMD, 并进到下载好的 WSL-Distribution-Switcher 目录中. 安装 CentOS 7 获取 CentOS 7 镜像 get-source.py 文件可以从 Docker Hub 上下载官方的镜像. 其支持的镜像可以在 README.md 找到, 其中就有 想要安装的 CentOS 7. 运行下面的命令, 就会下载最新的 CentOS 镜像. python get-source.py centos:latest 下载的文件名字大概 rootfs_centos_*.tar.xz的样子 获取依赖镜像 get-prebuilt.py 文件可以下载预先编译好的镜像的依赖层. python get-prebuilt.py centos:latest 安装新的 ROOTFS 需要注意的是, WSL-Distribution-Switcher 依赖 最开始的 ubuntu, 所以, 如果你没有安装 ubuntu, 它会在安装时先安装一个 ubuntu, 或者你可以自己安装一个 LxRun.exe /install install.py 文件会安装已经下载好的镜像包, 或者如果还没有下载, 它会下载并安装. python install.exe root_centos_xxx.tar.xz ## or python install.exe centos:latest 此时你运行 bash, 你就会进入安装好的 CentOS 7 环境. 安装的 CentOS 7 有一些问题, 下面会提到. 切换发行版 switch.py 文件可以让你在不同的发行版之间进行切换. 直接运行会列出已经安装的发行版. $ python switch.py usage: ./switch.y image[:tag] ... ### switch to default distribtuion $ python switch.py ubuntu:trusty 使用 ConEmu ConEmu 是 Windows 下非常好用的终端模拟器. 稍作配置, 就可以直接进入 CentOS 7 中. 安装好 ConEmu 后, 直接打开会进入 ubuntu. 通过在 Window 开始 搜索 Bash, 查看其属性, 我们会发现其指向 C:\\Window\\System32\\bash xxxxx 的字样, 记下后面的 字符串, 在 ConEmu 设置中, 将启动程序设为 bash xxx. 重新启动 ConEmu 后, 我们就进入了 CentOS 7 了. 相关问题 安装好 CentOS 7 后, 默认是没有安装 sudo 的, root 密码也不知道. 我们可以在 cmd 中运行如下命令, 使默认登录账号变为 root, 这样就有 root 权限了. 安装sudo, 并将 User 加到 sudo 组中去, 最后将默认用户改回User. lxrun /setdefaultuser root bash passwd yum install sudo -y usermod -aG wheel User sed -i \"s|# %wheel|%wheel|g\" /etc/sudoers exit lxrun /setdefaultuser User 新建文件夹权限问题. 新建的文件夹权限默认为 777, 可在 ~/.profile 中加入如下命令: $ echo \"umask 0022\" >> ~/.profile document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux windows wsl"},{"title":"在 CentOS 7 下配置 NFS 服务","url":"/2018/Setup-NFS-Service-on-CentOS-7/","text":"NSF (Network-base File System) can allow computers (clients) to access files on another computer(server) over the network even the clients have different operating systems from the server. And this article will guide you exstablishing your own NFS service on CentOS 7, which is generally used as server os. Prerequisites To establish your NFS service, you need at least two instances (one as server, and the remains as clients) with CentOS 7 fresh installed. CentOS 7 supports NFSv3 and NFSv4. NFS and RPC Services NFS service needs RPC service, which includes: rpcbind.service rpcbind.socket rpcidmapd.service rpc-statd.service Configure NSF Server Now we will configure our nfs server supposed that ip is 192.168.0.2. 1. Install Required NFS and RPC Packages sudo yum install -y nfs-utils rpcbind 2. Export Shared Directories The fortmat of nfs config file /etc/exports is like: /path/to/shared client1_ip (options) [client2_ip(options)] Suppose you want to expose the dirs to client1 with ip 192.168.0.3, the file is like: /path/to/shared 192.168.0.3(rw,sync,no_wdelay) One example: /opt/shared 192.168.0.3(rw,sync) If you wish all clients have rights to access to the server, you can make it by the following configure: /path/to/shared *(ro,sync) Caution: do not grant writing right to everyone. 3. Create Shared Directories mkdir -p /path/to/shared sudo chown -R nobody:nobody /path/to/shared sudo chmod -R 755 /path/to/shared 4. Enable and Start Services sudo systemctl enable --now nfs-server sudo systemctl enable --now rpcbind ## the following two may not work since CentOS 7.1 sudo systemctl enable --now nfs-lock sudo systemctl enable --now nfs-idmap ## check nfs status sudo systemctl status nfs sudo systemctl status rpcbind 5. Export the Share ## exportfs -r will re-exports entries in /etc/exports ## and sync /var/lib/nfs/etab with /etc/exports. sudo exportfs -r ## restart nfs service sudo systemctl restart nfs-server Configure NSF Clients 1. Install required packages ## install nfs-utils sudo yum install -y nfs-utils rpcbind ## enable rpcbind needed by nfs sudo systemctl enable --now rpcbind 2. Mount the Shared NFS Directory sudo mkdir -p /local/path sudo mount -t nfs -orw,nosuid remote_host:/path/to/shared /local/path The following is an example sudo mount -t nfs -o rw,nosuid 192.168.0.2/opt/shared /opt/shared This mounts /opt/shared on 192.168.0.2 to /opt/shared on local machine If you want to mount at boot time, you can add the following line to /etc/fstab, and backup /etc/fstab before changing it: remote_host:/path/to/shared /local/path nfs rw,nosuid 0 0 Configure Firewalld for NFS and RPC Services If you enable the firewalld service, you need to allow the nfs, mountd, rpc-bind services through the firewall. sudo firewall-cmd --permanent --add-service=nfs sudo firewall-cmd --permanent --add-service=mountd sudo firewall-cmd --permanent --add-service=rpc-bind sudo firewall-cmd --reload Now you can access files on 192.168.0.2 from clients. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"storage linux"},{"title":"Check XFS and EXT Filesystem Consistence","url":"/2018/Check-XFS-and-EXT-Filesystem-Consistence/","text":"The system will check the disk before booting, and you can check mannually in case any errors occurs. The post will show you how to check hard disk consistence and repair the disk errors 1. Umount the file system ## umount your file system sudo umount /data1 ## or sudo umount /dev/sdb1 2. Check if the disk has errors ## use xfs_check to check disk errors sudo xfs_repair -n /dev/sdb1 ## for the older version, use xfs_check sudo xfs_check /dev/sdb1 3. Repair the file system if any errors ## use xfs_repair to repair errors sudo xfs_repair /dev/sdb1 ## for the ext series, use fsck sudo fsck /dev/sdb1 4. Repair with data loss Finally, if xfs_repair /dev/sdb1 fails, you might use the following to repair the file system with losing some data #### clear journals sudo xfs_repair -L /dev/sdb1 #### repair the file system sudo xfs_repair /dev/sdb1 #### finally check the errors again sudo xfs_repair -n /dev/sdb1 Note Execute xfs_metadump /dev/sdb1 to backup the meta data in case repairation fails document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"Change SSH Port with SELinux Enabled","url":"/2018/Change-SSH-Port-with-SELinux-Enabled/","text":"This post will help you change your server ssh port with selinux enabled on Centos 7 or Fedora 28 or other distros. Suppose the new ssh port is 3333. Change SSH port port=3333 [root@localhost ~] cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak [root@localhost ~] sudo sed -i \"s/#Port 22/Port $port/g\" /etc/ssh/sshd_config Allow port 3333 in Selinux [root@localhost ~] semanage port -d -t ssh_port_t -p tcp 22 [root@localhost ~] semanage port -a -t ssh_port_t -p tcp $port [root@localhost ~] sudo systemctl restart sshd [root@localhost ~] sudo semanage port -l | grep ssh ssh_port_t tcp 3333 Allow port 3333 with firewalld [root@localhost ~] sudo firewall-cmd --permanent --zone=public --add-port=$port/tcp [root@localhost ~] sudo firewall-cmd --reload Now you can login your server with following command ssh -p $port user@ip_address document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux selinux"},{"title":"使用 K- 均值聚类方法进行异常检测","url":"/2018/Anomaly-Detection-with-K-Means-Clustering/","text":"K-means 聚类是一种常用的非监督的机器学习算法 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"machine-learning anomaly-detection"},{"title":"Parallelize Your Large Batches of Small Jobs","url":"/2018/Parallelize-Your-Large-Batches-of-Small-Jobs/","text":"GNU is a free software operating system that is upward-compatible with Unix. GNU parallel is a shell tool for executing jobs in parallel. It uses the lines of its standard input to modify shell commands, which are then run in parallel. – NASA … to be continued… Parallelize Your Job Check remote machine network connection. parallel -j 200 -N 4 \"ping {} >/dev/null 2>&1 && echo 0 {} is connected || echo 1 {} is disconnected\" ::: node{0000..2000} echo > node.lst; for i in {0000..20000};do echo node${i} >> node.lst;done parallel -j 200 -N 4 \"ping {} >/dev/null 2>&1 && echo 0 {} is connected || echo 1 {} is disconnected\" ::: < node.lst If the executing order is important, then you may need the -k option as following and parallel will execute ping in order. parallel -j 200 -N 4 -k \"ping {} >/dev/null 2>&1 && echo 0 {} is connected || echo 1 {} is disconnected\" ::: node{0000..2000} Execute commands on remote machines # obtain remote machine's hostname parallel -j 200 'ssh {} \"echo \\$(hostnamectl --static)\"' ::: < node.lst # obtain remote machine's ip parallel -j 200 'ssh {} \"echo \\$(hostname -I)\"' ::: < node.lst Transfer files to or retrive files from remote machines parallel -S servers --transfer document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux bash hpc"},{"title":"Compiling and Testing the WRF Model","url":"/2018/Compiling-and-Testing-the-WRF-Model/","text":"The Weather Research and Forecasting (WRF) Model is a next-generation mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications. It features two dynamical cores, a data assimilation system, and a software architecture supporting parallel computation and system extensibility. – WRF I. Prerequisites A. Install perl, mpi, hdf5, netcdf using the package manager ####### Fedora / CentOS / Redhat ####### #### Perl sudo dnf/yum install perl perl-devel #### MPICH2 sudo dnf/yum install mpich mpich-devel mpich-autoreload #### openmpi sudo dnf/yum install openmpi openmpi-devel #### hdf5 & netcdf sudo dnf/yum install hdf5 hdf5-devel hdf5-mpich hdf5-mpich-devel hdf5-openmpi hdf5-openmpi-devel netcdf netcdf-devel\\ netcdf-fortran netcdf-fortran-devel netcdf-fortran-mpich netcdf-fortran-mpich-devel netcdf-fortran-openmpi\\ netcdf-fortran-openmpi-devel netcdf-mpich netcdf-mpich-devel netcdf-openmpi netcdf-openmpi-devel ########### ubuntu/debian ############## #### Perl sudo apt install perl libperl-dev #### MPICH2 sudo apt install mpich2 libmpich2-dev #### openmpi sudo apt install openmpi-bin openmpi-common libopenmpi-dev #### netcdf & netcdf-fortran sudo apt install libhdf5-7 libhdf5-dev libhdf5-mpich2-7 libhdf5-mpich2-dev libhdf5-openmpi-7 libhdf5-openmpi-dev\\ libnetcdf-dev libnetcdff5 libnetcdfc7 libhdf5-7 libnetcdfc++4 libcf0 netcdf-bin ########### Windows / macOS ############## ### What? You wanna install WRF on Windows or macOS???? Why are you hating yourself??? ##### If you want compile wrf with intel compiler, you should compile and install hdf5, netcdf and netcdf-fortran mannually as below. B. Compiling hdf5 &amp; netcdf manually hdf5. Download hdf5 and unpack it to hdf5. Current versions are 1.8.20 and 1.10.1, and we choose the latest version. module load mpi/mpich-x86_64 export HDF5=/usr/local ./configure --prefix=$HDF5 --enable-fortran=yes --enable-hl --enable-optimization=high --enable-parallel CFLAGS='-fPIC' make && make check && make install netcdf and netcdf-fortran. Download netcdf and netcdf-fortran, and unpack them. Current version is 4.6.0 for netcdf and 4.4.4 for netcdf-fortran till 5/2/2018. cd netcdf-4.6.0 #### XXX: If you install hdf5 into a non-standard location, remember to add $HDF5/include to $C_INCLUDE_PATH and $HDF4/lib to $LD_LIBRARY_PATH export NETCDF=/usr/local ./configure --prefix=$NETCDF --enable-dot --enable-fsync --enable-cdf5 LDFLAGS=\"-L$HDF5/lib\" CPPFLAGS=\"-I$HDF5/include\" make -j4 && make check && make install cd ../ cd netcdf-fortran-4.4.4 ./configure --prefix=/usr/local --enable-dot LDFLAGS=\"-L$HDF5/lib -L$NETCDF/lib\" CPPFLAGS=\"-I$HDF5/include -I$NETCDF/include\" ake -j4 && make check && make install II. Compiling WRF The latest release of WRF is 3.9.1.1 till now 5/2/2018. Download the WRF source code, and extract it to WRF Configure WRF module load mpi/mpich-x86_64 #### If you didn't set NETCDF and HDF5 environment variables, set them to where they are installed. #export NETCDF=/usr/local #export HDF5=/usr/local export WRFIO_NCD_LARGE_FILE_SUPPORT=1 #### run ./configure ./configure After run `./configure`, it will let us to choose which option to compile. Here we choose `35`, which `wrf` will use `mpi` and `openmp`. Please select from among the following Linux x86_64 options: 1. (serial) 2. (smpar) 3. (dmpar) 4. (dm+sm) PGI (pgf90/gcc) 5. (serial) 6. (smpar) 7. (dmpar) 8. (dm+sm) PGI (pgf90/pgcc): SGI MPT 9. (serial) 10. (smpar) 11. (dmpar) 12. (dm+sm) PGI (pgf90/gcc): PGI accelerator 13. (serial) 14. (smpar) 15. (dmpar) 16. (dm+sm) INTEL (ifort/icc) 17. (dm+sm) INTEL (ifort/icc): Xeon Phi (MIC architecture) 18. (serial) 19. (smpar) 20. (dmpar) 21. (dm+sm) INTEL (ifort/icc): Xeon (SNB with AVX mods) 22. (serial) 23. (smpar) 24. (dmpar) 25. (dm+sm) INTEL (ifort/icc): SGI MPT 26. (serial) 27. (smpar) 28. (dmpar) 29. (dm+sm) INTEL (ifort/icc): IBM POE 30. (serial) 31. (dmpar) PATHSCALE (pathf90/pathcc) 32. (serial) 33. (smpar) 34. (dmpar) 35. (dm+sm) GNU (gfortran/gcc) 36. (serial) 37. (smpar) 38. (dmpar) 39. (dm+sm) IBM (xlf90_r/cc_r) 40. (serial) 41. (smpar) 42. (dmpar) 43. (dm+sm) PGI (ftn/gcc): Cray XC CLE 44. (serial) 45. (smpar) 46. (dmpar) 47. (dm+sm) CRAY CCE (ftn $(NOOMP)/cc): Cray XE and XC 48. (serial) 49. (smpar) 50. (dmpar) 51. (dm+sm) INTEL (ftn/icc): Cray XC 52. (serial) 53. (smpar) 54. (dmpar) 55. (dm+sm) PGI (pgf90/pgcc) 56. (serial) 57. (smpar) 58. (dmpar) 59. (dm+sm) PGI (pgf90/gcc): -f90=pgf90 60. (serial) 61. (smpar) 62. (dmpar) 63. (dm+sm) PGI (pgf90/pgcc): -f90=pgf90 64. (serial) 65. (smpar) 66. (dmpar) 67. (dm+sm) INTEL (ifort/icc): HSW/BDW 68. (serial) 69. (smpar) 70. (dmpar) 71. (dm+sm) INTEL (ifort/icc): KNL MIC 72. (serial) 73. (smpar) 74. (dmpar) 75. (dm+sm) FUJITSU (frtpx/fccpx): FX10/FX100 SPARC64 IXfx/Xlfx #### nesting option: 1=basic just ok Compile for nesting? (1=basic, 2=preset moves, 3=vortex following) [default 1]: 1 ...... #### configure done After configure finished, we then can run ./compile wrf to compile WRF. Note that if one choose INTEL compiler with mpi and openmp, one might need to edit configure.wrf by changing -openmp to -qopenmp, or it will fail to compile. ./compile wrf ./compile em_real #./compile em_fire #./compile convert_em When compilation is done, you will find `wrf.exe` and other executable files under the `main` directory. III. Runnig Test of WRF The test directory is WRF/run, and you can run a test job on a cluster… -_-… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux hpc"},{"title":"GDM Crashed Everytime Screen Blanks under Fedora 27","url":"/2018/GDM-Crashes-After-Screen-Blanks-under-Fedora-27/","text":"Recently my fedora 27 crashed so frequently after once system update that I switched to CentOS 7. CentOS 7 is stable quite enough, while the question is many packages are so old. And I had to return to fedora 27 again… Life goes on, and the system crashes as ever. Problem Finally, I found that everytime the screen went blank, the GDM service stopped and I had to login times before GMD works properly or forced to reboot the system. Fed up with this issue, I checked the journal log, and found the error: .... Feb 04 22:58:49 xxxxxx systemd[1]: Starting GNOME Display Manager... Feb 04 22:58:49 xxxxxx systemd[1]: Started GNOME Display Manager. Feb 05 14:46:27 xxxxxx gdm[768]: Tried to look up non-existent conversation gdm-launch-environment Feb 05 14:46:27 xxxxxx gdm[768]: Freeing conversation 'gdm-launch-environment' with active job Feb 05 14:46:27 xxxxxx systemd[1]: Stopping GNOME Display Manager... Feb 05 14:46:27 axxxxxx gdm[768]: Freeing conversation 'gdm-password' with active job Feb 05 14:46:27 xxxxxx gdm[768]: Child process -833 was already dead. Feb 05 14:46:27 xxxxxx gdm[768]: Child process -833 was already dead. Feb 05 14:46:27 xxxxxx gdm[768]: GLib: g_variant_new_string: assertion 'string != NULL' failed Feb 05 14:46:27 xxxxxx gdm[768]: Failed to contact accountsservice: Error calling StartServiceByName for org.freedesktop.Accounts: GDBus.Error:org.freedesktop.systemd1.ShuttingDown: Refusing activation, D-Bus Feb 05 14:46:27 xxxxxx gdm[768]: GdmDisplay: display lasted 0.000770 seconds Feb 05 14:46:27 xxxxxx gdm[768]: Failed to contact accountsservice: Error calling StartServiceByName for org.freedesktop.Accounts: GDBus.Error:org.freedesktop.systemd1.ShuttingDown: Refusing activation, D-Bus Feb 05 14:46:27 xxxxxx gdm[768]: GLib: g_hash_table_find: assertion 'version == hash_table->version' failed Feb 05 14:46:27 xxxxxx systemd[1]: Stopped GNOME Display Manager. .... Solution It seemed that something went wrong with GLib. After googling on GLib: g_hash_table_find: assertion 'version == hash_table-&gt;version' failed, I found the post. It turned out that the package gdm-3.26.2 has bugs that will lead to system crash. And it seems that the bug exists since Fedora 25… After downgrading gdm-3.26.2 to gdm-3.26.1, gdm is working well so far… W.T.F. … In the meanwhile, it reminds me of the pango-1.40.12 bug, which led to nautlis displaying filename incorrectly, and the solution was downgrading pango too … W.T.F. … Update The problem occurs on Fedora 28… Downgrading GDM from gdm-3.28.2 to gdm-3.28.1 seems to fix the problem. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux fedora gnome"},{"title":"在天河 II 号和 Cori 上安装 Chroma","url":"/2018/Install-Chroma-on-Tianhe-II-and-Cori/","text":"The Chroma package supports data-parallel programming constructs for lattice field theory and in particular lattice QCD. Here is how I compile chroma on Tianhe II and Cori at NERSC. 准备工作 下载依赖软件包 Chroma 依赖 qmp, qdp++, 如果要支持 GPU, 则需要 qdp-jit: qmp qdp++ qdp-jit chroma 相应地, qdp++, qdp-jit, chroma 等还依赖其他一些库: qio filedb c-lime libintrin xpath_reader qdp_lapack 从 github 上 clone 下相应包: $ mkdir chroma-all && cd chroma-all $ git clone https://github.com/usqcd-software/qmp $ git clone https://github.com/usqcd-software/qdpxx $ git clone https://github.com/fwinter/qdp-jit $ git clone https://github.com/JeffersonLab/chroma $ cd qdpxx && git submodule update --init --recursive && cd .. $ cd qdp-jit && git submodule update --init --recursive && cd .. $ cd chroma && git submodule update --init --recursive && cd .. $ cd .. && tar cvjf chroma-all.tar.bz2 chroma-all 打包好源码后, 上传到 天河 II 或 Cori 上. 环境变量设置 ### Destination: $HOME/usr $ export app_root=$HOME/usr ### dependent modules for Tianhe II ###### if using gcc compiler $ module load gcc/4.9.2 $ module load MPI/mpich/3.2.1-gcc-4.9.2-dynamic $ export mpi_home=/BIGDATA1/app/MPI/mpich/3.2.1-gcc-4.9.2-dynamic/ $ module load hdf5/1.8.13-gcc-4.8.5-parallel $ export hdf5_home=/BIGDATA1/app/hdf5/1.8.13-gcc-4.8.5-parallel/ $ module load libxml2/2.9.4-gcc-4.8.5 $ ccc=gcc $ cpc=g++ $ fort=gfortran ###### if using intel compilers $ module load intelcompiler/18.0.0 $ module load MPI/mpich/3.2.1-icc-18.0.0-dynamic $ module load hdf5/1.10.4-icc-14.0.2-parallel $ ccc=icc $ cpc=icpc $ fort=ifort $ export hdf5_home=/BIGDATA1/app/hdf5/1.10.4/ ##-------------------------------------## ### dependent modules for Cori $ module swap PrgEnv-intel PrgEnv-gnu $ module unload darshan ##?? $ module load hdf5-parallel/1.10.1 $ export hdf5_home=/usr/common/software/hdf5-parallel/1.10.1/ ######################################### ### dependence form IHEP ### PATH $ export PATH=${app_root}/bin:$PATH $ ### LD_LIBRARY_PATH $ export LD_LIBRARY_PATH=${app_root}/lib:$LD_LIBRARY_PATH:$mpi_home/lib:$hdf5_home/lib 如果要支持 GPU: #### for gcc $ module load gcc/4.9.2 $ module load MPI/Gnu/MPICH/3.2-gcc4.9.2-dyn $ export mpi_home=/NSFCGZ/app/MPI/Gnu/MPICH/3.2-gcc4.9.2-dyn/ $ module load hdf5/1.8.17/01-CF-14-par $ export hdf5_home=/NSFCGZ/app/hdf5/1.8.17/01-CF-14-par/ $ module load CUDA/8.0 $ export cuda_home=/NSFCGZ/app/CUDA/8.0 $ export cuda_libs=\"-L$cuda_home/lib64 -L$cuda_home/lib64/stubs\" $ ccc=gcc $ cpc=g++ $ fort=gfortran ### PATH $ export PATH=${app_root}/bin:$PATH:$cuda_home/bin $ ### LD_LIBRARY_PATH $ export LD_LIBRARY_PATH=${app_root}/lib:$LD_LIBRARY_PATH:$mpi_home/lib:$hdf5_home/lib:$cuda_home/lib64:$cuda_home/lib64/stubs 安装 QMP ### cd the source directory $ cd qmp && autoreconf ### for Tianhe II $ ./configure --prefix=$app_root --with-qmp-comms-type=MPI --with-qmp-comms-libs='-lmpi -lmpich -lmpichcxx' CFLAGS=\"-O2 -std=c11 -mavx -msse3\" CC=$ccc CXX=$cpc FC=$fort ### for Cori $ ./configure --prefix=$app_root --with-qmp-comms-type=MPI --with-qmp-comms-libs='-lmpich -lmpichcxx' ### both $ make -j4 && make check && make install 安装 QDP/C 安装 QDP++ $ cd qdp && autoreconfig ### for Tianhe II $ ./configure --prefix=${app_root} --with-qmp=${app_root} --enable-parallel-arch=parscalar --with-hdf5=${hdf5_home} CFLAGS=\"-O2 -std=c11 -mavx -msse3 -fopenmp\" CXXFLAGS=\"-O2 -std=c++11 -mavx -msse3 -fopenmp\" --enable-sse3 CC=$ccc CXX=$cpc FC=$fort ### for Cori $ ./configure --prefix=${app_root} --with-qmp=${app_root} --enable-parallel-arch=parscalar --with-hdf5=${hdf5_home} CXXFLAGS=\"-O2 --std=c++11\" LDFLAGS=\"-L/opt/cray/pe/mpt/7.6.0/gni/mpich-gnu/5.1/lib\" CPPFLAGS=\"-I/opt/cray/pe/mpt/7.6.0/gni/mpich-gnu/5.1/include\" ### both ### there are some errors when running \"make check\" $ make -j4 && make install 安装 QDP-JIT 如果要支持 GPU, 需要安装 QDP-JIT: $ cd qdp-jit && autoreconf -i $ ./configure --prefix=$app_root --enable-cuda-managed-memory --enable-filedb --enable-openmp --with-qmp=${app_root} --enable-parallel-arch=parscalar CFLAGS=\"-O2 -std=c11 -mavx -msse3 -fopenmp\" CXXFLAGS=\"-O2 -std=c++11 -mavx -msse3 -fopenmp\" CC=mpicc CXX=mpicxx Fort=mpifort CPPFLAGS=\"-I/usr/include/llvm6.0\" 安装 QUDA $ cd ../quda && aclocal && autoreconf -i $ mkdir quda_build && cd quda_build $ CC=$ccc CXX=$cpc cmake ../quda -DQUDA_GPU_ARCH=sm_37 -DCPU_ARCH=x86_64 -DQUDA_MPI=off -DQUDA_INTERFACE_QDPJIT=on -DQUDA_QDPJIT=on -DQUDA_QDPHOME=$app_root -DQUDA_QMP=on -DQUDA_QMPHOME=$app_root -DQUDA_QIO=on -DQUDA_QIOHOME=$app_root -DQUDA_LIMEHOME=$app_root -DMPI_CXX_COMPILER=mpicxx -DMPI_C_COMPILER=mpicc -DQUDA_USE_EIGEN=on -DCMAKE_C_FLAGS_DEVEL=\"-Wall -O3 -mavx -msse3 -std=c11\" -DCMAKE_BUILD_TYPE=DEBUG -DQUDA_POSIX_THREADS=on $ make -j24 && make install cd .. 安装 CHROMA $ cd chroma && autoreconf ############ for Tianhe II $ ./configure --prefix=${app_root} --with-qmp=${app_root} --with-hdf5=${hdf5_home} CXXFLAGS=\"-O2 -std=c++11 -mavx -msse3\" CFLAGS=\"-O2 -std=c11 -mavx -msse3\" CC=$ccc CXX=$cpc FC=$fort ### if cuda enabled $ ./configure --prefix=${app_root} --with-qmp=${app_root} --with-qdp=$app_root --with-hdf5=${hdf5_home} CXXFLAGS=\"-O3 -std=c++11 -mavx -msse3 -fopenmp\" CFLAGS=\"-O3 -std=c11 -mavx -msse3 -fopenmp\" CC=$ccc CXX=$cpc FC=$fort --with-qio=$app_root --enable-sse3 LDFLAGS=\"$cuda_libs\" --with-quda=$app_root --enable-jit-clover --enable-quda-deviface # --enable-opt-eigcg #--enable-sse-wilson-dslash --enable-lapack=lapack ############### for Cori $ ./configure --prefix=${app_root} --with-qmp=${app_root} --with-hdf5=${hdf5_home} CXXFLAGS=\"-O2 -std=c++11\"\\ LDFLAGS=\"-L/opt/cray/pe/mpt/7.6.0/gni/mpich-gnu/5.1/lib\" CPPFLAGS=\"-I/opt/cray/pe/mpt/7.6.0/gni/mpich-gnu/5.1/include\" ### both $ make -j4 && make check && make install document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"lattice linux mpi"},{"title":"Docker 使用笔记","url":"/2018/Docker-Learning-Notes/","text":"自从 13 年发布以来, Docker 已经成为虚拟容器领域最火热的技术. Docker 是什么 Docker 优势 Docker 常用命令 Docker 镜像选择 Docker 使用实例 SS Kubernetes 容器编排 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux docker kubernetes"},{"title":"Machine Learning Notes","url":"/2018/Machine-Learning-Notes/","text":"机器学习是什么呢？它是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。实际一点的说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。 准备知识 统计学知识 Python / R 视频课程 吴恩达 机器学习课程 Yaser Abu-Mostafa 机器学习与数据挖掘 Tom Mitchell 机器学习 学习书籍 统计学习方法 , 李航著 机器学习, 周志华著 机器学习, Peter Flach 机器学习, Tom. M. Mitchell 实用机器学习, Henrik Brink 等 TensorFlow 机器学习项目实战, Rodolfo Bonnin, TensorFlow 机器学习实战指南, Nick McClure 网站 入手项目 相关论文 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux machine-learning tensorflow python deep-learning"},{"title":"A Simple Instroduction to Slurm","url":"/2017/Slurm-Instroduction-Configuration-And-Usage/","text":"Slurm document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"hpc htc slurm"},{"title":"在 CentOS 7 上安装 4.9+ 内核并开启 BBR","url":"/2017/Install-Kernel-4-9-and-Turn-on-BBR-in-CentOS-7/","text":"什么是 BBR 呢? 来自 google 的黑科技。 安装并替换最新内核 可从 这里 查看, 安装最新的内核: $ sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org $ sudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm $ sudo sed -i \"s/enabled=0/enabled=1/g\" /etc/yum.repos.d/elrepo.repo $ sudo yum install kernel-ml -y 更新 grub 引导 安装好最新内核后, 我们要保证系统会从新内核启动: $ sudo egrep ^menuentry /etc/grub2.cfg ### 选择最新的那一个， 比如 0 $ sudo grub2-set-default 0 $ sudo reboot 开启 BBR 启用 bbr 内核模块: $ modprobe tcp_bbr $ echo \"tcp_bbr\" | sudo tee -a /etc/modules-load.d/modules.conf 执行并保存生效: $ echo \"net.core.default_qdisc=fq\" | sudo tee -a /etc/sysctl.conf $ echo \"net.ipv4.tcp_congestion_control=bbr\" | sudo tee -a /etc/sysctl.conf $ sudo sysctl -p 检查是否已经开启 BBR: $ sudo sysctl net.ipv4.tcp_available_congestion_control $ sudo sysctl net.ipv4.tcp_congestion_control 如果两个命令都有 bbr， 则说明内核已开启 BBR. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux centos"},{"title":"Access Your Local Machine Remotely with a Reverse SSH Tunnel","url":"/2016/Access-Your-Local-Machines-with-A-Reverse-SSH-Tunnel/","text":"You work on your machine in office behind a nat happily, and you wanna login into your system after work like at home just for something little things. So, what should you do? By searching on google, you find teamviewer, and what a program you may scream. Downloading, installing painfully (sure it is on linux), and you find your things finally. Is that awesome? Yeah, it is awesome, apart from the installation and uncertain security, but what I wanna do is just accessing my files and has to be open to other strangers? How about ssh? When coming to security, it’s time that you should turn to SSH, allowing you getting your files via a secure reverse tunnel, without exposing your privacy to others. In the meanwhile, SSH is usually installed by default, and you can easily setup a service for starting automatically. Things are simple. For simplicity, suppose we have A in office behind a nat, B at home behind a nat too (most cases now), C a remote server with public IP - remote_addr which is required, if you haven’t, you can obtain from my Aff link: Vultr. A. Establishing a secure connection from B to C ssh -gNfR remote_port:localhost:local_port user@remote_addr and we have now a secure connection , and a reverse tunnel meanwhile. Make sure that you have installed ssh-daemond both on B and C, and starting the daemond. B. Login to C from A It’s a common ssh connection, ssh user@remote_addr C. Login to B from C Using the previous connection from A to C, ssh -p remote_port user@localhost And now you are on B if everything goes well!! Have fun with your connection… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux ssh teamviewer"},{"title":"爱你就像爱生命 -- 王小波","url":"/2016/Love-You-Like-My-Life/","text":"《爱你就像爱生命》收录了王小波生前从未发表过的与李银河的“两地书”，以及婚后他们夫妇与其他朋友的书信往来，还完整收录了李银河深情怀念王小波的三篇文章。 这里节选的是其中的一封王小波写给李银河的 – 爱你就像爱生命 … 我对好多人怀有最深的感情，尤其是对你。 ……你知道吗，郊外的一条大路认得我呢。有时候，天蓝得发暗，天上的云彩白得好像一个凸出来的拳头。那时候这条路上就走来一个虎头虎脑、傻乎乎的孩子，他长得就像我给你那张相片上一样。后来又走过来一个又黑又瘦的少年。后来又走过来一个又高又瘦又丑的家伙，涣散的要命，出奇的喜欢幻想。后来，再过几十年，他就永远不会走上这条路了。你喜欢他的故事吗？ 什么样的灵魂就要什么样的养料。 谁也不应该死气白赖地不愿意从泥坑里站起来。 越悲怆的时候我越想嬉皮。 我把我整个灵魂都给你，连同它的怪癖，耍小脾气，忽明忽暗，一千八百种坏毛病。它真讨厌，只有一点好，爱你。 我要说的是：只要我们真正相爱，哪怕只有一天，一个小时，我们就不应该再有一刀两断的日子。 什么排山倒海的力量也止不住两个相爱过的人的互助。我觉得我爱了你了，从此以后，不管什么时候我都不能对你无动于衷。 我爱谁就觉得谁就是我本人。 我很讨厌我自己不温不凉的思虑过度，也许我是个坏人，不过我只要你吻我一下就会变好呢。 你呀，又勾起我想起好多事情。我们的生活的支点是什么？就是我们自己。自己要一个绝对美好的不同凡响的生活，一个绝对美好的不同凡响的意义。你让我想起光辉、希望、醉人的美好。今生今世永远爱美，爱迷人的美。任何不能令人满意的东西，不值得我们屈尊。 你一来，我就决心正经地、不是马虎地生活下去，哪怕要费心费力呢，哪怕我去牺牲呢。 我要你，和我有宿缘的人。 我要和他一起深入这个天地，一去再也不回来。 银河，我爱你，我们来过快乐的生活吧！ 要是你爱我，老和我好，变成老头我也不怕。咱们先来吃果酱吧，吃完了两个人就更好了，好到难舍难分，一起去见鬼。你怕吗？我就不怕，见鬼就见鬼。我和你好。 真希望下个星期日早来，并且那一天春光明媚。 人们懒于改造世界必然勤于改造自己，懒于改造生产方式，懒于进行思想劳动必然勤于体力劳动，懒于创造性的思想活动必然勤于死记硬背。 咱们应当在一起，否则就太伤天害理啦。 我的勇气和你的勇气加起来，对付这个世界总够了吧？ 可能有一个致命的缺点，生命力还不够强。我们灵魂缺燃料，它有时虽然能迸出火花，但是不能总是熊熊地燃烧。 你是我的天堂，可我是你的地狱。 我有时十分痛恨自己，觉得我是一个坏人，你来救我吧。 我们常常把事情弄得太沉重了，咱们该轻松些，咱们应该像一对疯子那样歌舞狂欢，对吗？生活本来是很美好、很美好的呵！ 如果我伤了你的心，请你原谅我，因为我们过去说过，要把心中发生的一切告诉对方。否则，它就会变成一种潜伏的危机。 别怀疑我们会不会幸福！ 有时候你难过了，这时候我更爱你。只要你不拒绝我的拥抱你，我会告诉你这是因为什么。我就是我不知是为了什么。我会告诉你爱，爱可以把一切都容下。如果我的爱不能容下整个的你，算个什么爱！ 我又瞎说了一通，千万不要有什么话又惹你生气。你生气了就哭，我一看见你哭就目瞪口呆，就像一个小孩子做了坏事在未受责备之前目瞪口呆一样，所以什么事你先别哭，先来责备我，好吗？ 我老觉得爱情奇怪，它是一种宿命的东西。对我来说，它的内容就是“碰上了，然后就爱了，然后一点办法也没有了”。它就是这样！爱上，还非要人家也来爱不可。否则不叫爱，要它也没有意思。海誓山盟有什么用？我要的不就是我爱了人家人家也爱我吗？我爱海誓山盟拉来的一个人吗？不呢，爱一个爱我的人，就这样。 你要是愿意，我就永远爱你，你要不愿意，我就永远相思。 别怕美好的一切消失，咱们先来让它存在。 我的心总向往你，特别是在悲伤的时候。 从第一天见到你时我就看出来了：你的生命的活力在吸引我，我不由自主地要到你那里去，因为你那里有生活，有创造，有不竭的火，有不尽的源泉。我们一起请求上帝，愿它永远不要枯竭吧！ 我非常非常地想你，特别是在紧张工作的间歇。我觉得这世界上好像除了你和工作，什么都不存在了。 总之现在我们要好，对吗？ 什么时候我们可以自由自在地爱就好了，我不爱让人知道我是怎么想的，不过我永远不怕对任何人承认我爱你。爱呀，写呀，自由自在，可以自由自在地在一起。 静下来想你，觉得一切都美好得不可思议。 告诉你，一想到你，我这张丑脸上就泛起微笑。还有在我安静的时候，你就从我内心深处浮现，就好像阿芙罗蒂从浪花里浮现一样。 你瞧，你从我内心深处经常出现，给我带来幸福，还有什么离间的了我们？咱们可不会变成火炉边的两个小傻瓜。别人也许会诧异咱们的幸福和他们的不一样，可那与我们有何相干？他们的我们不要，我们的他们也不知道。 你哭也好，说也好，懒也好，我都喜欢你。有时候我也会没精打采，那时候不许你欺负我！不过我反正不怕你笑话。 但愿我和你，是一支唱不完的歌。 我们俩都不是什么美男美女，可是心灵和智力上有种难以言传的吸引力。我起初怀疑，一对不美的人的恋爱能是美的吗？后来的事证明，两颗相爱的心在一起可以是美的。 我和你就像两个小孩子，围着一个神秘的果酱罐，一点一点地尝它，看看里面有多少甜。 人就像一本书，你要挑一本好看书的来看。我觉得我生命中最大的收获和幸运就是，我挑了小波这本书来看。 我曾经是世界上最幸福的人；失去了他，我现在是世界上最痛苦的人。 她哭着拥抱我，说在海上找了我一夜。人们都相信我已经淹死了，但是她不相信我会死。我把她引到那块石头前，让她看我写的诗。她默默地看了很久，然后向我要那片硬质合金，要我把我的名字刻上去。可是我不让她刻。我不需要刻上我的名字。名字对我无关紧要。我不希望人们知道我的名字，因为我的胜利是属于我的。 摘自《爱你就像爱生命》, 王小波与李银河的 两地情书. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"王小波 书信"},{"title":"纳兰词选","url":"/2016/Na-Lan-Ci-Xuan/","text":"纳兰性德, 或者更熟悉的, 纳兰容若, 一个写满哀伤的名字, 才情绝艳. “生于温柔富贵，却满篇哀感顽艳；身处花柳繁华，心却游离于喧闹之外；真正的八旗子弟，却喜结交落拓之人；行走于仕途，一生却为情所累；风华正茂之时，却匆匆离世…” – 纳兰容若词传 家家争唱饮水词，纳兰心事几曾知？ 初识容若, 是在高二的寒假, 看到了安意如的 &lt;&lt; 当时只道是寻常 &gt;&gt;, 便被其优美感伤的文字吸引了, 进而对容若其人充满了兴趣. 虽然安意如其书评价毁誉参半, 但它却是把我带进了容若的世界. 容若的词里, 满是忧伤, 满是遗憾， 满是会议， 满是踌躇不得志. 正是少年不识愁滋味的时候, 他的词对当时年少的自己充满了诱惑. 亦或许是当时也有着那样这样的遗憾, 所谓的感同身受, 让自己也浸润着一种深深的哀伤. 沉思往事立残阳, 当时只道是寻常 , 真的是这样啊. 可是人生哪会没有遗憾呢. 即使是当初认定的那个人, 如今又在何方呢. 还是收拾好行囊, 即刻出发吧. 沉思往事立残阳, 当时只道是寻常。 木兰花令·拟古决绝词 人生若只如初见，何事秋风悲画扇。 等闲变却故人心，却道故人心易变。 骊山语罢清宵半，泪雨霖铃终不怨。 何如薄幸锦衣郎，比翼连枝当日愿。 浣溪沙 其一 谁念西风独自凉，萧萧黄叶闭疏窗。沉思往事立残阳。 被酒莫惊春睡重，赌书消得泼茶香。当时只道是寻常。 其二 伏雨朝寒悉不胜，那能还傍杏花行。去年高摘斗轻盈。 漫惹炉烟双袖紫，空将酒晕一衫青。人间何处问多情。 其三 十八年来坠世间，吹花嚼蕊弄冰弦，多情情寄阿谁边？ 紫玉钗斜灯影背，红绵粉冷枕函边。相看好处却无言。 又 风髻抛残秋草生。高梧湿月冷无声。当时七夕记深盟。 信得羽衣传钿合，悔教罗袜葬倾城。人间空唱雨淋铃。 又 记绾长条欲别难。盈盈自此隔银湾。便无风雪也摧残。 青雀几时裁锦字，玉虫连夜剪春幡。不禁辛苦况相关。 又 莲漏三声烛半条，杏花微雨湿轻绡，那将红豆寄无聊？ 春色已看浓似酒，归期安得信如潮，离魂入夜倩谁招？ 又 一半残阳下小楼，朱帘斜控软金钩。倚栏无绪不能愁。 有个盈盈骑马过，薄妆浅黛亦风流。见人羞涩却回头。 采桑子 其一 谁翻乐府凄凉曲？风也萧萧，雨也萧萧，瘦尽灯花又一宵。 不知何事萦怀抱，醒也无聊，醉也无聊，梦也何曾到谢桥。 又 彤霞久绝飞琼字，人在谁边。人在谁边，今夜玉清眠不眠。 香销被冷残灯灭，静数秋天。静数秋天，又误心期到下弦。 梦江南 昏鸦尽，小立恨因谁? 急雪乍翻香阁絮，轻风吹到胆瓶梅，心字已成灰。 虞美人 其一 银床淅沥青梧老，屧粉秋蛩扫。采香行处蹙连钱，拾得翠翘何恨不能言。 回廊一寸相思地，落月成孤倚。背灯和月就花阴，已是十年踪迹十年心。 又（秋夕信步） 愁痕满地无人省，露湿琅玕影。闲阶小立倍荒凉。还剩旧时月色在潇湘。 薄情转是多情累，曲曲柔肠碎。红笺向壁字模糊，忆共灯前呵手为伊书。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"纳兰容若 诗词"},{"title":"Tmux - 远程工作的瑞士军刀","url":"/2016/Tmux-Swiss-Army-Knife-for-Telework/","text":"tmux 是一个优秀的终端复用软件, 是 BSD 下的 Screen 的替代品. 相对于 Screen，tmux 更加先进：支持屏幕切分，而且具备丰富的命令行参数，使其可以灵活、动态的进行各种布局和操作。 为什么用 tmux? 当你登录到远程终端, 经常发现，一个终端远远地不够用，经常需要同时打开几个，往往还希望这几个窗口同时显示在视线内。而 tmux 正是为这种需求而生。通过一个终端登录远程主机并运行 tmux 后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机. 与 screen 相比,tmux 可以横向和纵向分割窗口，且窗格可以自由移动和调整大小。可在多个缓冲区进行复制和粘贴，支持跨窗口搜索；非正常断线后不需重新 detach… tmux 简直是程序员的福音… 获取 tmux 一般 linux 发行版都有 tmux 包, 可以安装很方便的安装. #fedora sudo dnf -y install tmux #or sudo yum -y install tmux #opensuse sudo zypper in tmux #ubuntu/debian sudo apt-get install tux Mac 可以用 brew 或 macports 安装. #brew brew install tmux #macports sudo port install tmux 也可以手动编译安装, 可以从 这里 下载安装. tmux 依赖 libevent, 所以也一并装了吧, 到 这里 下载. tmux 初体验 安装 tmux 之后, 运行 tmux 就进入了 tmux 环境 ((⊙ o ⊙) 啊!!!). 和普通的 Terminal 没什么不同， 除了底部有一个状态栏. tmux 修改配置 tmux 的配置文件是$HOME/.tmux.conf 默认的配置感觉不方便, 可自己修改. 下面是博主的配置 #### Global Key-mapping #tmux 默认控制键是 Ctrl-b (C-A or ^b), 可以修改为 ^a set -g prefix ^a #设置全局的按键模式为 vi 模式 setw -g mode-keys vi # 鼠标滚轮滚屏, 如果需要. # setw -g mode-mouse on # # Options set-option -g base-index 1 set-option -g display-time 1000 set-option -g repeat-time 500 set-option -g status-keys vi # 修改配置文件后按 ^br 载入配置文件 bind r source-file ~/.tmux.conf set -g default-terminal \"screen-256color\" set -g status-utf8 on set -g pane-border-fg green set -g pane-active-border-fg white set -g message-fg white set -g message-attr bright set -g status-fg white set -g status-bg black setw -g window-status-fg cyan setw -g window-status-bg default setw -g window-status-attr dim setw -g window-status-current-fg cyan setw -g window-status-current-bg default setw -g window-status-current-attr dim set -g status-left-length 40 set -g status-left \"#[fg=green]Session: #S #[fg=yellow] #I #[fg=cyan]#P\" set -g status-right \"#[fg=cyan]%d/%b/%Y #(date +%H:%M:%S'')\" set -g status-interval 2 set -g status-justify centre setw -g monitor-activity on set -g visual-activity on ####Windows #垂直分割窗口 unbind '\"' bind - splitw -v #横向分割窗口 unbind % bind = splitw -h #上下左右移动活动窗口 bind k selectp -U bind j selectp -D bind h selectp -L bind l selectp -R # Windows Size: 调节窗口大小 bind ^k resizep -U 2 bind ^j resizep -D 2 bind ^h resizep -L 2 bind ^l resizep -R 2 # Swap Windows: 交换窗口 bind ^u swapp -U bind ^d swapp -D tmux 常用命令 # 新建 tmux 会话 tmux #查看 tmux 会话 tmux ls #or 在 tmux 内查看 C-b s #attach 到某一会话 tmux a -t 1 #从 tmux 中 detach 会话 C-b d 水水更健康 .(*￣︶￣)y document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux tmux"},{"title":"Setup Proxy for Fedora Package Manager - DNF","url":"/2016/Setup-Proxy-for-Fedora-Package-Manager-DNF/","text":"For various reasons, we may want let dnf run behind a proxy for some repos, or all repos. Here is a simple and effecient way that can sovle this problem. The behavior of dnf is controlled by /etc/dnf/dnf.conf, where we will do with dnf. The solution is simple, adding the following lines into dnf.conf, proxy=http://your.proxy.add:port proxy_username=user proxy_password=passwd There are serveral proxy types that dnf supports: http, https, socks5, socks5h, socks4, ftp, socks4a. For a socks5 proxy, proxy=socks5://your.socks5.add:port proxy_username=user proxy_password=passwd Then all network traffic from dnf will go through a proxy. In the meanwhile, what if we just want set proxy for some a repo? One can just add the proxy part to the specific repo like google-chrome.repo. name=google-chrome baseurl=https://dl.google.com/linux/chrome/rpm/stable/x86_64 enabled=1 gpgcheck=1 gpgkey=https://dl.google.com/linux/linux_signing_key.pub proxy=socks5://localhost:1080 That’s all and feel free with dnf. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux fedora proxy"},{"title":"Building Your Own Ngrok Service on CentOS 7","url":"/2016/Building-Your-Own-Ngrok-Service-on-CentOS-7/","text":"What is ngrok? Ngrok is a reverse proxy tool which can establish a secure tunnel between local machine and the common service. I. Why Do We Need Ngrok? Actually, there are many ngrok services on the web, free or non-free. II. How to Build Own Ngrok Service? Well then, how to build one ngrok service? A. Prerequisites First you need a actual or virtual private server that has own IP and have git installed, like vultr or DigitalOcean, and a local machine behind a router with Linux or Windows. B. Configuring Go Environment Since ngrok is written by go, we need install it. GO can be easily installed via yum or apt if ubuntu or debian. But if you want build a windows client, you need to compile go manually. On the Server， ### via yum sudo yum install go ### compiling manually ver: 1.7.1 wget https://storage.googleapis.com/golang/go1.7.1.linux-amd64.tar.gz sudo tar -C /usr/local -xzf go.1.7.1.linux-amd64.tar.gz When finished, we may need configure the bash environment like this mkdir ${HOME}/go echo 'export GOROOT=/usr/local/go' >> ~/.bashrc echo 'export GOPATH=$HOME/go' >> ~/.bashrc echo 'export PATH=$PATH:$GOROOT/bin' >> ~/.bashrc source $HOME/.bashrc C. Getting Ngrok On the server, git clone https://github.com/inconshreveable/ngrok.git D. Compiling Ngrok a. setting up environment variable export NGROK_DOMAN=\"your.domain.tld\" b. generating a self-signed ssl certificate cd ngrok openssl genrsa -out rootCA.key 4096 openssl req -x509 -new -nodes -key rootCA.key -subj \"/CN=$NGROK_DOMAIN\" -days 50000 -out rootCA.pem openssl genrsa -out device.key 4096 openssl req -new -key -device.key -subj \"/CN=$NGROK_DOMAIN\" -out device.csr openssl x509 -req -in device.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out device.crt -days 5000 cp rootCA.pem assets/client/tls/ngrokroot.crt cp device.crt assets/server/tls/snakeoil.crt cp device.key assets/server/tls/snakeoil.key c. building server and client make release-server release-client d. ngrok service Creating a systemd service file named ngrok.service: [Unit] Description=ngrok server service After=syslog.target network.target auditd.service [Service] Type=simple User=root ExecStart=/path/to/ngrok/bin/ngrokd -domain=your.domain.tld -httpAddr=:8000 -httpsAddr=:4443 KillMode=process Restart=on-failure [Install] WantedBy=multi-user.target Copy ngrok.service to /usr/lib/systemd/system, enable and start ngrok service sudo cp ngrok.service /usr/lib/systemd/system sudo systemctl enable ngrok.service ### if firewall-cmd is enabled sudo firewall-cmd --permanent --add-port=8000/tcp sudo firewall-cmd --permanent --add-port=4443/tcp sudo firewall-cmd --reload sudo systemctl start ngrok.service III. Comments document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ssh ngrok vultr"},{"title":"Frequently Used Apps Under MacOS","url":"/2016/Frequently-Used-Apps-Under-MacOS/","text":"Here I list some programs frequently used under MacOS alphabetically and give a biref comment for most of them. A Alfred2 Atom – An excellent code editor from github Aquaterm – A good term required by gnupot Avast – Mac 下免费的反病毒软件. C Caffeine Chrome – A famous web browser from google CMake D Dropbox – The most famous and the best file E Evernote – A very good notes F Firefox filezilla G github – github 的桌面客户端, 可以更方便地管理代码. GIMP – 堪比 PhotoShop 的非常强大的图像编辑软件. Go2shell GPG Tools gnuplot GPG I iTerm2 J Jabref K Keynote L Libreoffice M MacVim – Vim 的 GUI 版本, 加入了许多功能, 某种程度来说比 vim 更加方便. MacTeX Mathematica – 非常强大的数学软件 MplayerX mpv – 比 mplayerX 更强大的播放器， 可以充分定制 MS Office MWeb – 很好用的 Markdown 编辑器. Music.163 N -Netease.Music P pandoc Papers Pocket – 网页裁剪和笔记收藏软件. Proxifier Parallels Desktop Q QMBF QQ Qt qtgrace 清歌五笔输入法 – 轻量级又非常好用的五笔输入法 R Remember RAR Extractor Racket S Scheme ShadowsocksX SS-Win Simple Comic Skim – 比自带的 Preview 好用多的 PDF 阅读器. Skitch Skype 搜狗输入法 T Telegram Texmacs Texmaker TexShop Thunder TorBrowser Transmission Twitter V Vienna vim – vim 是非常出名的编辑器，被称为 神之编辑器 . VLC VOX W Wechat WeiboX X Xcode Y 有道词典 Command Tools gcc – GNU/Linux 家族默认的编译器. 需要自己编译, 或者通过 homebrew 或者 macports 安装. llvm/clang – apple 支持的 C/C++/Objective-C 编译器. git zsh tmux wget tsocks fontconfig axel aria2 gdb htop sbcl sigc++ automake imagemaick cscope sdl curl octave wxMaxima common library fftw gsl gettext libgif lua nettle node cairo pango qhull sed libxml2 libxml++ boost readline pdflib pixman pcre libpoppler isl hdf5 harfbuzz glib guile gl2ps glibmm fltk mpfr tree ghc swig w3m libunistring mtr libssh libcerf coreutils ctags gtk+ go gnutls openssl document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"macos"},{"title":"小米路由器青春版刷入基于 OpenWRT 的 PandoraBox 固件","url":"/2016/Flashing-Pandora-Based-on-OpenWRT-into-Mi-WiFi-Light/","text":"过年的时候入手了一个小米路由器青春版 ( 残废版), 想着可以刷入 OpenWRT, 欢乐地上网, 结果买回来之后突然发现, 除了十分 mini 外, 完全没有任何卵用. 翻遍了 Google 也没发现怎么刷. 小米这货根本就没有开放 SSH 权限啊!!! 结果就放在那里吃灰… 这两天因为流量超了, 无法上网了, 这日子就真无法过啦… 心血来潮又查了一下 刷机教程, 哎玛, 幸福就那样突然地出现了. 在co2c0(已失效) 的这篇 blog 里简要说了基本过程, 才发现, 原来 OpenWRT 官网已经有了详细的 tutorial. 虽然这样, 还是厚脸皮再来水一篇了. A. 刷入官方开发版 ROM 随机自带的 ROM 是没有 ssh 权限的, 无法上传固件, 自己更新. 还好出了一个 开发版 ROM, 我们的旅程就从这里出发. 下载了 dev 版 rom 后, 然后按照如下 steps 刷入 [参考了￼小米路由器青春版刷机教程], 断掉路由电源, 按住后面的 reset键并连接电源, 按住约 10 秒直至指示灯变为黄色闪烁时松开. 用网线连接电脑与路由的一个 LAN 端口, 手动为电脑设置一个固定 ip, 范围在192.168.31.2 到 192.168.31.253(为啥不是192.168.1.*), 打开浏览器, 访问 路由器管理后台地址192.168.31.1, 会显示 路由器处在恢复模式, 选择下载的 rom 更新系统 等待几分钟后, 指示灯会变成蓝色常量状态, 这时说明刷机成功了. 这样我们就刷好了开发板 ROM 了. 网页登录路由器， 这时需要设置 root 密码，下面会用到。 B. 刷入 基于 OpenWRT 的 Pandora 重新登录192.168.31.1, 登录成功后, 记下地址栏中的 stok 后面的数字, 这个非常重要. 在终端下运行如下命令, curl -d \"oldPwd=&newPwd=\" \"http://192.168.31.1/cgi-bin/luci/;stok=/api/xqsystem/set_name_password\" 比如, curl -d \"oldPwd=old_password&newPwd=new_password\" \"http://192.168.31.1/cgi-bin/luci/;stok=my_stok_num/api/xqsystem/set_name_passsword\" old_password 就是上面设置的密码. 如果返回 code 是 “0”, 说明 已经开启 root 和 ssh, 并且 root 密码已经更新为 \"new_password\". 从 openwrt 下载 Pandora, 并复制到 路由器的 /tmp 目录下, scp PandoraBox-ralink-mt7628-xiaomi-r1cl-squashfs-sysupgrade-r1468-20151001.bin root@192.168.31.1:/tmp/ 登录到路由器, 将固件刷入路由器, mtd -r write /tmp/PandoraBox-ralink-mt7628-xiaomi-r1cl-squashfs-sysupgrade-r1468-20151001.bin firmware 值此我们刷机就结束了. 撒花… 哎, 怎么通过 SS 科学上网呢? ORZ… 下回分解 PS: 有另一种获取 ssh 的方法, 但有些复杂. 难以实施. 就此放弃…(是因为你做不到吧) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux openwrt ss"},{"title":"Open Website as Native Apps in Chrome Under Linux","url":"/2016/Open-Websites-As-Native-Apps-in-Chrome/","text":"I use Linux as my primary system, and native apps are rare under linux, which is very disappointed. Recently, I need to use Skype under Linux, while Skype for linux is outdated. Thanks to chrome, we could use web Skype just like native app in a way, in that chrome provides a method that allows us open a website at a standalone windows like a true program… As a chromer, I am used to letting chrome opened all time, and can easily get notified when new message in. So what should we do now? First, open the target website like Skype, which is the main reason. Then click the menu button on the top-right of chrome, click More tools, and select “Add to desktop”, having Open as window checked on, Then you will find a Linux application launcher named “Skype” on your desktop. You can now double-click the launcher and open Skype at a standalone window like document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux chrome skype"},{"title":"Getting Process Name from PID and Vice Verse","url":"/2016/Obtaining-Process-Name-via-PID-and-Vice-Verse/","text":"Sometimes we need to know the process name via the specific pid, and sometimes vice verse, especially when only allowing one process running, so it is necessary to know how to. Getting Process Name via PID We could get process name easily by the following command ps -o comm= -p PID Getting PID for the Specific Command On the other hand, somtimes we wanna know the pid of specific process, we can do as following pgrep command #or ps -ef|grep -v grep|awk '{print $1}' Allowing Only One Process Runnnig Having only one process running and start a new one if not running pgrep command >/dev/null || command 水水更健康 \"o((&gt;ω&lt;))o\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux bash"},{"title":"Using Git to Deploy Your Codes and Manage Your Site","url":"/2016/Using-Git-to-Deploy-and-Manage-Your-Site/","text":"Since I use hexo as my primary static blog platform, I have tried to using git to deploy my site. And finally, after getting it down, I kept it down for in case. Also there is a option in config.yml to let you deploy your blog to github or your personal server via git. Setup for Our Remote Server There is a directory named hooks in a repository, which does something after receiving or before receiving a push or other operation, and that is what we need. First, we initialize a bare git repository under our home: mkdir -p ~/web/git/amito/{remote,deploy} cd ~/web/git/amito/remote && git init --bare After initializing the repo, we need to something more. Creating a bash file named post-receive under ~/web/git/amito/remote and set executable flag to it. #!/bin/bash deploy_dir=\"/home/amito/web/git/amito/deploy\" GIT_WORK_TREE=$deploy_dir git checkout -f master echo \"DEPLOY: master copied to $deploy_dir\" hexo_dir=\"/home/amito/web/amito/\" cd $hexo_dir ln -sf $deploy_dir/themes $hexo_dir/ ln -sf $deploy_dir/source $hexo_dir/ ln -sf $deploy_dir/scaffolds $hexo_dir/ ln -sf $deploy_dir/_config.yml $hexo_dir/ hexo --silent g if [[$? == 0]]; then echo \"Congratulations! Your blog has been correctly deployed\" else echo \"Unfortunately your blog is not been deployed correctly\" fi Something about the bash file: The deploy_dir contains all codes needed for our hexo project. The hexo_dir is where our real hexo project is. Under Our hexo directory ~/web/amito/, we will initialize a default hexo project, cd ~/web/amito && hexo init && npm install --save rm -rf themes source scaffolds _config.yml After first push, you might delete those ln lines in post-receive. Setup for Our Local Machine On our local machine, we shall create a local repo to deal with our codes and a hexo project for writing. mkdir -p ~/git/amtio/amito && mkdir -p ~/git/amito/hexo ### initialize our git repo cd ~/git/amito/amito/ && git init ### add the remote git repo to our local repo named amito git remote add amito ssh://amito@example.com:/home/amito/web/git/amito/remote/\\ ### initialize our hexo project cd ../hexo && hexo init && npm install --save ### do some linking things mv _conf.yml source themes scaffolds ../amtio/ ln -sf ../amito/_config.yml . ln -sf ../amito/source . ln -sf ../amito/scaffolds . ln -sf ../amito/themes . And it should be very clear for these commands beyond. Then we can push our codes to our remote server, cd ~/git/amito/amito git add . && git commit -m \"first init\" git push amito master If using public key to log in, it will be simple to push our codes. If everything right, you will see something like this: #### Congratulations! Your blog has been correctly deployed And so far, we have done most things. The remaining we need to do is just writing our post and publish it. Cheers! document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"hexo git nodejs"},{"title":"Applying and Installing An SSL Certificate for Nginx","url":"/2016/Apply-and-Install-An-SSL-Certificate-for-Nginx/","text":"Nowadays it is common to have your website encrypted, and then it is neccessary to applying a SSL Certificate for your site. Here is the general procedure to applying and install a SSL Certificate. A. Apply A SSL Certificate A.0 Prerequisites First of all, you need to have openssl installed on your system. You can easily install it via package manager like apt on ubuntu or yum on centos if not installed. ### ubunt or debian sudo apt install openssl openssl-dev ### fedora or centos sudo yum install openssl openssl-devel A.1 Generate the RSA key mkdir tmp && cd tmp openssl genrsa -des3 -out domain.tld.key 4096 A.2 Create a CSR openssl req -new -sha256 -key domain.tld.key -out domain.tld.csr You need to provide the following information: Common Name: www.domain.tld for single domain and *.domain.tld for a wildcard certificate Organization: The exact legal name of your company or organization. domain.tld will be fine City or Locality: the city where you are State or Province: the state or province you stay in. Contry: the two-letter ISO abbreviation for your country. In the end before generating your csr, you will be ask to enter the challenge password, leaving it blank by just pressing enter. A.3 Verify your CSR Before submitting your CSR to your ssl certificate provider, you might have to verify your CSR just in case any error accuring. openssl req -noout -text -in domain.tld.csr A.4 Submit Your CSR If no error when verifying the CSR, you can now submit it to your certificate authority. You should have the admin@domain.tld mail address accessible to receive the approval email. B. Install your SSL Certificate After get your SSL Certificate, you can then deploy it on your web server. You might need to decrypt your private key for following installation: openssl rsa -in domain.tld.key -out domain.tld.decrypted.key In the following, you need your decrypted privated key, and you should keep it away from others. B.1 Nginx server_tokens off; ## ssl config ssl_certificate /path/to/your/certificate; ssl_certificate_key /path/to/your/certificate/key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ssl_protocols TLSv1.2; ssl_prefer_server ciphers on; ssl_ciphers \"EECDH+ECDSA+AESGCM EECDH+aRSA+AESGCMEECDH+ECDSA+SHA384 EECDH+ECDSA+SHA256 EECDH+aRSA+SHA384 EECDH+aRSA+RC4 EECDH !RC4 !aNULL !eNULL !LOW !3DES !MD5 !EXP !PSK !ERP !DSS !DH !EDH\"; And add the following to your server block after listen 80;: listen 443 ssl http2; listen [::]:443 ssl http2; B.2 Apache document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"ssl nginx"},{"title":"Proxychains-ng -- Another One Proxy Tool for Terminal","url":"/2016/Proxychains-ng-Another-Proxy-for-Terminal/","text":"在前面的博文中介绍了 tsocks–一个终端代理工具。 此外还有其他的非常好用的代理工具，比如这次介绍的 proxychains-ng（感谢Luxing Huang 同学)， 同样的，支持 mac，linux. proxychains-ng 安装非常方便， 下载源码包， configure, make, and sudo make install 即可。 把 src/proxychains.conf 拷贝到 /etc/，按照自己的代理修改即可。 主要是类似于如下的代理设置： socks5 127.0.0.1 1080 如果嫌命令太长，可以加一个alias: alias pc='proxychains4' 如果用的是 mac+iTerm， 可以在 iTerm 中新建一个快捷键： Preferences-&gt;Profiles-&gt;Key, 新建一个快捷键，例如 ⌥+↩︎ ，Action 选择 Send Hex Code，键值为 0x1 0x70 0x63 0x20 0xd，保存生效[1]. 这样既可以直接敲命令，又可以保留补全。 PS: OSX 10.11 启用了 SIP (System Integrity Protection), 系统自带的命令行程序无法利用代理, 需要关闭 SIP 才可以使用. 具体关闭方法可以参考 这里. 水水更健康 O(∩_∩)O 1.http://yanghui.name/blog/2015/07/19/make-all-command-through-proxy/ ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"proxy socks5"},{"title":"Using Papers to Manage Your Docs on OSX","url":"/2016/Using-Papers-to-Manage-Your-Docs-on-OSX/","text":"你是否在为浩如烟海的文档管理而烦恼? 你是否在为写 paper 时引用文献头痛不已? 你是否在为想看的 papers 一时不知道放在哪里而发狂? 来吧, 欢迎投入 Papers 的怀抱…(≧▽≦)/ 啦啦啦 Papers 的初次相识 Papers 官网上是这样来描述自己的: ![papers](http://www.papersapp.com/img/v2/p.png) \"你的个人科研图书馆\" Papers helps you collect and curate the research material that you are passionate about. From citations to search, Papers will improve the way you find, organize, read, cite and share. Papers 的温柔体贴 可浏览、批注、评级文献 在 papers 内浏览文献, 对文献进行批注, 这算是各软件必备的了. 与各主流浏览器无缝相接 papers 有各种浏览器的插件, 可以把文献在 papers 打开导入. 不用手动导入引用信息了. 多方便. 内置搜索 arxiv 等数据库 papers 内置了一个在线文献搜索器, 可以搜索许多数据库, 当然需多管理软件都有这个功能. 自动搜索匹配已下载文献 导入已下载的文献后, 你可以在线匹配文献信息, 不用自己挨个设置. 这个非常方便. 不过重复的文献有可能识别不出来. 配合 dropbox 同步文献库 papers 可与 dropbox 结合, 同步你的文档库, 这点在国内并没有多少卵用. 支持导出数据库为 bibtex 通常我们使用 latex 来写 papers, 用 bibtex 管理引用文献, 而 papers 支持导出文献库为 bibtex, 真是绝配. Papers 的贵气傲娇 有点贵… 学生版也要 49 刀… 水水更健康 (≧▽≦)/ 啦啦啦 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"macos papers"},{"title":"tsocks -- A Simple Socks5 Proxy under Terminal","url":"/2016/tsocks-A-Simple-Socks5-Proxy-under-Terminals/","text":"有时候, 我们需要一些代理来穿过 GWF 访问一些网站, 我们可以使用一些公共代理, 但是公共代理容易产生各种安全问题. 而如果你有一台 VPS, 可以自己建立一个简单的 socks5 代理. 我们可以通过 SSH 创建一个 Socks5 代理： ssh -Nf [-6] -D localport user@address] 然后设置浏览器使用localhost:localport 的 socks5 代理即可. 但是终端下使用代理就有点困难. tsocks 就是一款可以让命令行程序使用 socks5 代理的软件. tsocks toscks可以应用程序不经过任何修改就可以轻松地使用已有 socks 代理, , 已经很久不更新了, 但还是可以很好地满足我们的要求的, 而且是 mac, linux 皆可用的. 安装 从 这里 下载最新的 tsocks-1.8beta5.tar.gz. 按照一般的安装过程安装即可. tar xf tsocks-1.8beta5.tar.gz cd tsocks-1.8 ./configure --prefix=/usr/local --enable-socksdns --disable-hostnames make sudo make install 配置 tsocks 的配置文件是/etc/tsocks.conf, 一般的配置如下: ### local address local = 192.168.0.0/255.255.255.0 local = 10.0.0.0/255.0.0.0 local = 127.0.0.1/255.255.255.0 ### server config server = 127.0.0.1 server_type = 5 server_port = localport 使用 只要在命令前面加上 tsocks 就可以了. 例如: tsocks wget https://drkshell.me 如果觉得不方便, 可以启用全局代理模式: tsocks on 然后就可以像平常一样运行命令了, 不过现在是走的 socks5 代理. 关闭全局代理模式: tsocks off 很简单吧.o(*￣▽￣*)ゞ 备注 tsocks 很久不更新了, 有时候不一定能满足我们的需求, 而作为它的替代者 - torsocks – Tor 计划的一部分, 正在不断地开发着. 有需要可以使用更新的 torsocks 吧.(不过暂时没有使用过 Σ(っ °Д °;)っ ). 另外看到socat, 不知体验如何… 问题 这里罗列了一些可能遇到的问题及解决方法. (待补充 (“▔□▔”))… 水水更健康 (*￣︶￣)y document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"proxy socks5"},{"title":"Install and Configure SS under Centos 7","url":"/2016/Install-and-Configure-SS-under-Centos-7/","text":"SS 是一款轻量级的 socks5 代理软件, 如果你有一台 VPS, 可以自己搭建 SS, 如果没有 vps, 可以通过我的推荐在 DigitalOcean 或者 Vultr 上购买一个 VPS. 下面简单介绍如何在 centos 7 下部署 SS. SS 有好多版本, 一般的就是 python 版的, 安装也比较方便, 还有 go 和 erlang 版本的, c 版本的优化比较好. A. Install SS 安装 Python 或 Nodejs 版本:: ##python version sudo pip install shadowsocks ##nodejs version sudo npm install -g shadowsocks 安装 C 版本: ##c with libev version. sudo wget https://copr.fedoraproject.org/coprs/librehat/shadowsocks/repo/epel-7/librehat-shadowsocks-epel-7.repo -o /etc/yum.repo.d/librehat-shadowsocks-epel-7.repo sudo yum update and sudo yum install shadowsocks-libev B. Create a Shadowsocks Config cat < EOF > config.json { \"server\": \"server_address\", \"server_port\": server_port, \"local_address\": \"127.0.0.1\", \"local_port\": local_port, \"password\":\"password\", \"method\": \"aes-256-cfb\", \"timeout\": 300, \"fast_open\": true, \"workers\":1 } EOF > /etc/sysctl.conf C. Configure firewalld If enabled firewalld, you need to add the following rule to open server port: sudo firewall-cmd --permanet --add-port=server_port/tcp sudo firewall-cmd --reload D. Create a Startup Service shadowsocks-libev 自带了了一个 shadowsocks-libev.service, 如果是 python 版的, 需要自己建立一个: ## under root cat < EOF > /usr/lib/systemd/shadowsocks.service [Unit] Description=ShadowSocks service After=syslog.target network.target auditd.service [Service] Type=simple User=nobody ExecStart=/usr/local/bin/ss-server -c /etc/shadowsocks/config.json ExecReload=/bin/kill -HUP $MAINPID ExecStop=/bin/kill -s QUIT $MAINPID PrivateTmp=true Restart=on-abort [Install] WantedBy=multi-user.target EOF systemctl enable shadowsocks[-libev] systemctl start shadowsocks[-libev] { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"centos ss proxy"},{"title":"Fix Incomplete Download of Torrents on Btrfs","url":"/2016/Fix-Incomplete-Download-of-Torrents-on-Btrfs/","text":"BTRFS 是一种非常先进的支持写入时复制（COW）的文件系统, 支持 可写快照 , 内建磁盘阵列 , 透明压缩 等许多高级特性, 其写时复制特性在修复文件数据方面比传统就地修改的文件系统有着极大的优势. 但是, 就是由于 写时复制 这一特性 [1], 导致了在 BTRFS(和其他的 COW 文件系统) 下, torrent 在下载时出现的明明已经下载完了, 但是当重新打开下载软件或者检查文件完整性时, 发现文件总是下载不全的问题(具体细节不清楚). 曾经在使用 opensuse 的时候就出现了这一问题, 当初还以为是 openSUSE 的原因…知道了问题所在, 就可以解决问题了. 1. 预分配空间 在通常的 torrent 下载软件中, 可以设置 pre-allocation, 预先分配整个文件大小, 如 transmission 就是在 配置文件中加入 \"preallocation\":2. 这可以减少文件碎片化, 但是对问题影响不大. 2. 禁用下载目录的 COW 更有效的方法是, 禁用下载目录 (通常是 home 下的 Dowloads文件夹) 的 写时复制功能, 让 torrent 就像在通常的文件系统中下载一样. chattr -R +C /home/user/Downloads 需要注意的是[2], Downloads 需要是空目录, 这样才能保证所有写入的文件的完整性, 如果当前有文件, 不能保证当前文件的稳定性. 所以最好就是预先清空整个目录. 想当初因为这个而弃用了 openSUSE, 无知者不怪… 1.http://ram.kossboss.com/torrenting-cow-filesystems-btrfs-zfs/ ↩2.https://wiki.archlinux.org/index.php/Btrfs_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"btrfs"},{"title":"Making an Installation USB Drive of OSX","url":"/2016/Making-an-Installation-USB-Drive-of-OSX/","text":"人在河边走, 哪能不湿鞋? 系统安装 U 盘几乎是每个程序猿必备的, 即使是 OS X 这样 \" 非常稳定 \" 的系统, 难保哪一天作死就搞挂了, 除了系统备份外, 制作一个系统安装 U 盘留以备用也是十分必要的. (血的教训) 制作安装 U 盘非常简单. 首先, 你要从 MAS 下载 OSX 安装镜像, 比如最新的 El Captain. 然后找一个 至少 8G 大小的 U 盘或者 TF 卡, 格式化成 OSX 默认的文件系统格式, 命名为 osx (这个随意) Open Terminal(or iTerm2) and run the following commands: sudo /Applications/Install\\ OS\\ X\\ El\\ Capitan.app/Contents/Resources/createinstallmedia --force --volume /Volumes/osx\\ --applicationpath \"/Applications/Install OS X El Capitan.app\" --nointeraction 等待它写入完即可… 这是比较方便的方法…也可以使用 Disk Utility 来制作, 比较繁琐, 就不介绍啦. \" 所以, 这就是你又水了一贴的理由??? ￣へ￣. \" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"macos"},{"title":"Forbidding Connections for Specific IPs in SS","url":"/2016/Forbidding-Connections-from-Specific-IPs-in-SS/","text":"ss 在较新的版本中, 有一个选项 forbidden-ip, 可以指定那些 ip 是无法被访问的, 例如 192.168.0.1, ::1(默认值), 可以在命令行选项中设定如下: ssserver -c config.json --pid-file config.pid --log-file config.log --forbidden-ip=\"192.168.0.1\" -d start 或者在 config.json 里设定, 加上如下选项: \"forbidden_ip\":[\"192.168.0.1\",\"::1\"] 这样就无法访问 server 端的本地地址了. 默认值 192.168.0.1, ::1 是为了安全考虑, 防止外部访问本地资源. 但是如果你的 server 还搭建了其他服务如有一个网站, 挂了 ss 是无法访问这个网站的. 所以可以将其留空, 即可正常访问你的网站了. 水水更健康 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"python ss"},{"title":"Using Hexo to Build Your Personal Static Blog","url":"/2016/Using-Hexo-to-Build-Your-Personal-Static-Blog/","text":"hexo is a static blog platform based on Nodejs by tommy351, using Markdown (or other engine) to parse post, and the most exciting point is its fast enough as to parse hundreds of posts with seconds. After using ghost for several months, I decided to move my blog from ghost to hexo. Before installing hexo, one needs to install nodejs and npm, which could be downloaded and installed from the official site nodejs. And be sure that git is installed as well sudo npm install -g hexo-cli hexo-server Getting Started With Hexo Hexo is easy to use, and most commands have abbreviation, like: hexo generate is equivalent to hexo g In the following, we use the convention to include full command ant its abbr. hexo g[enerate] Init a project hexo init [folder] cd [folder] && nam install If no [folder], hexo will initialize the current directory. Generate Static Web Pages hexo g[enerate] Create New Post/Page hexo n[ew] \"title\" hexo n[ew] page \"title\" Deploy Your Blog hexo d[eploy] Here one needs to have the project properly configured. Publish Your Draft hexo p[ublish] 'draft' post/page Clean Existed Files hexo c[lean] Run Hexo Server Run a hexo server to publish your site: hexo s[erver] Render Your Blog hexo render Other Commands hexo help hexo version Composite Command Several commands can be combined, hexo d[eploy] -g # create and deploy hexo s[erver] -g # create and preview Developing Your Hexo Basic Configuration Noting that there is a space after the colon followed by each option, like ### right configuration aaa: bbb ### wrong configuration aaa:bbb Some basic options are followed below, title: dark subtitle: darkshell description: a simple static blog author: dark language: en url: http://yoursite.com Deploy Config One can easily deploy hexo project to github pages via git, or your personal server. type: git repository: git@github.com/darkshell/darkshell.io.git branch: master Tuning Your Hexo Themes hexo project 上有许多非常棒的主题, 下面是个人觉得很棒的主题. Alberta Casper – The Ghost Casper theme ported to Hexo. Transparent – A transparent theme based on Light. Vno – The Ghost Vno theme ported to Hexo. Tranquilpeark – a good hexo theme Plugins http://hexo.io/plugins 上有很多插件, 可以按需安装. npm install hexo-render-ejs --save npm install hexo-render-stylus --save npm install hexo-render-marked --save 水水更健康 O(∩_∩)O~ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"hexo nodejs"},{"title":"Using GPG to Protect Your Pirvacy and Information","url":"/2016/Using-GPG-to-Protect-Your-Pirvacy-And-Information/","text":"GPG or GnuPG 是目前最流行、最好用的加密工具之一。是由 GNU 出品的 PGP 的开源实现版本. 广泛应用于加密、数字签章及产生非对称匙对等. Why Use GPG? 自从登登同学勇敢举报了美帝大范围窃听民众通讯之后, 越来越多的人开始注意自己的个人隐私 (o(▽)o (好~~). 其推荐的 tails 也是非常不错. 用 GPG 来加密往来邮件, 加密文件, 以防 帝国 美帝 窃听个人隐私…(\"▔□▔).(万一你写给女盆友的 亲密的 情书被偷看了, 嗯… 如何获取 GPG 呢? Under Linux 在 fedora/SL/Centos 下: sudo dnf install -y gnupg2 #or sudo yum install -y gnupg2 debian/ubuntu: sudo apt-get install gnupg2 #or sudo apt install gnupg2 opensuse: sudo zypper in gnupg2 Under Mac mac 下的推荐GPGTools, 直接下载安装就好了. 还有一个macgpg, 现在不推荐使用了. 源码安装 如果想自己编译, 可以下载源码编译安装. 怎么使用 GPG 呢? 生成个人密钥 在终端下运行如下命令 gpg --gen-key 会看到如下的输出: gpg (GnuPG) 1.4.19; Copyright (C) 2015 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. gpg: keyring '/home/dark/.gnupg/secring.gpg' created gpg: keyring '/home/dark/.gnupg/pubring.gpg' created Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) Your selection? 一般选择 1 就好, 接下来就是密钥的长度, 过期时效, 名字, 邮箱, 注释, 最后输入密码(一定要记住). 最后的最后确认一遍, 然后随便做点什么小动作给他点熵值. 几分钟后你会看到类似如下的结果: key 577F4A22 marked as ultimately trusted public and secret key created and signed. gpg: checking the trustdb gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model gpg: depth: 0 valid: 1 signed: 0 trust: 0-, 0q, 0n, 0m, 0f, 1u pub 4096R/577F4A22 2015-07-01 Key fingerprint = A573 2A0D 7237 5738 3A41 6A0D 2565 7C7D 577F 4A22 uid Yujiang Bi (Created at 2015.07.01. Old keys are not used.) sub 4096R/56AE325C 2015-07-01 其中, uid就是 用户 ID, 577F4A22 也可以用来当作 用户 ID, 以下用 UID 代替. 接着生成一张 \" 撤销证书 \", 用来将来密钥作废时，请求外部的公钥服务器撤销你的公钥 gpg --output revoke.asc --gen-revoke UID 同样地, 确认, 原因, 注释, 密码, revoke.asc 就是撤销证书, 好好保存. 不要让别人偷了去了. 管理个人密钥 列出密钥 gpg --list-keys 这会列出系统已有密钥, 如下: /home/dark/.gnupg/pubring.gpg ----------------------------- pub 4096R/577F4A22 2015-07-01 uid Yujiang Bi (Created at 2015.07.01. Old keys are not used.) sub 4096R/56AE325C 2015-07-01 第一行 pubring.gpg- 显示公钥文件名，第三行显示公钥特征（4096 位，Hash 字符串和生成时间），第四行显示 \" 用户 ID\"，第五行显示私钥特征。 删除密钥 列出密钥后, 可以删除不用的密钥. gpg --delete-key UID 这样就删除了 UID 这个密钥 输出密钥 公钥文件（.gnupg/pubring.gpg）以二进制形式储存, 可以这样导出到 ASCII 文本中: gpg --armor --output public.txt --export UID 导出密钥到 ASCII 文本中: gpg --armor --output private.txt --export-secret-keys UID 上传密钥 公钥服务器是网络上专门储存用户公钥的服务器, 上传到公钥服务器可以让别人导入你的公钥, 进而向你发送加密文件或邮件. gpg --send-keys UID --keyserver hkp://subkeys.pgp.net 这样, 你的公钥就上传到了公钥服务器上了, 通过交换机制，所有的公钥服务器最终都会包含你的公钥。由于公钥服务器不会检查上传人的身份, 故需要你将你的公钥指纹发布到网上, 确保真实性, 让其他下载你公钥的人可以检验是否为真. 用下面的命令输出你的公钥指纹. gpg --fingerprint UID 我的公钥指纹是 pub 4096R/577F4A22 2015-07-01 Key fingerprint = A573 2A0D 7237 5738 3A41 6A0D 2565 7C7D 577F 4A22 导入密钥 同样的, 你可以从公钥服务器上导入别人的公钥: gpg --keyserver keys.gnupg.net --search-key UID --search-key 参数可以是邮箱, 或者 uid的名字. 如果准确知道导入公钥的 uid, 可以直接导入 gpg --keyserver keys.gnupg.net --rec-key UID 或者让别人直接发给你, 然后导入 gpg --import [key-file] 确定公钥的真实性后, 可以对其签收: gpg --sign-key UID 同样上面也能导入其他的自己的秘钥. 文件加密解密 文件加密 假定加密文件为 file, 加密后的文件为 en-file: gpg --recipient UID --output en-file --encrypt file UID是接收人的秘钥UID. 文件解密 收到别人给自己的加密文件 file, 可以解密到 de-file gpg --output de-file -d file 不加--output 会直接输出到终端 文件数字签名 对文件签名 有时候我们需要对文件签名，表示这个文件确实是本人发出的, 若有多个秘钥, 则需要加上 -u(--local-user) uid gpg [-u UID] --sign file 之后会生成一个file.gpg 的二进制文件, 用--cleansign 代替--sign 会生成 ASCII 文件 file.asc 可以将签名写进另一个文件, 高度推荐这种用法，尤其是对二进制文件（如文档）签名的时候。另外， --armor 选项在这儿也非常有用, 会生成 ASCII 码形式的签名文件, 不加则是二进制的. gpg [-u UID] --armor --detach-sign file #or gpg -a -b [-u UID] file 对文件签名验证 收到别人签名后的文件, 需要用对方的公钥验证签名是否为真, 如果没有其公钥, 需要先导入. gpg --verify file.asc 如果是签名和文件合在一起, 可以使用如下命令提取原始文件: gpg --output file.orig -d file.asc 签名并加密 有时候我们需要签名的时候进行加密, 可以这样做: gpg [-u Sender] [-r Recipient] [--armor] --sign [--clearsign] --encrypt [Data] Sender 是自己的 UID, Recipient 是对方的 UID. 验证签名并解密 如果数据既签名又加密, 则验证签名过程是在解密中进行的 gpg --output de-file -d file 邮件加密解密 … Absent Now… 水水更健康 \"o((&gt;ω&lt;))o\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux macos gpg"},{"title":"Multiplot in Gnuplot","url":"/2015/Multiplot-in-Gnuplot/","text":"gnuplot 是一种非常好用的, 功能丰富的, 专业的科学绘图软件, 用途十分广泛. gnuplotting 有一些 examples 与 tips. 有时候, 我们需要在一幅图中分开画几个图, 例如下面这个: 如何实现呢, 这就是本文的主题 – multiplot. 等分绘制多图 将画布平均分成四等分, 每一部分照常绘图即可. 进入 gnuplot gnuplot> set term aqua size 8,4 solid gnuplot> set multiplot layout 2,2 multiplot> plot sin(x) t \"sin(x)\" multiplot> plot cos(x) t \"cos(x)\" multiplot> plot tan(x) t \"tan(x)\" multiplot> plot erf(x) t \"erf(x)\" multiplot> unset multiplot 最主要的就是 set multiplot layout 2,2, 即将其分成上下左右四分, 若要左右各一个图, 只需要将 layout 2,2 改成 layout 2,1 即可. 自由绘图–大小图 有时候, 我们还想在一幅图中画一个小图, 可以用来放大原图中的某一部分, 即大小图. 代码如下: set term epslatex standalone color solid size 8,4 set output \"demo.tex\" set multiplot set grid set title \"A smaller plot in a big plot\" set size 1,1 set origin 0,0 set xrange [-10:10] set yrange [-0.8:1.5] #plot the square set arrow from -0.3,0.5 to 0.,0.7 lw 1 back filled set arrow from -1,0.6 to 1,0.6 lw 1 front nohead set arrow from -1,-0.6 to 1,-0.6 lw 1 front nohead set arrow from -1,-0.6 to -1,0.6 lw 1 front nohead set arrow from 1,-0.6 to 1,0.6 lw 1 front nohead plot cos(x)*0.5 w l lt 3 t \"cos(x)*0.5\" unset arrow unset title set size 0.35,0.35 set origin 0.4,0.55 set xrange [-1:1] set yrange [-0.6:0.6] plot cos(x)*0.5 w l lt 6 t \"cos(x)*0.5\" unset multiplot set output !xelatex demo.tex 生成的 pdf 文件如下所示: 水水更健康 (≧▽≦)/ 啦啦啦 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"gnuplot tex"},{"title":"Fix VMware 12 not Working Under Fedora 23 Problem","url":"/2015/Fix-VMware-12-not-Working-Under-Fedora-23-Problem/","text":"Fedora 23 刚刚发布，冬日の骑士 就抛弃了 Fedora 22，升级到了 23。 不得不说 Fedora 23 还是蛮不错的，新人新装，让人欢喜。但是让人不爽的是 VMware 12 却不能正常启动， 一点错误 log 也没有，着实蛋疼。 本着自食其力，丰衣足食的精神，google 了一下，结果只有两条，报告了这个错误。其中 第二条 终于有人给出了答案。看来是 Fedora 更改了一些设定导致的这个问题。 那么怎么改正呢？ 进入到/usr/lib/vmware/lib/, sudo su - cd /usr/lib/vmware/lib for i in $(ls /usr/lib64/*4600*); do mkdir -p $(basename $i .4600.2); /bin/cp -afv $i $(basename $i .4600.2)/$(basename $i .4600.2); done 估计原因是 fedora 23 的 glib 运行库与 VMware 自带的不兼容。所以就把 VMware 自带的 glib 库换成 Fedora 自带的。这样就可以了… 如果仍旧不能启动, 按照如下方式从命令行启动vmware 或 vmplayer： VMWARE_USE_SHIPPED_LIBS=force vmware #or VMWARE_USE_SHIPPED_LIBS=force vmplayer 这一句加到 /usr/bin/vmware 和 /usr/bin/vmplayer 开始： export VMWARE_USE_SHIPPED_LIBS=force 像这样 ... ... set -e export VMWARE_USE_SHIPPED_LIBS=force ETCDIR=/etc/vmware ... ... 这样就可以正常启动 VMware 了。 水水更健康 O(∩_∩)O~ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"fedora vmware"},{"title":"安装使用 Arch Linux","url":"/2015/Installing-and-Using-Arch-Linux/","text":"Arch Linux 是一款轻量简单的 linux 发行版, 其秉承简洁、优雅、正确和代码最小化的设计宗旨。简洁、可定制度高：对于有系统洁癖的人，或者控制欲极强的人，Arch 绝对是首选. Arch linux 有着丰富详尽的 wiki, 指导新手如何安装, 但是初次接触, 总是有些地方不明所以. 故记录下在 vmare 中的安装过程, 以备所需. 安装 Arch Linux 制作安装媒介 从 Arch 镜像下载最新的安装 LiveCD， 并刻录到 U 盘, 在虚拟机中则不需要. dd if=/path/to/arch-livecd.iso of=/dev/sdb bs=1M && sync /dev/sdb 是 U 盘 盘符. 完成后, 可以重新启动电脑从 U 盘启动安装. 或者使用 grub2 引导 arch livecd： menuentry \"Archlinux x86_64\" { insmod part_msdos insmod part_gpt insmod ext2 set root=(hd0,gpt1) set isofile=/boot/archlinux-2019.01.01-x86_64.iso set label='ARCH_201901' set imgdevpath=/dev/sda1 loopback loop $isofile linux (loop)/arch/boot/x86_64/vmlinuz archisolabel=$label img_dev=$imgdevpath img_loop=$isofile earlymodules=loop initrd (loop)/arch/boot/intel_ucode.img (loop)/arch/boot/amd_ucode.img (loop)/arch/boot/x86_64/archiso.img boot } 新建虚拟机 我是在虚拟机中安装 arch 的，故先新建一个虚拟机 Arch.vmx, 启动安装. 如果想用 UEFI 模式, 则需要修改 Arch.vmx, 原始文件开始大致如下: #!/usr/bin/vmware .encoding = \"UTF-8\" config.version = \"8\" 在 encoding 后加入 firmware=\"efi\" 最后大概是这个样子: #!/usr/bin/vmware .encoding = \"UTF-8\" firmware=\"efi\" config.version = \"8\" 这样就启用了 UEFI 模式. 硬盘分区 UEFI 模式 需要将硬盘设成 GPT 分区表, 也是 UEFI 所要求的. cfdisk /dev/sda 会提示你将硬盘设成 GPT 模式, 按照自己喜好, 划分硬盘分区. 注意, 需要划分出 efi 分区, 格式化成 Fat32 或 Fat16 格式. 如第一分区为 efi 分区 mkfs.vfat -F32 /dev/sda1 习惯分为四个分区, /dev/sda1 - boot: ~400M /dev/sda2 - root: ~20G /dev/sda3 - swap: ~4G /dev/sda4 - home: remainings. BIOS 模式 和平常的硬盘分区一样. 习惯也是 4 个分区. 挂载分区. mount -t ext4 /dev/sda2 /mnt mkdir -p /mnt/boot /mnt/home mount -t ext4 /dev/sda1 /mnt/boot mount -t ext4 /dev/sda4 /mnt/home swapon /dev/sda3 网络连接 livecd 在启动时会开启 dhcpd 服务, 如果是 nat 连接方式, 这样会自动获得 ip. 如果是桥接模式, 则可以获取到 ipv6(如果有的话), 需要自己指定 DNS: echo nameserver 2001:4860:4860::8888 >> /etc/resolv.conf 然后选择合适的 mirror: vi /etc/pacman.d/mirrorlist 在真机中, 可以直接获得 ipv6 地址, 自己修改 dns 即可. 如果是静态 ip 或者是 WiFi, 参考 官方文档 安装系统 使用 pacstrap 脚本安装 base 和base-devel 等: pacstrap /mnt/ base base-devel 要安装 gnome 等，加上 gnome xorg xorg-drivers 等等 $ pacstrap /mnt base base-devel gnome xorg xorg-drivers texlive-most qt5 xorg-apps gnome-extra 配置系统 生成 fstab 文件: genfstab -p /mnt/ > /mnt/etc/fstab chroot 进入新系统: arch-chroot /mnt 设置主机名: echo Dark>/etc/hostname 设置时区: ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 设置 locale: vi /etc/locale.gen 取消所需 locale 的注释, 然后生成 locales: locale-gen echo LANG=en_US.UTF-8 > /etc/local.conf 终端字体与键盘: echo -e \"KEYMAP=us\\nFONT=Lat2-Terminus16\" >/etc/vconsole.conf 生成内核: mkinitcpio -p linux 用户与密码: 修改 root 密码: passwd 新建普通用户: useradd -G wheel -s /bin/bash -d /home/dark -m -U dark passwd dark 开机启动 dhcpd: systemctl enable dhcpd.service 安装 bootloader UEFI 模式 看一下有没有/sys/firmware/efi/efivars, 如果没有则需要激活 efi 模块: modprobe efivarfs pacman -S grub os-prober efibootmgr grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=arch_grub --recheck grub-mkconfig -o /boot/grub/grub.cfg BIOS 模式 pacman -S grub os-prober grub-install --force --boot-directory=/boot --target=i386-pc --recheck /dev/sda grub-mkconfig -o /boot/grub/grub.cfg 安装其他软件包 pacman -Sy vim bash-completion net-tools sudo 卸载分区 退出 chroot 环境, 并卸载分区, 重启进入新系统: exit umount -R /mnt reboot 安装 GUI 安装 xorg 在 系统安装时，加上 xorg $ pacstrap /mnt base base-devel xorg gnome Input and mouse not xf86-input-vmmouse sudo pacman -Sy xf86-input-libinput xf86-input-mouse xf86-input-synaptics 显卡驱动: 在 vmware 中, 需要安装xf86-video-vmware,mesa,xorg-twm, xorg-xclock, xterm, dbus: sudo pacman -Sy xf86-video-vmware mesa xorg-twm xorg-xclock xterm sudo pacman -Sy dbus svga-dri 查看系统显卡: lspci|grep VGA AMD/ATI 的显卡可以安装开源的 xf86-video-ati 或者闭源的catalyst.Nvidia 的显卡可以安装闭源的nvidia,nvidia-340xx 或nvidia-304xx或开源的xf86-video-nouveau. Intel 的显卡可以安装 xf86-video-intel. 声卡驱动 声卡一般不需要额外装驱动，只需要安装alas-utils: sudo pacman -Sy alas-utils vmware-tools 如果是 vmware 中, 需要安装 vmware-tools, 在 vmware 中选择 安装 VMware Tools: pacman -S linux-headers for x in {0..6}; do mkdir -pv /etc/init.d/rc$x.d; done mount /dev/cdrom /mnt cd /root tar zxf /mnt/VMwareTools*.tar.gz cd vmware-tools-distrib ./vmware-install.pl 安装字体[1] sudo pacman -Sy ttf-dejavu wqy-zenhei wqy-microhei 安装 gnome sudo pacman -Sy gnome gdm sudo pacman -Sy gnome-extra 启用 gdm 和 Network-manager: sudo systemctl enable gdm.service sudo systemctl enable NetworkManager.service 安装 plasma5(kde5) 在安装时加上 `plasma kdebase kdegraphics kde-applications kdeutils kdenetwork qt5 sudo pacman -Sy plasma-workspace plasma-desktop plasma-framework plasma-meta plasma-workspace-wallpapers sddm sudo systemctl enable sddm.service 重启完成安装 重新启动一般就可以进入桌面环境了…o(*≧▽≦)ツ n(≧▽≦)n 开始使用 Arch Linux Problems mouse not working. install xf86-input-libinput instead of xf86-input-vmmouse. Gnome-Terminal not working: running localectl set-locale LANG=\"en_US.UTF-8\" may work. 水水更健康 ╮(￣▽￣\"\")╭ 1.部分参考了Arch Linux 安装指引 ↩ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux arch"},{"title":"An Simple Introduction to Grub2","url":"/2015/An-Simple-Introduction-to-Grub2/","text":"GNU GRUB（GRand Unified Bootloader）是一个来自 GNU 项目的启动引导程序。GRUB 是多启动规范的实现，它允许用户可以在计算机内同时拥有多个操作系统，并在计算机启动时选择希望运行的操作系统。目前绝大部分现代 linux 发行版都采用 grub 来引导系统，并且 windows 和 mac 也可以用 grub 来引导。这使得用 grub 引导多系统非常方便。 grub 安装指南 GRUB 的一个重要的特性是安装它不需依附一个操作系统；但是，这种安装需要一个 Linux 副本，如果没有 linux 环境, 可以安装一个 linux 虚拟机来安装 grub. 当前在 Windows 下有一个实验性版本可以使用。 GRUB 实质上是一个微型系统，通过链式启动的方式，它可以启动所有安装的主流操作系统。除了安装到硬盘外，GRUB 也可安装到光盘、软盘和闪存盘等移动介质中，这样就可以带起一台无法从硬盘启动的系统。 安装到硬盘 这个一般不需要手动安装，在安装系统时，grub 会自动安装到硬盘上，如果需要手动安装，可参考下面安装到 u 盘的方法。 安装到 U 盘 制作一个可启动优盘会带在身边有时候会很方便，grub 是非常好的选择。 一般我们使用的就是 i386-pc 和 x86_64-efi 两种模式。 i386-pc platform 如果使用 ubuntu，一般是安装的这个模式。 可以在传统 bios 模式下工作。 x86_64-efi platform 如果使用 fedora 或 opensuse， 一般是 x86_64-efi 模式， 可以在 uefi 模式下工作。 生成引导文件： 将其放到 u 盘 的 /EFI/BOOT/ 下面，替换掉原来的 BOOTX64.EFI（做好备份）. 当然 fedora/opensuse 下也可以安装 i386-pc platform，只是相应地要改一下命令。安装到硬盘的话，只需将–removable 去掉就好了。 当然你需要一台已经装了 linux 的电脑，windows 下暂时没有试过官方提供的 grub for windows。 grub 引导 iso 一般的 linux 发行版都会有 livecd, 使用 grub 可以轻松引导 iso 并进行体验或者安装. 或者可以使用 syslinux 中的 引导 winpe 等 iso, memdisk 可以从这里 下载 编译. 使用方法按照下面的配置就可以. grub 一般配置 下面是一份 GPT/UEFI 分区的 USB 上 grub 的通用配置，用来做引导 u 盘， MBR 模式的可以相应修改， 通常存放在 /EFI/grub2/grug.cfg。 ### grub configuration ### Author: Darkshell ### Email: byujiang@163.com ### Home: https://www.darkshell.me ### Basic Setting ### ### Partition USB to two parts，and uuid1 and uuid2 are uuids of two partitions. set uuid1=1F83-3F65 set uuid2=1F83-3F65 search --no-floppy --fs-uuid --set root $uuid2 set default=0 set timeout=5 nsmod terminal set font=/efi/grub2/fonts/unicode.pf2 loadfont $font set gfxmode=auto insmod gfxterm insmod gfxmenu insmod gfxterm_menu # terminal_input gfxterm # terminal gfxterm terminal_output gfxterm insmod fat insmod ext2 insmod part_gpt insmod efi_gop insmod iso9660 insmod chain insmod linux insmod exfat insmod linuxefi insmod echo insmod configfile insmod boot insmod search_label insmod search_fs_file insmod search insmod search_fs_uuid insmod loadbios insmod linux16 insmod lsefi insmod lspci insmod memdisk insmod ls insmod video insmod video_fb insmod normal insmod test insmod sleep insmod probe insmod png insmod jpeg insmod gettext insmod video_bochs insmod video_cirrus insmod gzio insmod efi_uga if loadfont /efi/grub2/themes/ascii.pf2;then loadfont /efi/grub2/themes/DejaVuSans-Bold14.pf2 loadfont /efi/grub2/themes/DejaVuSans10.pf2 loadfont /efi/grub2/themes/DejaVuSans12.pf2 loadfont /efi/grub2/themes/asciii.pf2 set theme=/efi/grub2/themes/theme.txt background_image -m stretch /efi/grub2/themes/background.jpg fi # 引导 LiveCD ISO 和 DVD files entry， iso 文件都在 (hd,gpt1)/boot/ 下面 # Most from wiki.archlinux.org/index.php/GRUB ### Ubuntu AMD64 ### menuentry \"Ubuntu LiveCD\" { GRUB_DISTRIBUTOR=ubuntu setearch --no-floppy --fs-uuid --set=root $uuid1 set gfxpayload=keep loopback loop /boot/ubuntu.iso echo Loading vmlinuz.efi... linux (loop)/linuxcasper/vmlinuz.efi boot=casper iso-scan/filename=/boot/ubuntu.iso locale=en_US.UTF-8 echo Loading initrd.lz... initrd (loop)/casper/initrd.lz echo booting... } ### local opensuse ### menuentry \"Local Fedora\"{ set gfxpayload=keep set root=(hd1,gpt1) echo loading linux... chainloader (hd1,gpt1)/efi/fedora/grubx64.efi boot } menuentry \"Local openSUSE\"{ set gfxpayload=keep set root=(hd1,gpt3) echo loading linux... chainloaderinloader (hd1,gpt1)/EFI/opensuse/grubx64.efi boot } #### openSUSE KDE ### menuentry \"openSUSE KDE liveCD\" { GRUB_DISTRIBUTOR=opensuse set gfxpayload=keep #set root=(hd0,gpt1) search --no-floppy --fs-uuid --set root $uuid1 loopback loopback /boot/opensuse.iso echo Loading linux... linuxefi (loop)/boot/x86_64/loader/linux isofrom_device=/dev/sdb1 isofrom_system=/boot/opensuse.iso LANG=en_US.UTF-8 echo Loading initrd... initrdefi (loop)/boot/x86_64/loadfontder/initrd echo booting... } 当系统引导遭到破坏只能进入 grub 界面时，可以手动引导系统，然后再修复引导。 grub2 引导 LiveCD grub 美化教程 grub 是高度可定制的， 可以按照喜好，美化引导界面。 也可以套用现成的主题。 水水更健康…&lt;(￣▽￣)&gt; 哇哈哈… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux grub"},{"title":"配置你的 Vim 来提高工作效率","url":"/2015/Customizing-Your-Vim-to-Raise-Working-Efficiency/","text":"vim 被称为编辑器之神, 是一个超级超级强大的文本编辑器, 它和 Emacs 是一个数量级, 被无数程序员所推崇. 默认的 vim 配置基本可以使用, 但是若加以调教, 将会使你的效率大大提高. 按惯例, 贴上自己的配置文件.(*￣︶￣)y. 最新的在 这里 set nocompatible \" Set nocompatible // nc \" let g:mapleader=\";\" \" Mapping global leader \" let mapleader=\";\" \" Mapping local leader \" \" ==========>> Encoding Mapping Setting Autocmd Command Functions ctags Syntax Conkyrc PHP jquery JavaScripts AutoTags Auto-Pairs ls.vim Syntax todo.vim AutoClose Latex-Suite C/C++ IDE Compilor Taglist pyflakes-vim","tags":"linux vim"},{"title":"在 Gnome3 下安装和配置 Fcitx","url":"/2015/Install-and-Config-Fcitx-under-Gnome3/","text":"gnome3 大概自从 3.8 起, 就整合了 ibus, 然而 ibus 极其难用. 在受不了 ibus 之后, 果断转入了 fcitx. 但是由于 gnome3 的整合, 使得使用 fcitx 有点麻烦. 卸载 ibus ibus 不一定要删除, 有强迫症的就卸载吧. fedora 下: sudo dnf remove ibus 安装 fcitx sudo yum install -y fcitx fcitx-pinyin fcitx-configtool fcitx-cloudpinyin im-chooser &nbsp;fcitx-sunpinyin sudo yum install -y fcitx-libpinyin&nbsp; fcitx-table-extra fcitx-table-other fcitx-fbterm fcitx-gtk2 fcitx-gtk3 sudo yum install -y fcitx-qt4 fcitx-qt5 fcitx-table-chinese fcitx-m17n fcitx-configtool 禁用 ibus 为保证成功, 普通用户下: gsettings set org.gnome.settings-daemon.plugins.keyboard active false 映射 CapsLock 到 Ctrl (可选) gsettings set org.gnome.desktop.input-sources xkb-options \"['caps:ctrl_modifier']\" 配置 .xinitrc 这一步不是很确定是否必须. 修改 $HOME/.xinitrc, 加入以下配置 (没有可自己创建): export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\"@im=fcitx\" 在 im-chooser 中选择 fcitx 使用 im-chooser 选择 fcitx 作为默认输入方式. 注销再重新登录即可使用. 此方法在 wayland 下面已经失效. 可以使用 Gnome Tweak Tool 将 Fcitx 加入开机启动。 转到 Xorg 显示模式 目前 Fcitx 对 Wayland 支持并不好，目前 Fedora 27/28 已经默认使用 Wayland 作为显示系统, 需要在登陆时转换到 Xorg 模式。 配置 fcitx 重启系统， 登录进入系统后， 运行 fcitx-configtool 对 fcitx 按照喜好进行配置。就可以愉快地使用 Fcitx 了。 (*￣▽￣)y, 水水更健康 \"o((&gt;ω&lt;))o\" document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"linux fedora gnome fcitx"},{"title":"转 -- 慢慢地, 它们就消失了, 就像从未存在过","url":"/2015/They-Disappeared-Like-Never-Existed/","text":"有感于最近 SS 作者被迫删除代码, 想转发曾经看过的一篇文章–《慢慢地, 它们就消失了, 就像从未存在过》, 作者是豆瓣上的 某人, 大概也是迫于压力, 已经被删除了. 在这个高呼 “拥抱互联网” 的国度, 一步步打造着伟大的局域网, 慢慢地, 连这些声音也会听不到了吧. 是以谨记. 几年以前，我曾经嘲笑过某科技界大佬。当时他说：也许 90 后、95 后会慢慢不知道谷歌是什么网站。 那一年，这对于我来说简直就是世界上最好笑的笑话。谷歌，全世界最卓越的互联网公司，活在互联网的一代中国人，会不知道他们的网站？ 今天，我收回这句嘲笑。因为这件不可能的事，它慢慢变成了现实。 没有人再关注什么谷歌不谷歌。对他们来说，百度也蛮好用的，反正他们几乎没用过谷歌。没有谷歌又怎样？大家还是开心的刷微博，看微信，听歌，看娱乐节目。对于从来就不知道谷歌的人来说，少了谷歌又有什么影响？ 多年前，我们也是可以登陆 Facebook 的。其实这个网站和校内一样，也挺蠢的。可在上面你能看到老外们的生活，可以轻易的跟一万公里以外的人互相拜访， 可以看到很多根本不会开到校内上的主页。你用汉语回复，下面给你聊起来的可能是香港仔，可能是台湾人。你用英语回复，说不定有比你英语用的更蹩脚的寂寞的 北欧人来跟你搭讪。你感觉地球真的变成了地球村，你还没拉门走出去，别人就推门走了进来。 然后，它就没有了。起初，它的失踪激起了很大的声音，后来，声音就消失了。 多年前，我们也是可以登陆 Twitter 的。其实这个网站和微博一样，也不过是些信息流，刷上一整天，也不见得有什么用处。但至少，你可以以最快速度获取你 想知道的任何新事，你会真正了解什么事情在全世界是流行的，而不是经过各种截图、翻译、转发，甚至曲解、断章取义、黑白颠倒的东西。你知道的是真相，赤裸裸的，也许有点太短的真相。但至少中间不会有无数人的加工与再加工，偏激、片面，就在这个过程中产生了，不管后来者有意还是无意。 然后，它就没有了。首先是它的本体没有了，然后它的模仿者也没有了，模仿者的模仿者也没有了。只剩一个模仿者的模仿者的模仿者，现在你每天能在上面看到无数广告。 多年前，我们也是可以登陆 YouTube 的。对于有的人来说，这个网站就是个大型优酷，当年有人信誓旦旦的说，没有 YouTube，我们中国人会很快让优酷超过 YouTube。可这么多年过去了，视频还是那么卡，内容还是那么垃圾，原创还是那么容易被盗窃，视频丰富度还是那么的可怜。在 YouTube 上，你能看到全世界最棒的手艺人，最逗乐的笑话，最天马行空的创意，最激荡人心的音乐，最美好的完美瞬间，可在优酷上，你想看一分钟视频，请先看半分钟广告。 哦， 对了。Instagram，有些人可能感觉它和 QQ 空间也差不多。可我在上面关注了六百多个摄影师，它们都是顶好顶好的影像记录者，每天看他们的作品，我 感觉到很幸福，那种即使没有到那里去，也身临其境的幸福。我还在上面认识了一个日本的爱自拍的帅小伙，一个爱喝酒的韩国大叔，一个十年前到过中国今天会在每张我发的紫禁城照片下点赞的美国大爷，一个美丽无比的俄罗斯妹子，我和他们基本上都难以交流，语言是很大的障碍，但几个简单的单词，心意也就到了，这种感觉，有时候比多年老友相聚还兴奋。因为这是人类不同族群自由交流互相沟通的过程，这种过程很神奇，真的很神奇。 可现在，它没有 了，它之所以没有就因为在某个特定的时间你在搜索特定的词汇时，会搜出来特定的照片。虽然这么搜的人并不多，虽然看到的人也不会大惊小怪，也不会觉得天黑 了，天亮了，天要塌了，天要变了。可它就是没了，Instagram，就这么没了。谷歌也是这么没的，Twitter 也是这么没的，Facebook 也是这么没的。不知道是什么人，在什么场合，说了什么话，下了什么决定。就要有超过十亿人像陷于哥谭市的孤岛里一样，看着一座又一座桥梁被炸掉，又被炸掉，又被炸掉，然后，就什么都没了。 我时常觉得悲哀，真的好悲哀，一个我根本不认识也不知道是谁的人，也许是一个群体，在不断抢走我身边的东西，而我却无能为力。我抱怨一声，他听不到，任何人都听不到。我怒吼一句，身边的大多数人却像看疯子一样的看着我。我哀嚎一声，这声音被阻碍在黑黑的幕墙以里。我发出尖锐的嘶吼，这声音传不了多远，就和我那被抢走的东西一样，消失了，不见了，就像从来没存在过一样。 对于本来就没存在过的东西，有谁又会觉得在意呢？那些本来拥有又被掠夺的人的哀愁，后来的人又怎么懂呢？我曾经是拥有一切的，我曾经是拥有世界的，我站在这片土地上， 呼吸的是自由的空气，饮下的是自由的琼浆玉液。就在长的无法计数的时间里，我自由生命的一部分又一部分就这么被杀死了，突然就杀死了。可我还始终觉得，它们还奄奄一息的活着，就像它们是慢慢的死去的一样。 可它们终归是死了，而且随着它们的死，愈来愈多的事情慢慢的发生了，很慢很慢，几乎不被人察觉，可还是发生了。 没有谷歌，我可以用百度呀。可某些结果被越挪越后，越挪越后，最后就不见了。就像本来就不该搜出这个结果一样。 没有 Facebook，我可以用校内呀。可你想发只有在 Facebook 上能发的文章，很快在校内上就失踪了。接着，校内变成了人人，话题变成了人人都关心的话题。大家都在抢着看星座、明星、八卦、娱乐。没有人会关心什么消失了，反正它们本来也没多少存在感。 没有 YouTube，我可以用优酷呀。可你却经常只能在优酷上看到抄袭别人的作品，而且还不署名，而且还洋洋得意，而且还自我陶醉，就好像那个 idea 本来 属于他自己一样。你看了还要惊呼，他是如此的有创意！好一个抄袭的创意，可你却不知道，因为你不知道这个世界上有个网站叫 YouTube。 没有 Twitter，我还可以用微博呀。可你想知道最近发生了什么，你搜的越勤快，越能看到越明显的 “根据相关法律法规，相关搜索结果不予显示 ”。时间长了，你想，反正知道了也没什么用，不如不看了。 慢慢的，一扇又一扇的门关上了。今天你打开世界上最大的博客网站，发现它没了。明天你一看，世界上最好的设计师分享网站没了，一开始是刷新的很慢很慢，后来它就没了。过两天再一看，平常每天都会读两篇文章的媒体网站没了，那里的文章缤纷多彩，最后都变成了该页无法显示几个字。再过几个月，大学的网站不让上 了，摄影师的网站不让上了，就连百度日本这种自家网站，也没了。 接着，漫画看不了了，接着，动画看不成了。接着，美剧英剧失踪了。下载美剧英剧的网站又又又又又失踪了。尊重正版，保护权益，行吧，然后字幕网站也没了。 游戏没了，你习惯性登陆的游戏网站，发现下载栏正在整治中。论坛关了，天天都在看的论坛，突然接到相关部门的电话，因为 “报备问题 ”不让办了。个人网站，私人博客，对不起，说没就没有，你在上面存了多少多年辛勤耕耘的东西都没用。 你关注的人，有一天你登陆微博，发现他怎么好久都没说话了，然后你搜索了一下，发现他的账号不存在了，而且你搜他的名字，他的名字未予显示。 一盏一盏的灯，灭了。四面八方的光源，消失了。我们生活的五光十色的世界，变成了一片黑色。 天黑了，那么睡觉吧，但愿长醉不复醒，卧槽泥马勒戈壁。 最后，我们变成了一群做梦的人，这个梦的名字，叫根据相关法律法规，相关搜索结果不予显示梦。 注: 然而, 渐渐地, 这篇文章也不见了. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"浪漫稍纵即逝 --《老爸老妈浪漫史》经典台词","url":"/2015/Romance-in-How-I-Met-Your-Mother/","text":"最近心情波动的比较厉害, 常常难以平静… 百无聊赖中, 搜集一些 &lt;&lt; 老爸老妈浪漫史 &gt;&gt; 中的一些经典台词, 是以静心… None of us can vow to be perfect. In the end all we can do is promise to love each other with everything we’ve got. Because love’s the best thing we do. 没有人可以发誓做到完美，最后我们能保证的只有深爱彼此，倾尽所有。因为爱是最美好的事。 Whatever you do in this life, it’s not legendary, unless your friends are there to see it. 无论你今生做什么，没有朋友在你身旁见证，就算不上是传奇。 Ted: Kids, you can’t cling to the past, because no matter how tightly you hold on, it’s already gone. Ted: 孩子们, 你不能对过去念念不忘，因为无论你把回忆抓得多紧，它早已悄无声息地消失。 Life is short, and if you ever come across a beautiful, exciting, crazy moment in it, you gotta seize it while you can before that moment’s gone. 人生苦短，如果有机会碰到美丽、激动、疯狂的时刻，就得在那一刻消失之前紧紧把握住。 If you love something, you can never let it go, not even for a second, or it’s gone forever. 如果你爱一样东西，你就永远都不能放手，一秒都不能放，否则你将会永远失去它。 Kids, life is a dark road. You never really know what’s up ahead. 孩子们，人生就象一条黑暗的道路。你永远也不知道前方会发生什么。 Barney: Whatever you do in this life, it’s not legendary, unless your friends are there to see it. Barney: 无论你今生做什么，没有朋友在你身旁见证，就算不上是传奇。 Ted：sometimes things have to fall apart to make ways for better things. Ted : 为了遇到那些美好的事物，我们必须有所舍弃！ Lily: Say good_bye to all the times you felt lost,to all the times it was a no instead of a yes,to all the scrapes and bruises,to all the heartache.Say good bye to everything you really want to do for the last time. Lily: 向那些你感觉失落的日子说再见，在那些日子里你曾被一次次拒绝，你获得满身伤痕，你只觉心痛不止，向所有这些日子，向所有那些你真心渴望拥有的事情最后道一声再见. Stella: I know that you’re tired of waiting, but she’s no her way, Ted. And you may have to wait a little while more, and she’s getting here as fast as she can. Stella: 我知道你厌倦了等待, 但是她就在不远处了, Ted. 你可能只需要再等那么一小会儿. 她正在以最快的速度赶过来. Ted: First there’s the moment when you think you think it. There’s the moment you think you know it. There’s the moment where you know you know it, but you can’t yet say it. And then there’s the moment where you know you know it, and you can’t keep it in any longer. Ted: 开始的时候，你意识到自己开始思考这事，之后你意识到你确实爱他，再后来你知道你爱他，但你说不出口，再再后来，你知道你爱他，但你再也忍不住想说出来。 Ted: While baseball, strippers, and guns can’t help, the only thing that can really heal a broken heart is time. Ted: 棒球、脱衣舞娘、手枪都不起作用，惟一可以真正治愈那颗破碎的心的只有时间。 when you meet someone special, suddenly life is full of firsts. The first kiss. The first night together. The first weekend together. 当你遇到了喜欢的人，生活突然就被无数个第一次填满了。第一次接吻，第一次同居，第一次一起过周末。 Kids when you are single, all you’re looking for is happily ever after。 But only one of your stories can end that way the rest ended with somebody get hurt. 孩子们，当你们单身的时候，你们以为感情的结局就是王子公主永远过上幸福快乐的生活。但在你所有的感情经历中，只会有一个是以这种方式结尾，其它都以至少有一个人受伤而告终。 None of us can vow to be perfect. In the end all we can do is promise to love each other with everything we’ve got. Because love’s the best thing we do. 没有人可以发誓做到完美，最后我们能保证的只有深爱彼此，倾尽所有。因为爱是最美好的事。 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"himym movie"},{"title":"使用 XeTeX 配置 LaTeX 中文环境","url":"/2015/Setup-Chinese-Environment-with-XeTeX/","text":"LaTeX 是一种基于 TeX 的排版系统，由美国电脑学家莱斯利·兰伯特在 20 世纪 80 年代初期开发，利用这种格式，即使使用者没有排版和程序设计的知识也能在几天，甚至几小时内生成很多具有书籍品質的印刷品。对于生成复杂表格和数学公式，这一点表现得尤为突出。因此它非常适用于生成高印刷质量的科技和数学类文档。 TeX/LaTeX 作为装 X 是非常好用的. 当然这不是重点. 重点是, 一般科技论文都是需要用 latex 书写的, 不仅美观, 方便, 而且是平台无关的, 许多杂志都有自己的专用木板. 什么? 你说 word? 战斗力只有 5 的渣渣, 连自己的相同版本都无法做到格式一致性. 而且时不时告诉你版本过低什么的. 如果遇上没有安装 word, 只能呵呵了. 嗯, 程序员也特别钟爱 latex. 用 latex 写出来的简历简介美观, 当然你需要好好调教才可以啦…嗯~ o(*￣▽￣*)o TeX 本为英文排版设计, LaTeX 自然也是如此, 但是世界上有那么多种文字, 都想用上这么高大上的排版系统啊. 中国的汉语作为使用人数最多的语言, 这种需要就更加强烈啦, 于是就有大牛开发了各种宏包, 如 CCT, CJK, xeCJK 等等, 也就有了以各种中文排版方案作为特色的 LaTeX 套装. A. 获取安装 LaTeX 要在 LaTeX 中使用中文, 首先自然是要有 latex 啦.╭(′▽`)╭(′▽`)╯… windows 下, 可以下载 MikTeX 或 CTex… 这个不需要多说… linux 下最出名的就是 TeXLive 套装了. 各种发行版都可以从仓库中安装… Mac 下就是 MacTeX 了, TeXLive 的 马甲版本. 不嫌麻烦用 brew 装 texlive 吧 ((￣▽￣\")) Linux 下安装最好全部安装, 以免突然警告你什么什么包没有装… B. 中文环境配置 这里主要接受 Linux 和 Mac 下的中文环境配置. 1. CCT 这个很老了, 已经不维护了. 已经不推荐了. 2. CJK Tex 最初的设计没有考虑中文字符的问题，后来就有了 CJK 外挂宏包对 Latex 进行中文支持. CJK, 由德国人 Werner Lemberg (还不是国人 o(≧口≦)o) 编写。这个巨集包不仅仅支持繁简体中文、日文、朝鲜文等东亚语言，而且它也是一个多种语言支持包，另外还支持几十种其他不同的语言。 开始的时候, 但毕竟是外挂宏包，使用起来不甚方便，维护工作也很麻烦, 需要自己编译生成中文字体集，而且常常有些问题，比如，中文目录的问题就一直没能解决，总是乱码。于是有了用 unicode 编码的 xetex 的出现. 3. XeTeX xetex 是一种使用 Unicode 的 TeX 排版引擎，并支持一些现代字体技术，例如 OpenType、Graphite 和 Apple Advanced Typography（AAT）, 可以直接利用其高级特性，例如额外的字形，花体，合字，可变的文本粗细等等。 xetex 原生支持 Unicode，并默认其输入文件为 UTF-8 编码，也即, 英文字符与非英文字符不再有区别，原生支持系统字体，这意味着我们无需再额外编译字体, 故可以在不进行额外配置的情况下直接使用操作系统中安装的字体。这简直就是大家一直以来梦寐以求的功能, 大大简化了使用中文的难度. 从此再也不用使用老旧的 pdflatex,CJK 等等啦… 安装完 TeXLive/MacTeX 后, 就可以直接在 tex 文件中使用中文了… 下面就是一个简单的中文 tex. \\documentclass[12pt,a4paper]{article} \\usepackage{xltxtra,fontspec,xunicode} \\setmainfont{WenQuanYi Zen Hei} % 设置文档默认字体 \\date{} % 不显示文档生成日期 \\title{\\XeTeX{} 中英文环境测试 } \\begin{document} \\maketitle \\XeTeX{} is a \\TeX{} typesetting engine using Unicode and supporting modern font technologies.\\\\ \\XeTeX{} 是一个使用 Unicode 的 \\TeX{} 排版系统，并支持一些现代字体技术.\\\\ \\end{document} 当然你需要安装 wqy-zenhei-fonts, 也可以改成其他中文字体. 保存为 test.tex, 运行以下命令: xelatex test.tex 打开生成的 test.pdf 就可以看到如下的 一般来说, 到这里就可以了, 剩下的就是设置字体的问题了. 但是这样英文数字也使用中文字体了, 看上去不是很美观. 于是, xetex 和 CJK 商量了一下, 就有了 xeCJK 宏包, 可以分别指定中英文字体, 于是可以很好的排版中英混合的文章了. 当然首先是要使用 xeCJK 包, 添加上 \\usepackage[slantfont,boldfont]{xeCJK} 分别设置中英文字体, 英文字体可以不用设置, xetex 会调用默认的字体: \\setCJKmainfont{Kai} % 设置缺省中文字体 \\setCJKmonofont{Hei} % 设置等宽字体 %\\setmainfont{Optima} % 英文衬线字体 %\\setmonofont{Monaco} % 英文等宽字体 %\\setsansfont{Trebuchet MS} % 英文无衬线字体 下面是测试, \\documentclass[12pt,a4paper]{article} \\usepackage{xltxtra,fontspec,xunicode} \\usepackage[slantfont,boldfont]{xeCJK} \\setCJKmainfont{WenQuanYi Zen Hei} % 设置缺省中文字体 %\\setCJKmonofont{Hei} % 设置等宽字体 %\\setmainfont{Optima} %% 不指定，使用 Tex 的默认英文衬线字体 %\\setmonofont{Monaco} % 英文等宽字体 %\\setsansfont{Trebuchet MS} % 英文无衬线字体 \\date{} % 不显示文档生成日期 \\title{\\XeTeX{} 中英文环境测试 } \\begin{document} \\maketitle \\XeTeX{} is a \\TeX{} typesetting engine using Unicode and supporting modern font technologies.\\\\ \\XeTeX{} 是一个使用 Unicode 的 \\TeX{} 排版系统，并支持一些现代字体技术.\\\\ \\end{document} 依照上面的, 可以看到新的效果. 中英文见加了一个空格, 看上去更加美观, 只是需要设置以下字体而已. 到这里应该就结束, 但是字海茫茫, 对于不熟悉的童鞋, 怎么找到需要的字体呢? C. 查找设置中英字体. 1. Linux/macOS 下字体设置 linux 下可以通过 fc-list 来查看已安装字体, 这是我安装的中文字体. $ fc-cache && fc-list :lang=zh /usr/share/fonts/google-noto-cjk/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK SC,Noto Sans Mono CJK SC Bold:style=Bold,Regular /usr/share/fonts/tex-gyre/texgyreheroscn-bolditalic.otf: TeX Gyre Heros Cn:style=Bold Italic /usr/share/fonts/google-noto-cjk/NotoSansCJK-DemiLight.ttc: Noto Sans CJK TC,Noto Sans CJK TC DemiLight:style=DemiLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Black.ttc: Noto Serif CJK SC,Noto Serif CJK SC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Light.ttc: Noto Sans CJK SC,Noto Sans CJK SC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Bold.ttc: Noto Sans CJK SC,Noto Sans CJK SC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Black.ttc: Noto Sans CJK SC,Noto Sans CJK SC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Thin.ttc: Noto Sans CJK SC,Noto Sans CJK SC Thin:style=Thin,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Thin.otf: Noto Sans CJK SC,Noto Sans CJK SC Thin:style=Thin,Regular /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-Bold.otf: Source Han Serif CN, 思源宋体 CN:style=Bold /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-ExtraLight.otf: Source Han Serif CN, 思源宋体 CN,Source Han Serif CN ExtraLight, 思源宋体 CN ExtraLight:style=ExtraLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Bold.ttc: Noto Serif CJK TC:style=Bold /usr/share/fonts/google-noto-cjk/NotoSansCJK-Medium.ttc: Noto Sans CJK TC,Noto Sans CJK TC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Light.ttc: Noto Serif CJK SC,Noto Serif CJK SC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Bold.ttc: Noto Serif CJK SC:style=Bold /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Regular.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Regular, 思源黑体 CN Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Light.otf: Noto Sans CJK TC,Noto Sans CJK TC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Black.ttc: Noto Sans CJK TC,Noto Sans CJK TC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Thin.ttc: Noto Sans CJK TC,Noto Sans CJK TC Thin:style=Thin,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK SC,Noto Serif CJK SC ExtraLight:style=ExtraLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-ExtraLight.ttc: Noto Serif CJK TC,Noto Serif CJK TC ExtraLight:style=ExtraLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Medium.ttc: Noto Serif CJK TC,Noto Serif CJK TC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Black.otf: Noto Sans CJK SC,Noto Sans CJK SC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Light.ttc: Noto Serif CJK TC,Noto Serif CJK TC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansMonoCJKtc-Bold.otf: Noto Sans Mono CJK TC,Noto Sans Mono CJK TC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-ExtraLight.otf: Noto Serif CJK SC,Noto Serif CJK SC ExtraLight:style=ExtraLight,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-ExtraLight.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN ExtraLight, 思源黑体 CN ExtraLight:style=ExtraLight,Regular /usr/share/fonts/google-noto-cjk/NotoSansMonoCJKtc-Regular.otf: Noto Sans Mono CJK TC,Noto Sans Mono CJK TC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Black.otf: Noto Sans CJK TC,Noto Sans CJK TC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-Bold.otf: Noto Serif CJK SC:style=Bold /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Light.otf: Noto Sans CJK SC,Noto Sans CJK SC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-Bold.otf: Noto Serif TC:style=Bold /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-Medium.otf: Source Han Serif CN, 思源宋体 CN,Source Han Serif CN Medium, 思源宋体 CN Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Regular.otf: Noto Sans CJK TC,Noto Sans CJK TC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Black.ttc: Noto Serif CJK TC,Noto Serif CJK TC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-Medium.otf: Noto Serif CJK SC,Noto Serif CJK SC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Light.ttc: Noto Sans CJK TC,Noto Sans CJK TC Light:style=Light,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Normal.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Normal, 思源黑体 CN Normal:style=Normal,Regular /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-SemiBold.otf: Source Han Serif CN, 思源宋体 CN,Source Han Serif CN SemiBold, 思源宋体 CN SemiBold:style=SemiBold,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-Black.otf: Noto Serif CJK SC,Noto Serif CJK SC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-ExtraLight.otf: Noto Serif TC,Noto Serif TC ExtraLight:style=ExtraLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-Black.otf: Noto Serif TC,Noto Serif TC Black:style=Black,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-DemiLight.otf: Noto Sans CJK SC,Noto Sans CJK SC DemiLight:style=DemiLight,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Medium.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Medium, 思源黑体 CN Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansMonoCJKsc-Bold.otf: Noto Sans Mono CJK SC,Noto Sans Mono CJK SC Bold:style=Bold,Regular /usr/share/fonts/tex-gyre/texgyreheroscn-bold.otf: TeX Gyre Heros Cn:style=Bold /usr/share/fonts/google-noto-cjk/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK TC,Noto Sans Mono CJK TC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-Light.otf: Noto Serif TC,Noto Serif TC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-SemiBold.otf: Noto Serif TC,Noto Serif TC SemiBold:style=SemiBold,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Medium.ttc: Noto Serif CJK SC,Noto Serif CJK SC Medium:style=Medium,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Bold.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Bold, 思源黑体 CN Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Bold.ttc: Noto Sans Mono CJK TC,Noto Sans Mono CJK TC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Bold.otf: Noto Sans CJK TC,Noto Sans CJK TC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Medium.otf: Noto Sans CJK SC,Noto Sans CJK SC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-Light.otf: Noto Serif CJK SC,Noto Serif CJK SC Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Bold.otf: Noto Sans CJK SC,Noto Sans CJK SC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Regular.ttc: Noto Sans Mono CJK SC,Noto Sans Mono CJK SC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-Regular.otf: Noto Serif CJK SC:style=Regular /usr/share/fonts/tex-gyre/texgyreheroscn-italic.otf: TeX Gyre Heros Cn:style=Italic /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-Heavy.otf: Source Han Serif CN, 思源宋体 CN,Source Han Serif CN Heavy, 思源宋体 CN Heavy:style=Heavy,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-Regular.otf: Noto Serif TC:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-DemiLight.otf: Noto Sans CJK TC,Noto Sans CJK TC DemiLight:style=DemiLight,Regular /usr/share/fonts/google-noto-cjk/NotoSerifTC-Medium.otf: Noto Serif TC,Noto Serif TC Medium:style=Medium,Regular /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-Light.otf: Source Han Serif CN, 思源宋体 CN,Source Han Serif CN Light, 思源宋体 CN Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Bold.ttc: Noto Sans CJK TC,Noto Sans CJK TC Bold:style=Bold,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Medium.otf: Noto Sans CJK TC,Noto Sans CJK TC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansMonoCJKsc-Regular.otf: Noto Sans Mono CJK SC,Noto Sans Mono CJK SC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Medium.ttc: Noto Sans CJK SC,Noto Sans CJK SC Medium:style=Medium,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Regular.ttc: Noto Sans CJK SC,Noto Sans CJK SC Regular:style=Regular /usr/share/fonts/tex-gyre/texgyreheroscn-regular.otf: TeX Gyre Heros Cn:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKsc-Regular.otf: Noto Sans CJK SC,Noto Sans CJK SC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK TC,Noto Serif CJK TC SemiBold:style=SemiBold,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Heavy.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Heavy, 思源黑体 CN Heavy:style=Heavy,Regular /usr/share/fonts/adobe-source-han-sans-cn/SourceHanSansCN-Light.otf: Source Han Sans CN, 思源黑体 CN,Source Han Sans CN Light, 思源黑体 CN Light:style=Light,Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-Regular.ttc: Noto Sans CJK TC,Noto Sans CJK TC Regular:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-SemiBold.ttc: Noto Serif CJK SC,Noto Serif CJK SC SemiBold:style=SemiBold,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJKsc-SemiBold.otf: Noto Serif CJK SC,Noto Serif CJK SC SemiBold:style=SemiBold,Regular /usr/share/fonts/adobe-source-han-serif-cn/SourceHanSerifCN-Regular.otf: Source Han Serif CN, 思源宋体 CN:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJKtc-Thin.otf: Noto Sans CJK TC,Noto Sans CJK TC Thin:style=Thin,Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Regular.ttc: Noto Serif CJK TC:style=Regular /usr/share/fonts/google-noto-cjk/NotoSerifCJK-Regular.ttc: Noto Serif CJK SC:style=Regular /usr/share/fonts/google-noto-cjk/NotoSansCJK-DemiLight.ttc: Noto Sans CJK SC,Noto Sans CJK SC DemiLight:style=DemiLight,Regular 可以按照所需设置字体, 比如如下这样: \\usepackage[slantfont,boldfont]{xeCJK} \\setCJKmainfont[BoldFont=Noto Sans CJK SC]{Noto Serif CJK SC} \\setCJKsansfont{Noto Sans CJK SC} \\setCJKmonofont{Noto Sans Mono CJK SC} \\setmainfont[BoldFont=Noto Sans][Noto Serif] \\setsansfont{Noto Sans} \\setmonofont{Noto Mono} 2. Windows 下字体设置 Windows 下的中文字体也可以通过 fc-list :lang=zh 得到， D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolFang-Regular.otf: FandolFang,FandolFang R:style=Regular C:/WINDOWS/fonts/msyhl.ttc: Microsoft YaHei,Microsoft YaHei Light:style=Light,Regular C:/WINDOWS/fonts/Dengb.ttf: DengXian:style=Bold C:/WINDOWS/fonts/STZHONGS.TTF: STZhongsong:style=Regular C:/WINDOWS/fonts/simsun.ttc: NSimSun, 新宋体:style=Regular, 常规 C:/WINDOWS/fonts/Dengl.ttf: DengXian,DengXian Light:style=Light,Regular C:/WINDOWS/fonts/simfang.ttf: FangSong:style=Regular,Normaali C:/WINDOWS/fonts/simkai.ttf: KaiTi:style=Regular,Normaali C:/WINDOWS/fonts/msyh.ttc: Microsoft YaHei UI:style=Normal D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolKai-Regular.otf: FandolKai:style=Regular D:/Program Files/texlive/2018/texmf-dist/fonts/truetype/public/arphic-ttf/bsmi00lp.ttf: AR PL Mingti2L Big5, 文鼎ＰＬ細上海宋:style=Reguler,Regular C:/WINDOWS/fonts/STCAIYUN.TTF: STCaiyun:style=Regular C:/WINDOWS/fonts/Deng.ttf: DengXian:style=Regular C:/WINDOWS/fonts/STXINGKA.TTF: STXingkai:style=Regular C:/WINDOWS/fonts/msyhl.ttc: Microsoft YaHei UI,Microsoft YaHei UI Light:style=Light,Regular C:/WINDOWS/fonts/STXINWEI.TTF: STXinwei:style=Regular C:/WINDOWS/fonts/msyhbd.ttc: Microsoft YaHei UI:style=Έντονα C:/WINDOWS/fonts/msjhl.ttc: Microsoft JhengHei, 微軟正黑體 Light:style=Light,Regular C:/WINDOWS/fonts/msjhl.ttc: Microsoft JhengHei UI,Microsoft JhengHei UI Light:style=Light,Regular C:/WINDOWS/fonts/msyh.ttc: Microsoft YaHei:style=Normal D:/Program Files/texlive/2018/texmf-dist/fonts/truetype/public/arphic-ttf/gbsn00lp.ttf: AR PL SungtiL GB:style=Regular C:/WINDOWS/fonts/SIMYOU.TTF: YouYuan:style=Regular C:/WINDOWS/fonts/STXIHEI.TTF: STXihei:style=Regular C:/WINDOWS/fonts/STKAITI.TTF: STKaiti:style=Regular C:/WINDOWS/fonts/STLITI.TTF: STLiti:style=Regular C:/WINDOWS/fonts/ARIALUNI.TTF: Arial Unicode MS:style=Regular,normal C:/WINDOWS/fonts/msyhbd.ttc: Microsoft YaHei:style=Έντονα C:/WINDOWS/fonts/simsun.ttc: SimSun, 宋体:style=Regular, 常规 D:/Program Files/texlive/2018/texmf-dist/fonts/truetype/public/arphic-ttf/gkai00mp.ttf: AR PL KaitiM GB:style=Regular C:/WINDOWS/fonts/STHUPO.TTF: STHupo:style=Regular D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolHei-Bold.otf: FandolHei:style=Bold C:/WINDOWS/fonts/STFANGSO.TTF: STFangsong:style=Regular D:/Program Files/texlive/2018/texmf-dist/fonts/truetype/public/arphic-ttf/bkai00mp.ttf: AR PL KaitiM Big5, 文鼎ＰＬ中楷:style=Regular D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolSong-Bold.otf: FandolSong:style=Bold C:/WINDOWS/fonts/msjh.ttc: Microsoft JhengHei:style=Regular C:/WINDOWS/fonts/FZYTK.TTF: FZYaoTi:style=Regular C:/WINDOWS/fonts/SIMLI.TTF: LiSu:style=Regular C:/WINDOWS/fonts/msjhbd.ttc: Microsoft JhengHei UI:style=Félkövér D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolHei-Regular.otf: FandolHei:style=Regular C:/WINDOWS/fonts/STSONG.TTF: STSong:style=Regular C:/WINDOWS/fonts/msjh.ttc: Microsoft JhengHei UI:style=Regular C:/WINDOWS/fonts/msjhbd.ttc: Microsoft JhengHei:style=Félkövér C:/WINDOWS/fonts/simhei.ttf: SimHei:style=Normal D:/Program Files/texlive/2018/texmf-dist/fonts/opentype/public/fandol/FandolSong-Regular.otf: FandolSong:style=Regular C:/WINDOWS/fonts/FZSTK.TTF: FZShuTi:style=Regular 将 LaTeX 中的 CJK 字体设置成合适的中文字体就行，如 \\usepackage[slantfont,boldfont]{xeCJK} \\setCJKmainfont[BoldFont=SimHei,ItalicFont=KaiTi]{SimSun} \\setCJKsansfont{SimHei} \\setCJKmonofont{STFangSong} \\setmainfont[BoldFont=Noto Sans][Noto Serif] \\setsansfont{Noto Sans} \\setmonofont{Noto Mono} 3. 设置其他字体命令 比如我们想要使用华文琥珀、楷体 \\newfontfamily\\huawenhupo{STHupo} \\newfontfamily\\kai{STKaiti} % 楷体 {\\huawenhupo 自定义的字体 - 华文琥珀} {\\kai 自定义的字体 - 楷体} 水水更健康 ╮(￣▽￣\")╭ document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"latex"},{"title":"Configure Apache Under OSX 10.9 & 10.10","url":"/2015/Configure-Apache-Under-OSX-10-9-10-10/","text":"It’s common to test our site on local machine. And it’s useful to keep it down about configuring apache on mac osx 10.9 and 10.10. The process under linux is similar. 配置启用 Virtual Host 修改/etc/apache2/httpd.conf, 将下面两行取消注释. 这两行用来启用 Virtual Host #Include /private/etc/apache2/extra/httpd-vhosts.conf #LoadModule vhost_alias_module libexec/apache2/mod_vhost_alias.so 在 /etc/apache2/extra/httpd-vhost.conf中有一个 vhost 的样本, 可以按照它来设置一个Virtual Host. 如: ServerAdmin webmaster@dummy-host2.example.com DocumentRoot \"/usr/docs/dummy-host2.example.com\" ServerName dummy-host2.example.com ErrorLog \"/private/var/log/apache2/dummy-host2.example.com-error_log\" CustomLog \"/private/var/log/apache2/dummy-host2.example.com-access_log\" common 启用个人 Sites apache 可以设置 userdir, 通过访问 http://localhost/~user/ 来访问个人 home 下的 Sites 文件夹下的网页. 在 /etc/apache2/httpd.conf 中找到 #LoadModule userdir_module libexec/apache2/mod_userdir.so #Include /private/etc/apache2/extra/httpd-userdir.conf 并取消注释. 在 /etc/apache2/extra/http-userdir.conf 中, 找到下面一行 #Include /private/etc/apache2/users/*.conf 也取消注释. 最后建立如下配置文件 user.conf, 并拷贝到/etc/apache2/users/ 下: Options +Indexes +MultiViews +FollowSymLinks +SymLinksIfOwnerMatch +ExecCGI AllowOverride All Require local Order allow,deny Allow from all 然后重启Apache: sudo apachectl restart 启用 PHP 在 / etc/apache2/httpd.conf 中找到如下一行, 并取消注释: #LoadModule php5_module libexec/apache2/libphp5.so 设置反向代理 找到如下几行并取消注释, 开启 http, https, ftp, fcgi 等反向代理: #LoadModule proxy_module libexec/apache2/mod_proxy.so #LoadModule proxy_connect_module #libexec/apache2/mod_proxy_connect.so #LoadModule proxy_ftp_module libexec/apache2/mod_proxy_ftp.so #LoadModule proxy_http_module libexec/apache2/mod_proxy_http.so #LoadModule proxy_fcgi_module libexec/apache2/mod_proxy_fcgi.so #LoadModule proxy_balancer_module #libexec/apache2/mod_proxy_balancer.so #LoadModule proxy_express_module #libexec/apache2/mod_proxy_express.so 例如配置 ghost 博客平台: ServerAdmin admin@admin DocumentRoot \"/Users/user/Sites/ghost/\" ServerName ghost.localhost ServerAlias www.ghost.localhost ErrorLog \"/private/var/log/apache2/ghost.localhost-error_log\" CustomLog \"/private/var/log/apache2/ghost.localhost-access_log\" common ProxyPreserveHost on ProxyRequests off ProxyPass / http://localhost:3000/ ProxyPassReverse / http://localhost:3000/ PS: If any question, please leave a note. (o)/~ 水水更健康 O(∩_∩)O~… document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":"macos apache"},{"title":"Personal GPG Public Key","url":"//gpg.html","text":"In the past, I have several personal gpg keys, and all lost now. So again I signed a new private gpg key. The following is my public key. You can search on MIT PGP Key Server to verify it. Modified: 2018-09-21 PM 16:16:51 The newest pgp key is B939CFD2, of which fingerpint is: CDB4 F475 37A2 E077 B4CB 43A7 D52F 2430 B939 CFD2 and all others are deprecated. -----BEGIN PGP PUBLIC KEY BLOCK----- Version: GnuPG v2.0.22 (GNU/Linux) mQINBFukjyABEADUfkaOYGVZLUoiYgrg2mMXiA6xaQIMfrgb2JU99e9H6UV0VnV4 +J9fZU9SycDg90vX286H+WvaDuhDAgjllwb0iP/Xt2D4B4E/sHetk03PxBxzgPzo 1n2R9uEYpVUJi9kGuY8HGUxc6acc8pYB9sd1iySWzPDeKW+M9rPRPwE4c8KSUA6u GvrCG2THDle2/Z7zjDCemOZk8mdjYFnpowyHEOsGki3pvKCGH4aDpYTtGpuxj6JS xU0ShMFRSUDUr0uevTkndzk4zBau+laINQD8/jnLDjzeIUnkGDhwq/PvvJHFaXjs p8SfmqU87WH+0/mYVv0i+IvNt7MVgKfRz2/TRLnp97+MFQCrLZyM8zzBNe+k7XHu wGrHx4CxL+8eOo1Pv2OH1UpzSoehxFQMX63kkTYnSnjC2YJMGb5aSO9lD9l+Zjg0 67RcQkocC81ICJRbtKllHHIZI4nlTyBgwTpw/kBAdO8RIbu7V+CtNyXr0hv71Qc6 rxKvRrqS/6iecKn46fgtsQgJ8J4zePf2CTb2/hf7iW96PbE7LsAFQ8ccYvCFSu0n sChn8l8/R8jDnSDJDEzHiHJT4/zCprjP5kMeGyWJ66GLvc4dgzaSVUb5DoVKaSBs 2qnkWiBK/h0Ta9STBUkAj8+VrX+oXhYmFO9bdVd/N71WT/8v5qFaGDIVVwARAQAB tDtZdWppYW5nIEJpIChQZXJzb25hbCBLZXkgZm9yIGFtaXRvLm1lKSA8Ynl1amlh bmdAZ21haWwuY29tPokCPwQTAQIAKQUCW6SPIAIbAwUJBaOagAcLCQgHAwIBBhUI AgkKCwQWAgMBAh4BAheAAAoJENUvJDC5Oc/SmtAQALRa4w8IQZ6pZ4TaQPWl8XF1 esfYslwVab0Q/W8OvLnOna6KZImVK0paOa9WULmtWEEY/SS1Cns+Q5f3dlbkt8Hu zeznCd24rmhYSpnptpAu5BK6yr8gbb/pgxnzKUGQo5Zr+ECKbsDQym2TPJvevwXu JSMcrcS+XfN7aXSrAvbwgO3/4SPoX2lF1bZIcNOfgdNklHJ2COga55BQQyHkRRvZ 8hiuVQLe6FdtY5uBoPqo4yFRmJr/OgYt7nzBsdgR86SOELmU7FZn1Rkm//v166I+ HuI+E745jLf/5R8oviseiHLSsc+jtuX9FhsyWUwNQ7CybdPTHDzOF8j/qW4MGeQz 1XNql03/OfM6SlYoczBu73bThQa+jgaQGZt98XguSmDzeP5Bmec0+OYQ8H0fUZUc +L/7g+0pu3SEfzlYz7imsiKx3uM9RbDDQhBIEk3BiOByW1uDADO4GlEIuiAtPfGC O+Crz+ahxmaxyq7ilDAsdz5lGiWm7z3aHgNdNRrKmqiXb0q3HnHjkDGdvU3GKo+n lqKQQ1oxbpdPU5mUzjR0re56xXLem+UtZSTGRP3iPZJBiSt7451D4x7E7b0x0kJD IYacTIK2sB4jj+mcUaKisBQaySdXWPfbLPAJhYXlbTVH+y2wH2WEyDE/JCmNf11F CdFYYtJ29njcAS6zj4OAuQINBFukjyABEADn2hu4gyqHEej2OfdNooKk7PmFHtL4 dYi/Jlo4+kJ2Ev7Ei2VN4LLSYCkXpfpBXGI/kr4LZH8qMGDvJEIH0hKDJyYlWsEY VzuVnqxEQFxnvMXV0OgDJt8u0jii6kfSyeH8SdfNuIO7eSN6pwSQZDZL3GmqVhjW cNkIINAZQ/hOjmXrBvmEBzYwOXk07IHwkyh/Z+/9FLJyEGAOWWOeVdjQ79mWUf63 kZRDsxtBl9QLiTy1Z3sqowqAwWHudYi5Ot0bMctswBpilMmuwLvVKfRdKcnEg6F8 55gwaxmIUg6swxOMqt7AS5sxrSLptnwqsNNKA5hlb1Hn1IcIL6Va7yK6oh+zs1BM 7uCkpJA7+NiN5Uo0BkmMztAb8SScnkjkT5vfWHxPe+7OXb/ewfp6qsH6DMuWVbWe QT+YrWUF54nG5HpAzP21LWHwvO7MJ4UnVTP4GD/5N3U5ToOii6GSfQ8u7QTOCG/L gLGfg1A0n2leJ1dZZEx8DUEpHZNh4YLED/NzBJSlMTdaB2MtOEvxNA9CBfEZWtON CggMNKC2EbRjKGxyE89UnMC9HO9GrjZGvPpzlQK6BXOBjh0IzEQvrPm9Blikd8JM 1QauyO1QTe7lW0HN/nHO4a1Kw8oQpkYdXIyb9OdPTb8b28m5MjIEbF3TKcuGK7/N I7A4VWaHFR8NMwARAQABiQIlBBgBAgAPBQJbpI8gAhsMBQkFo5qAAAoJENUvJDC5 Oc/SnegP/iHatOX9iNp3eTs+LvmLR+CrXc2xihogwKPoSdVqOMo2+9MR6la+slp5 RlNH6TQh96wZiTJYcc4JMc3fvFcD5YHiaMrh+/TwDnsykyD8B76PiazkQh/Ow5jx VcbZHnFLHW4+Zu+kEzPncnK1JGNCMExdgO+vpWJOT3vI1ZhZGOANrGmVaYBAhST5 82bYVBIqHM0uRFs3qiZo/uqAiE3e6jdTXBpPwWgp/cJqZ8doVoUhb18nuimf+Na9 OJmyUKw+9tFOq/2u5AgYXUj1wNIvuWZTrmGpJvUrQoXnaLxI9GpbillZcs2dtD48 /WnWOHXhA3QeoXSP8JLV/x2RKNVGC35/86M8eaMsY6XGDV0hnilVmDgAuKDMQqEl 0LE4m/NnzaIVJqWrtAMTX29IkIg6YC07y91HVzSl/7o2UojKO3BgGxsruBA9+/cG RiOBFKXUFQhxv1fgwsRU+7OybKI+fStHoAHBKYgnJlFkkAooLv0vpH0oqUaaBMJc 4ylv8GI1RK9r8/mR9vdU3M1LWK4bESLAgiwTGEoY9wd0GAvoOx7OEtr4J/Voh9A9 ayMHgmkqpQM0veeCcdbiFLS55lweq7fL1onUKOQc7u6vJkyfmTGqrTEeF9RDgQsi /UtUVXV6zoI/dtLxImVbctTvpgCi1Jte8sRfBsAX4vpaHHTFSwjG =6Ts5 -----END PGP PUBLIC KEY BLOCK----- document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"About Me","url":"//about/index.html","text":"Absent Now!! document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"all-archives","url":"/all-archives/index.html","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"all-categories","url":"/all-categories/index.html","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"all-tags","url":"/all-tags/index.html","text":"document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"404 Not Found","url":"//404.html","text":"Ops, Seems that you find something missing.... Click here to return home or report to Amito for this error if you're sure it should exist. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"个人读书清单","url":"/books/index.html","text":"待阅读 书名 作者 分类 日期 评分 评语 古拉格：一部历史 安妮•阿普尔鲍姆 (美) 历史、纪实 切尔诺贝利的祭祷 阿列克谢耶维奇 (俄) 历史、纪实 古拉格之恋 奥兰多·费吉斯 (英) 纪实、爱情 苏联的最后一天 康纳·奥克莱利 (爱) 历史、报告 活着回来的男人 小熊英二 (日) 战争、历史 爱的边境 朵莉・拉宾雅 (以) 宗教、爱情 2020 年 书名 作者 分类 日期 评分 评语 祈祷落幕时 东野圭吾 (日) 推理、亲情 11/04 ⭐️️⭐️⭐️️⭐️️ “阿初是这样想的: 终是一死, 不如被那个心底里仰慕的男人刺死。德兵卫明白了她的想法, 成全了她。在他看来, 这只是为自己拼命爱着的女人完成心愿。” 双城记 查尔斯·狄更斯 (英) 复仇，抗争 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “那是最美好的时代，那是最糟糕的时代” 费马最终定理 日冲樱皮 (日) 数学，历史 06/01 ⭐️️ ⭐️️ 费马大定理的通俗介绍，本身故事比较孱弱， 介绍也相对一般 夜光的阶梯 松本清张 (日) 政治秩序的起源：从前人类时代到法国大革命 弗朗西斯·福山 (美) 娜塔莎之舞 奥兰多·费吉斯 (英) 历史、文化 鼠疫 阿尔贝·加缪 (法) 2019 年 书名 作者 分类 日期 评分 评语 悬崖边的名士 大生 (中) 历史、文化 04/01 ⭐️️ ⭐️️ ⭐️️ ⭐️️ “是真名士自风流”，只有独立人格，才能与众不同 往事不曾离去 朱利安·费罗斯 (英) 爱情、历史 12/10 ⭐️️ ⭐️️ ⭐️️ ⭐️️ 这是一幅英伦社会百态图, 这是一段跨越阶级与时代的爱情, 这是一个敢于打破常规的年轻人白手起家的故事 1984 乔治•奥威尔 (英) 政治、乌托邦 11/11 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “历史不是一面镜子，而是黑板上的记号，可以随时擦去，随时填补。更为可怕的是，一旦涂改了，你找不到证据去证明这是篡改历史的行为。” 沙丘 弗兰克·赫伯特 (美) 科幻、哲学 05/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “恐惧是思维杀手。恐惧是引向彻底毁灭的小小死神。我将正视恐惧，任它通过我的躯体。当恐惧逝去，我会打开心眼，看清它的轨迹。恐惧所过之处，不留一物，唯我独存。” 沙丘 2·救世主 弗兰克·赫伯特 (美) 科幻、哲学 06/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “权力会使那些掌握着过多权力的人陷入孤立，逐渐与真实世界脱节……最后垮台。” 沙丘 3·沙丘之子 弗兰克·赫伯特 (美) 科幻、哲学 07/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “大多数生命都是一条脱离了自我的航程。大多数人偏爱圈养的生活。你把头伸进食槽，然后满意的咀嚼着，直到死的那天。你从来不曾离开过牲口棚，抬起头，做回你自己。” 沙丘 4·沙丘神帝 弗兰克·赫伯特 (美) 科幻、哲学 08/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “大多数文明建立在怯懦之上。教人怯懦是教化的捷径。你淡化勇敢的标准。你削弱意志，扼制欲望，画地为牢。你为一举一动都设定条条框框。你不允许存在无序状态。你甚至教导孩子放慢呼吸频率。最终，你得到顺民。” 沙丘 5·沙丘异端 弗兰克·赫伯特 (美) 科幻、哲学 09/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “现在已经没有人发生反对她的计划，但是这种情况往往预示着暴力事件。” 沙丘 6·圣殿沙丘 弗兰克·赫伯特 (美) 科幻、哲学 10/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “有些浪花会抛弃你，默贝拉。但是你要重整旗鼓，继续上路。失败一千次，站起一千零一次。你就能够在起起伏伏中保持平衡。” 2018 年 书名 作者 分类 日期 评分 评语 虚无的十字架 东野圭吾 (日) 推理 10/02 ⭐️️ ⭐️️ ⭐️️ ⭐️️ 到底有谁可以断言，“这个杀人凶手只要在监狱关多少多少年，就可以改邪归正”，把杀人凶手绑在这种虚无的十字架上，到底有什么意义？ 梦幻花 东野圭吾 (日) 推理 22/12 ⭐️️ ⭐️️ ⭐️️ 不是不擅长学习，只是没有找到自己想要学习的东西。这既是在给自己鼓起勇气，也是在提醒自己：如果一味逃避便一事无成 假面饭店 东野圭吾 (日) 推理 28/12 ⭐️️ ⭐️️ ⭐️️ “以前，前辈曾经这样教过我。他说到饭店里来的人，都带着一张叫作客人的假面，绝对不能忘记这一点。” 盛夏的方程式 东野圭吾 (日) 推理 16/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ “放着好奇心不去理会，这可是最大的罪过。一个人成长的最大能源，就是好奇心。” 神探伽利略 东野圭吾 (日) 推理 25/12 ⭐️️ ⭐️️ ⭐️️ 疾风回旋曲 东野圭吾 (日) 推理 27/12 ⭐️️ ⭐️️ ⭐️️ 预知梦 东野圭吾 (日) 推理 29/12 ⭐️️ ⭐️️ ⭐️️ “只要说出越多预言，那么总是会有些预言碰巧说中。预言师更强调此事…这就是预言师常用的伎俩。” 嫌疑人 X 的献身 东野圭吾 (日) 推理 29/12 ⭐️️ ⭐️️ ⭐️️ ⭐️️ “你我都不可能摆脱时钟的束缚，彼此都已沦为社会这个时钟的齿轮。一旦少了齿轮，时钟就会出乱子。纵然渴望率性而为，周遭也不容许。我们虽然得到了安定，但失去自由也是不争的事实。” 拉普拉斯的魔女 东野圭吾 (日) 推理 29/11 ⭐️️ ⭐️️ ⭐️️ “人类是原子，即使每一个个体都很平凡，无自觉地活在世上，然而一旦成为集合体，就会戏剧性地实现物理法制。 这个世界上没有任何个体不具有存在地意义，没有任何一个。” 圣女的救济 东野圭吾 (日) 推理 16/11 ⭐️️ ⭐️️ ⭐️️ ⭐️️ “人是种既复杂难懂又充满矛盾的生物，不管是前一分针还在派对上玩得很热闹开心，还是前秒预订了餐位，想死的时候随时都会死。” 2017 年 书名 作者 分类 日期 评分 评语 秘密 东野圭吾 (日) 推理 10/06 ⭐️️ ⭐️️ ⭐️️ ⭐️️ 一早就猜到了这种结局，但是还是有些小小的期待。秘密藏在心底，不被人知，独自承担 被偷走的人 提埃里·科恩 (法) 悬疑 12/03 ⭐️️ ⭐️️ ⭐️️ ⭐️️ 比较俗套的小说，但是还是给人心灵上的触动。真正的人生是一直向前的列车，无法重来 解忧杂货店 东野圭吾 (日) 推理 10/01 ⭐️️ ⭐️️ ⭐️️ ⭐️️ ⭐️️ “我的回答之所以发挥了作用，原因不是别的，是因为他们自己很努力。如果自己不想积极认真地生活，不管得到什么样的回答都没用。” table th:nth-of-type(1){width: 13%;} table th:nth-of-type(2){width: 13%;} table th:nth-of-type(3){width: 10%;} table th:nth-of-type(4){width: 8%;} table th:nth-of-type(5){width: 12%;} table th:nth-of-type(6){width: 45%;} document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"友情链接","url":"/links/index.html","text":"Absent Now! document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"观影记录","url":"/movies/index.html","text":"泰国 鬼夫 初恋这件小事 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""},{"title":"帮助本站","url":"//do.html","text":"如果你觉得这里的内容有点用，想支持一下，可以通过如下的方式。 I. Buy Me a Coffee 支付宝扫一扫 , 微信扫一扫, 或者 Paypal 扫一扫 II. VPS Affs 如果需要 VPS， 可通过下面的链接注册，可各获得 $5 or $10。 Linode : 老牌厂商, 稳定可靠, 1G 内存起, SSD 硬盘 Vultr : 15 数据中心, 低至 $5/ 月, 1GB 内存起 DigitalOcean : 1GB 内存起,5 刀每月, SSD 硬盘 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","tags":""}]}